{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170c3adb-882b-47cc-9f71-6020cf0cbae5",
   "metadata": {},
   "source": [
    "# Assignment 2 — Neural Sequence Models: From Recurrence to Self-Attention\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Recap: Assignment 1 Baseline Performance\n",
    "\n",
    "Assignment 1 established classical machine learning baselines for **EXIST 2025 — Task 1, Subtask 1.1 (Sexism Identification in Tweets)**, a binary text classification task on a bilingual (English/Spanish) corpus of ~10,000 tweets. Using TF-IDF feature representations paired with linear classifiers (Logistic Regression and LinearSVC), and FastText mean-pooled embeddings as an alternative dense representation, the following results were obtained on the held-out validation set (1,038 samples):\n",
    "\n",
    "| Model | Feature Representation | CV F1-Macro | Val F1-Macro | Val Accuracy |\n",
    "|---|---|---|---|---|\n",
    "| **Logistic Regression** *(best overall)* | TF-IDF (unigrams, sublinear_tf) | 0.6536 | **0.75** | 0.75 |\n",
    "| LinearSVC | TF-IDF (unigrams) | 0.6524 | — | — |\n",
    "| SVM (LinearSVC) *(best dense)* | FastText embeddings (scratch) | 0.5322 | 0.62 | 0.62 |\n",
    "| Logistic Regression | FastText embeddings (scratch) | 0.5234 | — | — |\n",
    "\n",
    "Key takeaways from Assignment 1:\n",
    "\n",
    "- **Simple lowercasing** was the optimal preprocessing strategy (F1: 0.6506), outperforming stemming, lemmatization, and stopword removal.\n",
    "- **Unigrams only** outperformed all higher-order n-gram configurations, due to feature space explosion relative to the small training corpus (~6,920 samples).\n",
    "- **Sparse TF-IDF representations decisively outperformed dense FastText embeddings** trained from scratch (~12.1 F1-point gap), highlighting the limitations of mean-pooled embeddings on a small, domain-specific corpus.\n",
    "- **Error analysis** revealed two systematic failure modes of bag-of-words models: (1) false negatives on tweets that *report or condemn* sexism (anti-sexist tone masks SEXIST label), and (2) false positives driven by gender-related keywords used in neutral or non-sexist contexts (e.g., references to *patriarchy* or *mgtow* without sexist intent). These errors share a common root cause: TF-IDF cannot model *how* words are used, only *which* words appear.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Research Questions\n",
    "\n",
    "Assignment 1 demonstrated the ceiling of bag-of-words approaches for this task. This assignment transitions to **neural sequence models** — architectures that explicitly model sequential structure, word order, and long-range dependencies. Concretely, we implement and evaluate a **Bidirectional LSTM** and a **Transformer-based model** (fine-tuned from a pre-trained checkpoint, given the small dataset size), comparing them against the Assignment 1 baselines. \n",
    "\n",
    "This motivates the following research questions:\n",
    "\n",
    "> **RQ1 — Performance:** Do neural sequence models (BiLSTM, Transformer/BERT) yield meaningful F1-Macro improvements over the TF-IDF + Logistic Regression baseline on sexism identification in tweets?\n",
    "\n",
    "> **RQ2 — Error correction:** Which specific error types from Assignment 1 — irony/sarcasm, anti-sexist reporting, keyword mismatch — are resolved by architectures that capture sequential context and pragmatic meaning?\n",
    "\n",
    "> **RQ3 — Data efficiency:** Given the limited training size (~6,920 samples), do neural models trained from scratch suffer from data starvation, and does transfer learning via a pre-trained Transformer (e.g., multilingual BERT / XLM-R) mitigate this?\n",
    "\n",
    "> **RQ4 — Computational cost:** Is the added complexity of neural models — in training time, GPU memory, and inference latency — justified by the performance gains over the classical baseline?\n",
    "\n",
    "> **RQ5 — Architecture trade-offs:** How do BiLSTM and Transformer architectures compare in terms of their inductive biases for this specific task (short bilingual tweets, subtle/ironic content, near-balanced classes)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de207790-3430-4e53-9ecc-c9cabc3463ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from time import time, sleep\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "# NLTK downloads\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Configurar semilla para reproducibilidad\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fbd1b65-9dc3-47f6-835b-5eba4a2c6f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "\n",
      "Total Samples - Training: 6920\n",
      "final_label_str\n",
      "YES    3553\n",
      "NO     3367\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total Samples - Validation (Will be used as TEST): 1038\n",
      "final_label_str\n",
      "YES    559\n",
      "NO     479\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_and_parse_data(filepath):\n",
    "    \"\"\"\n",
    "    Parses nested JSON and applies Majority Voting for labels.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    df = df.reset_index(drop=True).rename(columns={'index': 'id_EXIST'})\n",
    "    \n",
    "    # Label Processing (Majority Voting)\n",
    "    if 'labels_task1_1' in df.columns:\n",
    "        def get_majority_vote(labels_list):\n",
    "            if not isinstance(labels_list, list): return np.nan\n",
    "            counts = pd.Series(labels_list).value_counts()\n",
    "            # Tie-breaking: Prioritize 'YES' (Sexism) if tie\n",
    "            if len(counts) > 1 and counts.iloc[0] == counts.iloc[1]:\n",
    "                if 'YES' in counts.index[:2]: return 'YES'\n",
    "            return counts.idxmax()\n",
    "        \n",
    "        df['final_label_str'] = df['labels_task1_1'].apply(get_majority_vote)\n",
    "        df['label'] = df['final_label_str'].map({'YES': 1, 'NO': 0})\n",
    "        df = df.dropna(subset=['label'])\n",
    "        df['label'] = df['label'].astype(int)\n",
    "        \n",
    "    return df\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "df_train = load_and_parse_data('../data/training/EXIST2025_training.json')\n",
    "df_val = load_and_parse_data('../data/dev/EXIST2025_dev.json')\n",
    "df_test = load_and_parse_data('../data/test/EXIST2025_test_clean.json')\n",
    "\n",
    "print(f\"\\nTotal Samples - Training: {len(df_train)}\")\n",
    "print(df_train['final_label_str'].value_counts())\n",
    "print(f\"\\nTotal Samples - Validation (Will be used as TEST): {len(df_val)}\")\n",
    "print(df_val['final_label_str'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada5f9c9-f261-45c0-8435-c53af7017e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) | set(stopwords.words('spanish'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text, strategy='raw'):\n",
    "    text_processed = str(text)\n",
    "    if strategy == 'raw':\n",
    "        return text_processed\n",
    "    if strategy == 'lowercase':\n",
    "        return text_processed.lower()\n",
    "    if strategy == 'no_punct':\n",
    "        text_processed = re.sub(r'[^\\w\\s]', '', text_processed)\n",
    "        return text_processed.lower()\n",
    "    if strategy == 'no_stopwords':\n",
    "        text_processed = text_processed.lower()\n",
    "        words = text_processed.split()\n",
    "        return \" \".join([w for w in words if w not in stop_words])\n",
    "    if strategy == 'stemmed':\n",
    "        text_processed = text_processed.lower()\n",
    "        words = text_processed.split()\n",
    "        return \" \".join([stemmer.stem(w) for w in words])\n",
    "    if strategy == 'lemmatized':\n",
    "        text_processed = text_processed.lower()\n",
    "        words = text_processed.split() \n",
    "        return \" \".join([lemmatizer.lemmatize(w) for w in words])\n",
    "    return text_processed\n",
    "\n",
    "# Process texts\n",
    "df_train['text_clean'] = df_train['tweet'].apply(lambda x: preprocess_text(x, 'lowercase'))\n",
    "df_val['text_clean'] = df_val['tweet'].apply(lambda x: preprocess_text(x, 'lowercase'))\n",
    "df_test['text_clean'] = df_test['tweet'].apply(lambda x: preprocess_text(x, 'lowercase'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c164fcab-034d-4abc-9b83-63037c8286af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=2):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.min_freq = min_freq\n",
    "        \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 2\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                if frequencies[word] == self.min_freq:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "                    \n",
    "    def tokenize(self, text):\n",
    "        return re.findall(r'\\w+', text)\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokenized_text]\n",
    "\n",
    "vocab = Vocabulary(min_freq=2)\n",
    "vocab.build_vocabulary(df_train['text_clean'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff00d06-244c-4acf-b22c-720de46a5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXISTDataset(Dataset):\n",
    "    def __init__(self, df, vocab, max_len=64):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.iloc[index]['text_clean']\n",
    "        label = self.df.iloc[index]['label'] if 'label' in self.df.columns else -1\n",
    "        \n",
    "        tokens = self.vocab.numericalize(text)\n",
    "        length = len(tokens)\n",
    "        \n",
    "        # Evitar secuencias vacías (rompen el pack_padded_sequence)\n",
    "        if length == 0:\n",
    "            tokens = [self.vocab.stoi[\"<UNK>\"]]\n",
    "            length = 1\n",
    "            \n",
    "        # Truncar si es más largo que max_len\n",
    "        if length > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "            length = self.max_len\n",
    "            \n",
    "        # Rellenar (Padding)\n",
    "        padded_tokens = tokens + [self.vocab.stoi[\"<PAD>\"]] * (self.max_len - length)\n",
    "            \n",
    "        return torch.tensor(padded_tokens), torch.tensor(label, dtype=torch.long), torch.tensor(length, dtype=torch.long)\n",
    "\n",
    "# Dataset global de Train (se dividirá en Folds)\n",
    "train_dataset = EXISTDataset(df_train, vocab)\n",
    "\n",
    "# El dataset \"dev\" original será nuestro \"TEST\" real\n",
    "test_dataset_real = EXISTDataset(df_val, vocab)\n",
    "test_loader_real = DataLoader(test_dataset_real, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b94b92ec-de05-46fe-b33d-c5863ce334c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout=0.5):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                            bidirectional=True, batch_first=True,\n",
    "                            dropout=dropout if n_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_output, (hidden, cell) = self.lstm(packed)\n",
    "        forward_hidden = hidden[-2]\n",
    "        backward_hidden = hidden[-1]\n",
    "        final_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "        output = self.dropout(final_hidden)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_score, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "            self.best_model_state = model.state_dict().copy()\n",
    "        elif val_score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.best_model_state = model.state_dict().copy()\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "\n",
    "# Fixed constants\n",
    "VOCAB_SIZE  = len(vocab.stoi)\n",
    "OUTPUT_DIM  = 2\n",
    "EPOCHS      = 20\n",
    "N_SPLITS    = 5\n",
    "SEED        = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a901673b-bd9e-4621-a556-b0856fb920eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/24 20:17:03 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/02/24 20:17:03 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/02/24 20:17:04 INFO mlflow.tracking.fluent: Experiment with name 'BiLSTM_Optuna_Tuning' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# ── MLflow setup ────────────────────────────────────────────────────────────\n",
    "MLFLOW_EXPERIMENT = \"BiLSTM_Optuna_Tuning\"\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT)\n",
    "\n",
    "# ── Search space ────────────────────────────────────────────────────────────\n",
    "# All hyperparameters Optuna will tune are defined here for easy editing.\n",
    "SEARCH_SPACE = {\n",
    "    \"embedding_dim\" : (\"categorical\", [100, 200, 300]),\n",
    "    \"hidden_dim\"    : (\"categorical\", [128, 256, 512]),\n",
    "    \"n_layers\"      : (\"int\",         [1, 3]),          # suggest_int(low, high)\n",
    "    \"dropout\"       : (\"float\",       [0.2, 0.5]),       # suggest_float(low, high)\n",
    "    \"lr\"            : (\"loguniform\",  [1e-4, 1e-2]),     # suggest_float(..., log=True)\n",
    "    \"batch_size\"    : (\"categorical\", [16, 32, 64]),\n",
    "}\n",
    "\n",
    "\n",
    "def suggest_hyperparams(trial):\n",
    "    \"\"\"Translate SEARCH_SPACE into Optuna suggestions.\"\"\"\n",
    "    params = {}\n",
    "    for name, (kind, bounds) in SEARCH_SPACE.items():\n",
    "        if kind == \"categorical\":\n",
    "            params[name] = trial.suggest_categorical(name, bounds)\n",
    "        elif kind == \"int\":\n",
    "            params[name] = trial.suggest_int(name, *bounds)\n",
    "        elif kind == \"float\":\n",
    "            params[name] = trial.suggest_float(name, *bounds)\n",
    "        elif kind == \"loguniform\":\n",
    "            params[name] = trial.suggest_float(name, *bounds, log=True)\n",
    "    return params\n",
    "\n",
    "\n",
    "def run_cv(params: dict) -> tuple[float, dict]:\n",
    "    \"\"\"\n",
    "    5-fold stratified CV for a given hyperparameter configuration.\n",
    "    Returns (mean_val_f1, per-fold metrics dict).\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    y_array = df_train['label'].values\n",
    "    fold_f1s = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        skf.split(np.zeros(len(y_array)), y_array)\n",
    "    ):\n",
    "        train_loader = DataLoader(\n",
    "            Subset(train_dataset, train_idx),\n",
    "            batch_size=params[\"batch_size\"], shuffle=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            Subset(train_dataset, val_idx),\n",
    "            batch_size=params[\"batch_size\"], shuffle=False\n",
    "        )\n",
    "\n",
    "        model = BiLSTMClassifier(\n",
    "            vocab_size    = VOCAB_SIZE,\n",
    "            embedding_dim = params[\"embedding_dim\"],\n",
    "            hidden_dim    = params[\"hidden_dim\"],\n",
    "            output_dim    = OUTPUT_DIM,\n",
    "            n_layers      = params[\"n_layers\"],\n",
    "            dropout       = params[\"dropout\"],\n",
    "        ).to(device)\n",
    "\n",
    "        criterion     = nn.CrossEntropyLoss()\n",
    "        optimizer     = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "        early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            # ── Train ──\n",
    "            model.train()\n",
    "            for texts, labels, lengths in train_loader:\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(texts, lengths), labels)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            # ── Validate ──\n",
    "            model.eval()\n",
    "            preds, targets = [], []\n",
    "            with torch.no_grad():\n",
    "                for texts, labels, lengths in val_loader:\n",
    "                    texts = texts.to(device)\n",
    "                    out   = model(texts, lengths)\n",
    "                    preds.extend(out.argmax(1).cpu().numpy())\n",
    "                    targets.extend(labels.numpy())\n",
    "\n",
    "            val_f1 = f1_score(targets, preds, average='macro')\n",
    "\n",
    "            if early_stopping(val_f1, model):\n",
    "                break\n",
    "\n",
    "        fold_f1s.append(early_stopping.best_score)\n",
    "\n",
    "    mean_f1      = float(np.mean(fold_f1s))\n",
    "    per_fold_log = {f\"fold_{i+1}_f1\": v for i, v in enumerate(fold_f1s)}\n",
    "    return mean_f1, per_fold_log\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"Optuna objective: suggest params → CV → log to MLflow → return metric.\"\"\"\n",
    "    params = suggest_hyperparams(trial)\n",
    "\n",
    "    with mlflow.start_run(\n",
    "        run_name=f\"trial_{trial.number}\", nested=True\n",
    "    ):\n",
    "        # Log all hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"trial_number\", trial.number)\n",
    "\n",
    "        mean_f1, per_fold_log = run_cv(params)\n",
    "\n",
    "        # Log per-fold and aggregate metrics\n",
    "        mlflow.log_metrics(per_fold_log)\n",
    "        mlflow.log_metric(\"mean_cv_f1_macro\", mean_f1)\n",
    "\n",
    "        # Optuna pruning hook (optional but useful with a Pruner)\n",
    "        trial.report(mean_f1, step=0)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return mean_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c7d89e2-7dc0-4acd-b7f8-3c5587afbaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-24 20:17:20,096]\u001b[0m A new study created in memory with name: bilstm_sexism\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02e012723f84b8982d92a5d794a1f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-24 20:19:27,678]\u001b[0m Trial 0 finished with value: 0.7184964405687821 and parameters: {'embedding_dim': 200, 'hidden_dim': 128, 'n_layers': 1, 'dropout': 0.45985284373248053, 'lr': 0.0015930522616241021, 'batch_size': 64}. Best is trial 0 with value: 0.7184964405687821.\u001b[0m\n",
      "\u001b[32m[I 2026-02-24 20:21:58,257]\u001b[0m Trial 1 finished with value: 0.6961201858060088 and parameters: {'embedding_dim': 100, 'hidden_dim': 512, 'n_layers': 2, 'dropout': 0.2873687420594126, 'lr': 0.0016738085788752138, 'batch_size': 64}. Best is trial 0 with value: 0.7184964405687821.\u001b[0m\n",
      "\u001b[32m[I 2026-02-24 20:25:35,490]\u001b[0m Trial 2 finished with value: 0.6993984921712976 and parameters: {'embedding_dim': 200, 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.2511572371061875, 'lr': 0.00013492834268013249, 'batch_size': 32}. Best is trial 0 with value: 0.7184964405687821.\u001b[0m\n",
      "\u001b[32m[I 2026-02-24 20:29:16,560]\u001b[0m Trial 3 finished with value: 0.6987525959654033 and parameters: {'embedding_dim': 300, 'hidden_dim': 512, 'n_layers': 1, 'dropout': 0.47279612062363463, 'lr': 0.00032927591344236165, 'batch_size': 16}. Best is trial 0 with value: 0.7184964405687821.\u001b[0m\n",
      "\u001b[32m[I 2026-02-24 20:32:26,689]\u001b[0m Trial 4 finished with value: 0.7168840425731726 and parameters: {'embedding_dim': 300, 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.47656227050693506, 'lr': 0.00015030900645056822, 'batch_size': 64}. Best is trial 0 with value: 0.7184964405687821.\u001b[0m\n",
      "\u001b[32m[I 2026-02-24 20:36:30,426]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-24 20:40:25,008]\u001b[0m Trial 6 finished with value: 0.7227649784481978 and parameters: {'embedding_dim': 200, 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.23476071785753894, 'lr': 0.005323617594751502, 'batch_size': 16}. Best is trial 6 with value: 0.7227649784481978.\u001b[0m\n",
      "\u001b[32m[I 2026-02-24 20:42:36,706]\u001b[0m Trial 7 finished with value: 0.7194186661518881 and parameters: {'embedding_dim': 300, 'hidden_dim': 256, 'n_layers': 1, 'dropout': 0.4139734361668985, 'lr': 0.0033233042062267943, 'batch_size': 32}. Best is trial 6 with value: 0.7227649784481978.\u001b[0m\n",
      "\u001b[32m[I 2026-02-24 20:44:36,872]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-24 20:47:02,378]\u001b[0m Trial 9 finished with value: 0.7281862926162865 and parameters: {'embedding_dim': 300, 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.4614381770563153, 'lr': 0.004048966222584676, 'batch_size': 32}. Best is trial 9 with value: 0.7281862926162865.\u001b[0m\n",
      "\u001b[32m[I 2026-02-24 20:50:20,691]\u001b[0m Trial 10 finished with value: 0.7200125274194954 and parameters: {'embedding_dim': 300, 'hidden_dim': 128, 'n_layers': 3, 'dropout': 0.38680911674883384, 'lr': 0.0006228997444898865, 'batch_size': 32}. Best is trial 9 with value: 0.7281862926162865.\u001b[0m\n",
      "\u001b[32m[I 2026-02-24 20:58:53,555]\u001b[0m Trial 11 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-24 21:02:44,556]\u001b[0m Trial 12 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-24 21:04:56,040]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-24 21:12:22,330]\u001b[0m Trial 14 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-24 21:15:09,858]\u001b[0m Trial 15 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-24 21:18:33,744]\u001b[0m Trial 16 finished with value: 0.7218247612741887 and parameters: {'embedding_dim': 200, 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.49880509680836405, 'lr': 0.009947589451282216, 'batch_size': 32}. Best is trial 9 with value: 0.7281862926162865.\u001b[0m\n",
      "\u001b[32m[I 2026-02-24 21:24:53,755]\u001b[0m Trial 17 finished with value: 0.7241831786769153 and parameters: {'embedding_dim': 200, 'hidden_dim': 256, 'n_layers': 3, 'dropout': 0.3452752122143557, 'lr': 0.002723262253122305, 'batch_size': 16}. Best is trial 9 with value: 0.7281862926162865.\u001b[0m\n",
      "\u001b[32m[I 2026-02-24 21:29:02,500]\u001b[0m Trial 18 finished with value: 0.7229837640262348 and parameters: {'embedding_dim': 300, 'hidden_dim': 256, 'n_layers': 3, 'dropout': 0.361967826450427, 'lr': 0.0021932608435924436, 'batch_size': 32}. Best is trial 9 with value: 0.7281862926162865.\u001b[0m\n",
      "\u001b[32m[I 2026-02-24 21:35:36,498]\u001b[0m Trial 19 finished with value: 0.7209235991562208 and parameters: {'embedding_dim': 100, 'hidden_dim': 128, 'n_layers': 3, 'dropout': 0.41577122131321803, 'lr': 0.0010363860042430153, 'batch_size': 16}. Best is trial 9 with value: 0.7281862926162865.\u001b[0m\n",
      "\u001b[32m[I 2026-02-24 21:38:54,264]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-24 21:42:58,329]\u001b[0m Trial 21 finished with value: 0.7203738026069507 and parameters: {'embedding_dim': 300, 'hidden_dim': 256, 'n_layers': 3, 'dropout': 0.3596405950458684, 'lr': 0.0028089600155877187, 'batch_size': 32}. Best is trial 9 with value: 0.7281862926162865.\u001b[0m\n",
      "\u001b[32m[I 2026-02-24 21:46:13,283]\u001b[0m Trial 22 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-24 21:49:34,069]\u001b[0m Trial 23 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-24 21:53:14,802]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-24 22:00:42,259]\u001b[0m Trial 25 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-24 22:03:03,820]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-24 22:07:11,455]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-24 22:09:30,895]\u001b[0m Trial 28 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-24 22:15:27,579]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\n",
      "==================================================\n",
      "Best trial:  #9\n",
      "Best CV F1-Macro: 0.7282\n",
      "Best hyperparameters:\n",
      "  embedding_dim: 300\n",
      "  hidden_dim: 256\n",
      "  n_layers: 2\n",
      "  dropout: 0.4614381770563153\n",
      "  lr: 0.004048966222584676\n",
      "  batch_size: 32\n"
     ]
    }
   ],
   "source": [
    "# Parent MLflow run wraps the entire study\n",
    "with mlflow.start_run(run_name=\"optuna_study\"):\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "    pruner  = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction   = \"maximize\",\n",
    "        sampler     = sampler,\n",
    "        pruner      = pruner,\n",
    "        study_name  = \"bilstm_sexism\",\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials         = 30,      # ← adjust to your compute budget\n",
    "        timeout          = None,\n",
    "        show_progress_bar= True,\n",
    "    )\n",
    "\n",
    "    # ── Log best result to the parent run ──────────────────────────────────\n",
    "    best = study.best_trial\n",
    "    mlflow.log_params({f\"best_{k}\": v for k, v in best.params.items()})\n",
    "    mlflow.log_metric(\"best_mean_cv_f1_macro\", best.value)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Best trial:  #{best.number}\")\n",
    "    print(f\"Best CV F1-Macro: {best.value:.4f}\")\n",
    "    print(\"Best hyperparameters:\")\n",
    "    for k, v in best.params.items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e794c2-5dcc-4574-86da-60e54675b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "\n",
    "with mlflow.start_run(run_name=\"final_model_best_params\"):\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    final_model = BiLSTMClassifier(\n",
    "        vocab_size    = VOCAB_SIZE,\n",
    "        embedding_dim = best_params[\"embedding_dim\"],\n",
    "        hidden_dim    = best_params[\"hidden_dim\"],\n",
    "        output_dim    = OUTPUT_DIM,\n",
    "        n_layers      = best_params[\"n_layers\"],\n",
    "        dropout       = best_params[\"dropout\"],\n",
    "    ).to(device)\n",
    "\n",
    "    full_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = best_params[\"batch_size\"],\n",
    "        shuffle    = True\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params[\"lr\"])\n",
    "\n",
    "    # No early stopping: we train for a fixed number of epochs on the full\n",
    "    # training set. A reasonable default is the mean epoch at which early\n",
    "    # stopping triggered across the CV folds; here we reuse EPOCHS directly.\n",
    "    for epoch in range(EPOCHS):\n",
    "        final_model.train()\n",
    "        train_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for texts, labels, lengths in full_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(final_model(texts, lengths), labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(final_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            all_preds.extend(final_model(texts, lengths).detach().argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        mlflow.log_metric(\"train_f1_macro\", train_f1, step=epoch)\n",
    "        mlflow.log_metric(\"train_loss\", train_loss / len(full_loader), step=epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d}/{EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss/len(full_loader):.4f} | \"\n",
    "              f\"Train F1: {train_f1:.4f}\")\n",
    "\n",
    "    # Save the final model artifact\n",
    "    mlflow.pytorch.log_model(final_model, artifact_path=\"bilstm_final_model\")\n",
    "    print(\"\\nFinal model trained and logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea24bfd-47b9-4ef4-9267-a76e62759c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating the final model on the TEST set (original 'dev' set)...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"final_model_test_evaluation\"):\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    final_model.eval()\n",
    "    test_preds, test_labels_list = [], []\n",
    "\n",
    "    start_time = time()\n",
    "    with torch.no_grad():\n",
    "        for texts, labels, lengths in test_loader_real:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = final_model(texts, lengths)\n",
    "            test_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "            test_labels_list.extend(labels.cpu().numpy())\n",
    "    inference_time = time() - start_time\n",
    "\n",
    "    test_f1 = f1_score(test_labels_list, test_preds, average='macro')\n",
    "\n",
    "    # Log metrics to MLflow\n",
    "    mlflow.log_metric(\"test_f1_macro\", test_f1)\n",
    "    mlflow.log_metric(\"inference_time_seconds\", inference_time)\n",
    "\n",
    "    # Console output\n",
    "    print(f\"\\nF1-Macro Final (Test Set):       {test_f1:.4f}\")\n",
    "    print(f\"Tiempo de Inferencia total:      {inference_time:.3f} segundos\")\n",
    "    print(\"\\nConfusion Matrix:\\n\",\n",
    "          confusion_matrix(test_labels_list, test_preds))\n",
    "    print(\"\\nClassification Report:\\n\",\n",
    "          classification_report(test_labels_list, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c51f5-ec23-4d8b-8d77-a05729ca3b1a",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "621a4a24-90e5-4ec3-823c-58d0fcc657d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2fc9a171a247558f42a3467b6bd9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00199a004b0b472e919ce4fb6331a2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb861e5459541498b574643c170123f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ef6a16fdb94f82ac8c85ca360ae7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Usaremos un modelo base ligero, ideal para empezar.\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f36b59c-a155-42f8-b1c8-0e1f12c45305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # Llamada directa al tokenizador (la forma moderna y estándar)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Preparamos los textos crudos\n",
    "X_train_bert = df_train['tweet']\n",
    "y_train_bert = df_train['label']\n",
    "\n",
    "X_test_bert = df_val['tweet'] \n",
    "y_test_bert = df_val['label']\n",
    "\n",
    "bert_test_dataset = BERTDataset(X_test_bert, y_test_bert, tokenizer)\n",
    "bert_test_loader = DataLoader(bert_test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50498595-6fac-44df-b3d6-92c9f5fe8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, freeze_bert=False):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        \n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Usamos el token [CLS] (primera posición)\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "def unfreeze_last_n_layers(model, n):\n",
    "    \"\"\"Utilidad para tu Estudio de Ablación\"\"\"\n",
    "    # Congelar todo primero\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Descongelar las últimas n capas del encoder\n",
    "    if n > 0:\n",
    "        for layer in model.bert.encoder.layer[-n:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "    # La cabeza de clasificación siempre debe poder entrenarse\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156197d-3fc4-41e7-8b47-2431f84ba1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando BERT en: cuda\n",
      "\n",
      "================ BERT FOLD 1/5 ================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322cfd3fdd9a440fae040aaba62b4fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 | Train Loss: 0.5995 | Val Loss: 0.5007 | Val F1: 0.7452\n",
      "Epoch 2/4 | Train Loss: 0.4460 | Val Loss: 0.5136 | Val F1: 0.7764\n",
      "Epoch 3/4 | Train Loss: 0.3107 | Val Loss: 0.5435 | Val F1: 0.7879\n",
      "Epoch 4/4 | Train Loss: 0.2146 | Val Loss: 0.6583 | Val F1: 0.7876\n",
      "\n",
      "Mejor F1-Macro en Fold 1: 0.7879\n",
      "\n",
      "================ BERT FOLD 2/5 ================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a833f872902342eeadc2b11aa488b499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Entrenando BERT en: {device}\\n\")\n",
    "\n",
    "# Parámetros específicos para Fine-Tuning según el PDF\n",
    "EPOCHS_BERT = 4 \n",
    "BATCH_SIZE = 16 \n",
    "N_SPLITS = 5\n",
    "\n",
    "skf_bert = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "y_train_array_bert = y_train_bert.values\n",
    "bert_fold_metrics = []\n",
    "best_bert_global_f1 = 0\n",
    "best_bert_model_state = None\n",
    "\n",
    "bert_full_dataset = BERTDataset(X_train_bert, y_train_bert, tokenizer)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf_bert.split(np.zeros(len(y_train_array_bert)), y_train_array_bert)):\n",
    "    print(f\"================ BERT FOLD {fold + 1}/{N_SPLITS} ================\")\n",
    "    \n",
    "    train_sub = Subset(bert_full_dataset, train_idx)\n",
    "    val_sub = Subset(bert_full_dataset, val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_sub, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_sub, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Instanciar el modelo (Full fine-tuning inicial)\n",
    "    model = BERTClassifier(MODEL_NAME, num_classes=2, freeze_bert=False).to(device)\n",
    "    \n",
    "    # Diferentes learning rates: bajo para BERT, más alto para la nueva capa (Exigencia del PDF)\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.bert.parameters(), 'lr': 2e-5},\n",
    "        {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
    "    ])\n",
    "    \n",
    "    # Scheduler con Warmup (10% de los pasos totales)\n",
    "    total_steps = len(train_loader) * EPOCHS_BERT\n",
    "    warmup_steps = int(total_steps * 0.1)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_fold_f1 = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS_BERT):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        all_train_preds, all_train_labels = [], []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (max_norm = 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step() # Actualizar learning rate\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            all_train_preds.extend(predicted.cpu().numpy())\n",
    "            all_train_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_val_preds, all_val_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                logits = model(input_ids, attention_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                all_val_preds.extend(predicted.cpu().numpy())\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='macro')\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS_BERT} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f} | Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        # Guardar el mejor modelo del Fold\n",
    "        if val_f1 > best_fold_f1:\n",
    "            best_fold_f1 = val_f1\n",
    "            best_model_state_for_this_fold = model.state_dict().copy()\n",
    "            \n",
    "    bert_fold_metrics.append(best_fold_f1)\n",
    "    print(f\"\\nMejor F1-Macro en Fold {fold+1}: {best_fold_f1:.4f}\\n\")\n",
    "    \n",
    "    if best_fold_f1 > best_bert_global_f1:\n",
    "        best_bert_global_f1 = best_fold_f1\n",
    "        best_bert_model_state = best_model_state_for_this_fold\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(f\"F1-Macro Promedio BERT (5 Folds): {np.mean(bert_fold_metrics):.4f}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba30b92-ccc8-4508-9b07-e69328b2613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluando el mejor modelo BERT en el conjunto de TEST...\")\n",
    "\n",
    "final_bert = BERTClassifier(MODEL_NAME, num_classes=2).to(device)\n",
    "final_bert.load_state_dict(best_bert_model_state)\n",
    "final_bert.eval()\n",
    "\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "from time import time\n",
    "start_time = time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in bert_test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        logits = final_bert(input_ids, attention_mask)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "inference_time = time() - start_time\n",
    "\n",
    "test_f1 = f1_score(test_labels, test_preds, average='macro')\n",
    "\n",
    "print(f\"\\nF1-Macro Final BERT (Test Set): {test_f1:.4f}\")\n",
    "print(f\"Tiempo de Inferencia total: {inference_time:.3f} segundos\")\n",
    "print(\"\\nReporte de Clasificación:\\n\", classification_report(test_labels, test_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
