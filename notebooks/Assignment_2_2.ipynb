{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de207790-3430-4e53-9ecc-c9cabc3463ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from time import time, sleep\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "# NLTK downloads\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Configurar semilla para reproducibilidad\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fbd1b65-9dc3-47f6-835b-5eba4a2c6f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "\n",
      "Total Samples - Training: 6920\n",
      "final_label_str\n",
      "YES    3553\n",
      "NO     3367\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total Samples - Validation (Will be used as TEST): 1038\n",
      "final_label_str\n",
      "YES    559\n",
      "NO     479\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_and_parse_data(filepath):\n",
    "    \"\"\"\n",
    "    Parses nested JSON and applies Majority Voting for labels.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    df = df.reset_index(drop=True).rename(columns={'index': 'id_EXIST'})\n",
    "    \n",
    "    # Label Processing (Majority Voting)\n",
    "    if 'labels_task1_1' in df.columns:\n",
    "        def get_majority_vote(labels_list):\n",
    "            if not isinstance(labels_list, list): return np.nan\n",
    "            counts = pd.Series(labels_list).value_counts()\n",
    "            # Tie-breaking: Prioritize 'YES' (Sexism) if tie\n",
    "            if len(counts) > 1 and counts.iloc[0] == counts.iloc[1]:\n",
    "                if 'YES' in counts.index[:2]: return 'YES'\n",
    "            return counts.idxmax()\n",
    "        \n",
    "        df['final_label_str'] = df['labels_task1_1'].apply(get_majority_vote)\n",
    "        df['label'] = df['final_label_str'].map({'YES': 1, 'NO': 0})\n",
    "        df = df.dropna(subset=['label'])\n",
    "        df['label'] = df['label'].astype(int)\n",
    "        \n",
    "    return df\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "# Ajusta las rutas según donde tengas tus datos\n",
    "df_train = load_and_parse_data('../data/training/EXIST2025_training.json')\n",
    "df_val = load_and_parse_data('../data/dev/EXIST2025_dev.json')\n",
    "# El test original no tiene labels, pero lo cargamos por si acaso\n",
    "df_test = load_and_parse_data('../data/test/EXIST2025_test_clean.json')\n",
    "\n",
    "print(f\"\\nTotal Samples - Training: {len(df_train)}\")\n",
    "print(df_train['final_label_str'].value_counts())\n",
    "print(f\"\\nTotal Samples - Validation (Will be used as TEST): {len(df_val)}\")\n",
    "print(df_val['final_label_str'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada5f9c9-f261-45c0-8435-c53af7017e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) | set(stopwords.words('spanish'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text, strategy='raw'):\n",
    "    text_processed = str(text)\n",
    "    if strategy == 'raw':\n",
    "        return text_processed\n",
    "    if strategy == 'lowercase':\n",
    "        return text_processed.lower()\n",
    "    if strategy == 'no_punct':\n",
    "        text_processed = re.sub(r'[^\\w\\s]', '', text_processed)\n",
    "        return text_processed.lower()\n",
    "    if strategy == 'no_stopwords':\n",
    "        text_processed = text_processed.lower()\n",
    "        words = text_processed.split()\n",
    "        return \" \".join([w for w in words if w not in stop_words])\n",
    "    if strategy == 'stemmed':\n",
    "        text_processed = text_processed.lower()\n",
    "        words = text_processed.split()\n",
    "        return \" \".join([stemmer.stem(w) for w in words])\n",
    "    if strategy == 'lemmatized':\n",
    "        text_processed = text_processed.lower()\n",
    "        words = text_processed.split() \n",
    "        return \" \".join([lemmatizer.lemmatize(w) for w in words])\n",
    "    return text_processed\n",
    "\n",
    "# Limpiamos textos\n",
    "df_train['text_clean'] = df_train['tweet'].apply(lambda x: preprocess_text(x, 'lowercase'))\n",
    "df_val['text_clean'] = df_val['tweet'].apply(lambda x: preprocess_text(x, 'lowercase'))\n",
    "df_test['text_clean'] = df_test['tweet'].apply(lambda x: preprocess_text(x, 'lowercase'))\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=2):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.min_freq = min_freq\n",
    "        \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 2\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                if frequencies[word] == self.min_freq:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "                    \n",
    "    def tokenize(self, text):\n",
    "        return re.findall(r'\\w+', text)\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokenized_text]\n",
    "\n",
    "vocab = Vocabulary(min_freq=2)\n",
    "vocab.build_vocabulary(df_train['text_clean'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fff00d06-244c-4acf-b22c-720de46a5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXISTDataset(Dataset):\n",
    "    def __init__(self, df, vocab, max_len=64):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.iloc[index]['text_clean']\n",
    "        label = self.df.iloc[index]['label'] if 'label' in self.df.columns else -1\n",
    "        \n",
    "        tokens = self.vocab.numericalize(text)\n",
    "        length = len(tokens)\n",
    "        \n",
    "        # Evitar secuencias vacías (rompen el pack_padded_sequence)\n",
    "        if length == 0:\n",
    "            tokens = [self.vocab.stoi[\"<UNK>\"]]\n",
    "            length = 1\n",
    "            \n",
    "        # Truncar si es más largo que max_len\n",
    "        if length > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "            length = self.max_len\n",
    "            \n",
    "        # Rellenar (Padding)\n",
    "        padded_tokens = tokens + [self.vocab.stoi[\"<PAD>\"]] * (self.max_len - length)\n",
    "            \n",
    "        return torch.tensor(padded_tokens), torch.tensor(label, dtype=torch.long), torch.tensor(length, dtype=torch.long)\n",
    "\n",
    "# Dataset global de Train (se dividirá en Folds)\n",
    "train_dataset = EXISTDataset(df_train, vocab)\n",
    "\n",
    "# El dataset \"dev\" original será nuestro \"TEST\" real\n",
    "test_dataset_real = EXISTDataset(df_val, vocab)\n",
    "test_loader_real = DataLoader(test_dataset_real, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b94b92ec-de05-46fe-b33d-c5863ce334c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout=0.5):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n",
    "                           bidirectional=True, batch_first=True, \n",
    "                           dropout=dropout if n_layers > 1 else 0)\n",
    "        \n",
    "        # El tamaño es hidden_dim * 2 por ser bidireccional\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        # Empaquetar secuencias para ignorar el padding en el LSTM\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # LSTM processing\n",
    "        packed_output, (hidden, cell) = self.lstm(packed)\n",
    "        \n",
    "        # hidden tiene forma: [num_layers * 2, batch_size, hidden_dim]\n",
    "        # Concatenamos los estados ocultos de la última capa forward y backward\n",
    "        forward_hidden = hidden[-2]\n",
    "        backward_hidden = hidden[-1]\n",
    "        final_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "        \n",
    "        # Clasificación\n",
    "        output = self.dropout(final_hidden)\n",
    "        logits = self.fc(output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_score, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "            self.best_model_state = model.state_dict().copy()\n",
    "        # Evaluamos con F1-Macro, por lo que buscamos que el score SUBA\n",
    "        elif val_score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.best_model_state = model.state_dict().copy()\n",
    "            self.counter = 0\n",
    "            \n",
    "        return self.early_stop\n",
    "\n",
    "# Configuración de arquitectura\n",
    "VOCAB_SIZE = len(vocab.stoi)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 2\n",
    "N_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c7d89e2-7dc0-4acd-b7f8-3c5587afbaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento en: cuda\n",
      "\n",
      "================ FOLD 1/5 ================\n",
      "Epoch 01/20 | Train Loss: 0.6773 | Train F1: 0.5679 | Val Loss: 0.6591 | Val F1: 0.5863\n",
      "Epoch 02/20 | Train Loss: 0.6400 | Train F1: 0.6267 | Val Loss: 0.6171 | Val F1: 0.6473\n",
      "Epoch 03/20 | Train Loss: 0.6025 | Train F1: 0.6784 | Val Loss: 0.6043 | Val F1: 0.6597\n",
      "Epoch 04/20 | Train Loss: 0.5682 | Train F1: 0.7029 | Val Loss: 0.6171 | Val F1: 0.6720\n",
      "Epoch 05/20 | Train Loss: 0.5243 | Train F1: 0.7347 | Val Loss: 0.6275 | Val F1: 0.6747\n",
      "Epoch 06/20 | Train Loss: 0.4948 | Train F1: 0.7597 | Val Loss: 0.6746 | Val F1: 0.6712\n",
      "Epoch 07/20 | Train Loss: 0.4691 | Train F1: 0.7780 | Val Loss: 0.6784 | Val F1: 0.6906\n",
      "Epoch 08/20 | Train Loss: 0.4320 | Train F1: 0.8012 | Val Loss: 0.7056 | Val F1: 0.6961\n",
      "Epoch 09/20 | Train Loss: 0.3971 | Train F1: 0.8216 | Val Loss: 0.7375 | Val F1: 0.6944\n",
      "Epoch 10/20 | Train Loss: 0.3719 | Train F1: 0.8357 | Val Loss: 0.8967 | Val F1: 0.7000\n",
      "Epoch 11/20 | Train Loss: 0.3329 | Train F1: 0.8548 | Val Loss: 0.8100 | Val F1: 0.7038\n",
      "Epoch 12/20 | Train Loss: 0.3155 | Train F1: 0.8628 | Val Loss: 0.9178 | Val F1: 0.6956\n",
      "Epoch 13/20 | Train Loss: 0.2957 | Train F1: 0.8778 | Val Loss: 0.9289 | Val F1: 0.6948\n",
      "Epoch 14/20 | Train Loss: 0.2701 | Train F1: 0.8843 | Val Loss: 1.0682 | Val F1: 0.6901\n",
      "Epoch 15/20 | Train Loss: 0.2574 | Train F1: 0.8972 | Val Loss: 0.9826 | Val F1: 0.6835\n",
      "Epoch 16/20 | Train Loss: 0.2413 | Train F1: 0.9004 | Val Loss: 1.1603 | Val F1: 0.7088\n",
      "Epoch 17/20 | Train Loss: 0.2281 | Train F1: 0.9029 | Val Loss: 1.2299 | Val F1: 0.6960\n",
      "Epoch 18/20 | Train Loss: 0.2092 | Train F1: 0.9172 | Val Loss: 1.2204 | Val F1: 0.7081\n",
      "Epoch 19/20 | Train Loss: 0.2006 | Train F1: 0.9219 | Val Loss: 1.1517 | Val F1: 0.6983\n",
      "Epoch 20/20 | Train Loss: 0.1885 | Train F1: 0.9254 | Val Loss: 1.2801 | Val F1: 0.6955\n",
      "\n",
      "Mejor F1-Macro en Fold 1: 0.7088\n",
      "\n",
      "================ FOLD 2/5 ================\n",
      "Epoch 01/20 | Train Loss: 0.6835 | Train F1: 0.5550 | Val Loss: 0.6715 | Val F1: 0.5884\n",
      "Epoch 02/20 | Train Loss: 0.6483 | Train F1: 0.6249 | Val Loss: 0.6562 | Val F1: 0.6329\n",
      "Epoch 03/20 | Train Loss: 0.6067 | Train F1: 0.6771 | Val Loss: 0.6098 | Val F1: 0.6608\n",
      "Epoch 04/20 | Train Loss: 0.5809 | Train F1: 0.6954 | Val Loss: 0.6124 | Val F1: 0.6745\n",
      "Epoch 05/20 | Train Loss: 0.5457 | Train F1: 0.7330 | Val Loss: 0.6110 | Val F1: 0.6828\n",
      "Epoch 06/20 | Train Loss: 0.5057 | Train F1: 0.7549 | Val Loss: 0.6023 | Val F1: 0.6904\n",
      "Epoch 07/20 | Train Loss: 0.4744 | Train F1: 0.7741 | Val Loss: 0.6281 | Val F1: 0.6805\n",
      "Epoch 08/20 | Train Loss: 0.4499 | Train F1: 0.7962 | Val Loss: 0.6792 | Val F1: 0.7072\n",
      "Epoch 09/20 | Train Loss: 0.4133 | Train F1: 0.8133 | Val Loss: 0.7449 | Val F1: 0.6950\n",
      "Epoch 10/20 | Train Loss: 0.3839 | Train F1: 0.8312 | Val Loss: 0.8498 | Val F1: 0.6906\n",
      "Epoch 11/20 | Train Loss: 0.3582 | Train F1: 0.8443 | Val Loss: 0.7403 | Val F1: 0.7038\n",
      "Epoch 12/20 | Train Loss: 0.3322 | Train F1: 0.8536 | Val Loss: 0.7680 | Val F1: 0.6987\n",
      "Epoch 13/20 | Train Loss: 0.3154 | Train F1: 0.8660 | Val Loss: 0.8836 | Val F1: 0.7072\n",
      "--> Early stopping activado en la época 13\n",
      "\n",
      "Mejor F1-Macro en Fold 2: 0.7072\n",
      "\n",
      "================ FOLD 3/5 ================\n",
      "Epoch 01/20 | Train Loss: 0.6779 | Train F1: 0.5662 | Val Loss: 0.6485 | Val F1: 0.6057\n",
      "Epoch 02/20 | Train Loss: 0.6465 | Train F1: 0.6292 | Val Loss: 0.6240 | Val F1: 0.6474\n",
      "Epoch 03/20 | Train Loss: 0.6170 | Train F1: 0.6621 | Val Loss: 0.6273 | Val F1: 0.6462\n",
      "Epoch 04/20 | Train Loss: 0.5944 | Train F1: 0.6891 | Val Loss: 0.6460 | Val F1: 0.6528\n",
      "Epoch 05/20 | Train Loss: 0.5644 | Train F1: 0.7121 | Val Loss: 0.5955 | Val F1: 0.6833\n",
      "Epoch 06/20 | Train Loss: 0.5277 | Train F1: 0.7400 | Val Loss: 0.5935 | Val F1: 0.7008\n",
      "Epoch 07/20 | Train Loss: 0.5033 | Train F1: 0.7542 | Val Loss: 0.5914 | Val F1: 0.7110\n",
      "Epoch 08/20 | Train Loss: 0.4661 | Train F1: 0.7812 | Val Loss: 0.6141 | Val F1: 0.7153\n",
      "Epoch 09/20 | Train Loss: 0.4409 | Train F1: 0.7986 | Val Loss: 0.6048 | Val F1: 0.7196\n",
      "Epoch 10/20 | Train Loss: 0.4090 | Train F1: 0.8146 | Val Loss: 0.6782 | Val F1: 0.7205\n",
      "Epoch 11/20 | Train Loss: 0.3748 | Train F1: 0.8359 | Val Loss: 0.6591 | Val F1: 0.7180\n",
      "Epoch 12/20 | Train Loss: 0.3543 | Train F1: 0.8456 | Val Loss: 0.7503 | Val F1: 0.7268\n",
      "Epoch 13/20 | Train Loss: 0.3308 | Train F1: 0.8601 | Val Loss: 0.8522 | Val F1: 0.7176\n",
      "Epoch 14/20 | Train Loss: 0.3029 | Train F1: 0.8764 | Val Loss: 0.7452 | Val F1: 0.7212\n",
      "Epoch 15/20 | Train Loss: 0.2949 | Train F1: 0.8762 | Val Loss: 0.9039 | Val F1: 0.7163\n",
      "Epoch 16/20 | Train Loss: 0.2606 | Train F1: 0.8902 | Val Loss: 0.9512 | Val F1: 0.7128\n",
      "Epoch 17/20 | Train Loss: 0.2469 | Train F1: 0.8980 | Val Loss: 0.9699 | Val F1: 0.7268\n",
      "--> Early stopping activado en la época 17\n",
      "\n",
      "Mejor F1-Macro en Fold 3: 0.7268\n",
      "\n",
      "================ FOLD 4/5 ================\n",
      "Epoch 01/20 | Train Loss: 0.6857 | Train F1: 0.5477 | Val Loss: 0.6597 | Val F1: 0.5720\n",
      "Epoch 02/20 | Train Loss: 0.6576 | Train F1: 0.6057 | Val Loss: 0.6281 | Val F1: 0.6488\n",
      "Epoch 03/20 | Train Loss: 0.6395 | Train F1: 0.6267 | Val Loss: 0.6111 | Val F1: 0.6611\n",
      "Epoch 04/20 | Train Loss: 0.6065 | Train F1: 0.6703 | Val Loss: 0.5983 | Val F1: 0.6663\n",
      "Epoch 05/20 | Train Loss: 0.5751 | Train F1: 0.6979 | Val Loss: 0.6153 | Val F1: 0.7021\n",
      "Epoch 06/20 | Train Loss: 0.5349 | Train F1: 0.7314 | Val Loss: 0.5772 | Val F1: 0.7130\n",
      "Epoch 07/20 | Train Loss: 0.5012 | Train F1: 0.7606 | Val Loss: 0.5754 | Val F1: 0.7166\n",
      "Epoch 08/20 | Train Loss: 0.4765 | Train F1: 0.7768 | Val Loss: 0.6390 | Val F1: 0.7124\n",
      "Epoch 09/20 | Train Loss: 0.4389 | Train F1: 0.8025 | Val Loss: 0.6144 | Val F1: 0.7315\n",
      "Epoch 10/20 | Train Loss: 0.4140 | Train F1: 0.8169 | Val Loss: 0.6215 | Val F1: 0.7326\n",
      "Epoch 11/20 | Train Loss: 0.3841 | Train F1: 0.8296 | Val Loss: 0.7182 | Val F1: 0.7340\n",
      "Epoch 12/20 | Train Loss: 0.3569 | Train F1: 0.8399 | Val Loss: 0.7167 | Val F1: 0.7514\n",
      "Epoch 13/20 | Train Loss: 0.3302 | Train F1: 0.8540 | Val Loss: 0.7204 | Val F1: 0.7427\n",
      "Epoch 14/20 | Train Loss: 0.3129 | Train F1: 0.8677 | Val Loss: 0.8142 | Val F1: 0.7338\n",
      "Epoch 15/20 | Train Loss: 0.2849 | Train F1: 0.8834 | Val Loss: 0.9560 | Val F1: 0.7389\n",
      "Epoch 16/20 | Train Loss: 0.2681 | Train F1: 0.8861 | Val Loss: 0.8600 | Val F1: 0.7368\n",
      "Epoch 17/20 | Train Loss: 0.2560 | Train F1: 0.8964 | Val Loss: 0.9234 | Val F1: 0.7378\n",
      "--> Early stopping activado en la época 17\n",
      "\n",
      "Mejor F1-Macro en Fold 4: 0.7514\n",
      "\n",
      "================ FOLD 5/5 ================\n",
      "Epoch 01/20 | Train Loss: 0.6796 | Train F1: 0.5596 | Val Loss: 0.6545 | Val F1: 0.6029\n",
      "Epoch 02/20 | Train Loss: 0.6496 | Train F1: 0.6243 | Val Loss: 0.6373 | Val F1: 0.6264\n",
      "Epoch 03/20 | Train Loss: 0.6082 | Train F1: 0.6626 | Val Loss: 0.6316 | Val F1: 0.6564\n",
      "Epoch 04/20 | Train Loss: 0.5704 | Train F1: 0.7080 | Val Loss: 0.6363 | Val F1: 0.6685\n",
      "Epoch 05/20 | Train Loss: 0.5407 | Train F1: 0.7337 | Val Loss: 0.6160 | Val F1: 0.6726\n",
      "Epoch 06/20 | Train Loss: 0.4957 | Train F1: 0.7566 | Val Loss: 0.6922 | Val F1: 0.6643\n",
      "Epoch 07/20 | Train Loss: 0.4667 | Train F1: 0.7804 | Val Loss: 0.6577 | Val F1: 0.6858\n",
      "Epoch 08/20 | Train Loss: 0.4383 | Train F1: 0.7978 | Val Loss: 0.7166 | Val F1: 0.6894\n",
      "Epoch 09/20 | Train Loss: 0.3973 | Train F1: 0.8179 | Val Loss: 0.8861 | Val F1: 0.6778\n",
      "Epoch 10/20 | Train Loss: 0.3663 | Train F1: 0.8404 | Val Loss: 0.8756 | Val F1: 0.6783\n",
      "Epoch 11/20 | Train Loss: 0.3350 | Train F1: 0.8627 | Val Loss: 0.7918 | Val F1: 0.6869\n",
      "Epoch 12/20 | Train Loss: 0.3165 | Train F1: 0.8688 | Val Loss: 0.9699 | Val F1: 0.6818\n",
      "Epoch 13/20 | Train Loss: 0.2948 | Train F1: 0.8778 | Val Loss: 0.9933 | Val F1: 0.7010\n",
      "Epoch 14/20 | Train Loss: 0.2704 | Train F1: 0.8828 | Val Loss: 1.0239 | Val F1: 0.6823\n",
      "Epoch 15/20 | Train Loss: 0.2752 | Train F1: 0.8801 | Val Loss: 0.9167 | Val F1: 0.6986\n",
      "Epoch 16/20 | Train Loss: 0.2264 | Train F1: 0.9064 | Val Loss: 1.1947 | Val F1: 0.6961\n",
      "Epoch 17/20 | Train Loss: 0.2287 | Train F1: 0.9045 | Val Loss: 1.1140 | Val F1: 0.6903\n",
      "Epoch 18/20 | Train Loss: 0.2165 | Train F1: 0.9154 | Val Loss: 1.1269 | Val F1: 0.6917\n",
      "--> Early stopping activado en la época 18\n",
      "\n",
      "Mejor F1-Macro en Fold 5: 0.7010\n",
      "\n",
      "========================================\n",
      "F1-Macro Promedio Cross-Validation (5 Folds): 0.7190\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Iniciando entrenamiento en: {device}\\n\")\n",
    "\n",
    "EPOCHS = 20\n",
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "y_train_array = df_train['label'].values\n",
    "fold_metrics = []\n",
    "\n",
    "# Guardaremos el mejor modelo global del mejor fold (opcional, útil para la inferencia final)\n",
    "best_global_f1 = 0\n",
    "best_global_model_state = None\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(y_train_array)), y_train_array)):\n",
    "    print(f\"================ FOLD {fold + 1}/{N_SPLITS} ================\")\n",
    "    \n",
    "    train_sub = Subset(train_dataset, train_idx)\n",
    "    val_sub = Subset(train_dataset, val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_sub, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_sub, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model = BiLSTMClassifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    early_stopping = EarlyStopping(patience=5) \n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        all_train_preds, all_train_labels = [], []\n",
    "        \n",
    "        for texts, labels, lengths in train_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient Clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_train_preds.extend(predicted.cpu().numpy())\n",
    "            all_train_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "        train_f1 = f1_score(all_train_labels, all_train_preds, average='macro')\n",
    "            \n",
    "        # VALIDATION\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_val_preds, all_val_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for texts, labels, lengths in val_loader:\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                outputs = model(texts, lengths)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                all_val_preds.extend(predicted.cpu().numpy())\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='macro')\n",
    "        \n",
    "        print(f'Epoch {epoch+1:02d}/{EPOCHS} | Train Loss: {train_loss/len(train_loader):.4f} | Train F1: {train_f1:.4f} | Val Loss: {val_loss/len(val_loader):.4f} | Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        # Early Stopping\n",
    "        if early_stopping(val_f1, model):\n",
    "            print(f\"--> Early stopping activado en la época {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "    # Al final del Fold, guardamos el mejor F1 de este Fold\n",
    "    fold_best_f1 = early_stopping.best_score\n",
    "    fold_metrics.append(fold_best_f1)\n",
    "    print(f\"\\nMejor F1-Macro en Fold {fold+1}: {fold_best_f1:.4f}\\n\")\n",
    "    \n",
    "    # Trackeamos el mejor modelo de los 5 Folds para usarlo luego en Test\n",
    "    if fold_best_f1 > best_global_f1:\n",
    "        best_global_f1 = fold_best_f1\n",
    "        best_global_model_state = early_stopping.best_model_state.copy()\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(f\"F1-Macro Promedio Cross-Validation (5 Folds): {np.mean(fold_metrics):.4f}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea24bfd-47b9-4ef4-9267-a76e62759c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando el mejor modelo encontrado en el conjunto de TEST (Dev Original)...\n",
      "\n",
      "F1-Macro Final (Test Set): 0.7168\n",
      "Tiempo de Inferencia total: 0.486 segundos\n",
      "\n",
      "Matriz de Confusión:\n",
      " [[343 136]\n",
      " [157 402]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.72      0.70       479\n",
      "           1       0.75      0.72      0.73       559\n",
      "\n",
      "    accuracy                           0.72      1038\n",
      "   macro avg       0.72      0.72      0.72      1038\n",
      "weighted avg       0.72      0.72      0.72      1038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluando el mejor modelo encontrado en el conjunto de TEST (Dev Original)...\")\n",
    "\n",
    "# Cargar el mejor modelo de todos los Folds\n",
    "final_model = BiLSTMClassifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS).to(device)\n",
    "final_model.load_state_dict(best_global_model_state)\n",
    "final_model.eval()\n",
    "\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "start_time = time()\n",
    "with torch.no_grad():\n",
    "    for texts, labels, lengths in test_loader_real:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = final_model(texts, lengths)\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "inference_time = time() - start_time\n",
    "\n",
    "test_f1 = f1_score(test_labels, test_preds, average='macro')\n",
    "\n",
    "print(f\"\\nF1-Macro Final (Test Set): {test_f1:.4f}\")\n",
    "print(f\"Tiempo de Inferencia total: {inference_time:.3f} segundos\")\n",
    "print(\"\\nMatriz de Confusión:\\n\", confusion_matrix(test_labels, test_preds))\n",
    "print(\"\\nReporte de Clasificación:\\n\", classification_report(test_labels, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c51f5-ec23-4d8b-8d77-a05729ca3b1a",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "621a4a24-90e5-4ec3-823c-58d0fcc657d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2fc9a171a247558f42a3467b6bd9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00199a004b0b472e919ce4fb6331a2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb861e5459541498b574643c170123f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ef6a16fdb94f82ac8c85ca360ae7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Usaremos un modelo base ligero, ideal para empezar.\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f36b59c-a155-42f8-b1c8-0e1f12c45305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # Llamada directa al tokenizador (la forma moderna y estándar)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Preparamos los textos crudos\n",
    "X_train_bert = df_train['tweet']\n",
    "y_train_bert = df_train['label']\n",
    "\n",
    "X_test_bert = df_val['tweet'] \n",
    "y_test_bert = df_val['label']\n",
    "\n",
    "bert_test_dataset = BERTDataset(X_test_bert, y_test_bert, tokenizer)\n",
    "bert_test_loader = DataLoader(bert_test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50498595-6fac-44df-b3d6-92c9f5fe8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, freeze_bert=False):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        \n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Usamos el token [CLS] (primera posición)\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "def unfreeze_last_n_layers(model, n):\n",
    "    \"\"\"Utilidad para tu Estudio de Ablación\"\"\"\n",
    "    # Congelar todo primero\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Descongelar las últimas n capas del encoder\n",
    "    if n > 0:\n",
    "        for layer in model.bert.encoder.layer[-n:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "    # La cabeza de clasificación siempre debe poder entrenarse\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156197d-3fc4-41e7-8b47-2431f84ba1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando BERT en: cuda\n",
      "\n",
      "================ BERT FOLD 1/5 ================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322cfd3fdd9a440fae040aaba62b4fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 | Train Loss: 0.5995 | Val Loss: 0.5007 | Val F1: 0.7452\n",
      "Epoch 2/4 | Train Loss: 0.4460 | Val Loss: 0.5136 | Val F1: 0.7764\n",
      "Epoch 3/4 | Train Loss: 0.3107 | Val Loss: 0.5435 | Val F1: 0.7879\n",
      "Epoch 4/4 | Train Loss: 0.2146 | Val Loss: 0.6583 | Val F1: 0.7876\n",
      "\n",
      "Mejor F1-Macro en Fold 1: 0.7879\n",
      "\n",
      "================ BERT FOLD 2/5 ================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a833f872902342eeadc2b11aa488b499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Entrenando BERT en: {device}\\n\")\n",
    "\n",
    "# Parámetros específicos para Fine-Tuning según el PDF\n",
    "EPOCHS_BERT = 4 \n",
    "BATCH_SIZE = 16 \n",
    "N_SPLITS = 5\n",
    "\n",
    "skf_bert = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "y_train_array_bert = y_train_bert.values\n",
    "bert_fold_metrics = []\n",
    "best_bert_global_f1 = 0\n",
    "best_bert_model_state = None\n",
    "\n",
    "bert_full_dataset = BERTDataset(X_train_bert, y_train_bert, tokenizer)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf_bert.split(np.zeros(len(y_train_array_bert)), y_train_array_bert)):\n",
    "    print(f\"================ BERT FOLD {fold + 1}/{N_SPLITS} ================\")\n",
    "    \n",
    "    train_sub = Subset(bert_full_dataset, train_idx)\n",
    "    val_sub = Subset(bert_full_dataset, val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_sub, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_sub, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Instanciar el modelo (Full fine-tuning inicial)\n",
    "    model = BERTClassifier(MODEL_NAME, num_classes=2, freeze_bert=False).to(device)\n",
    "    \n",
    "    # Diferentes learning rates: bajo para BERT, más alto para la nueva capa (Exigencia del PDF)\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.bert.parameters(), 'lr': 2e-5},\n",
    "        {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
    "    ])\n",
    "    \n",
    "    # Scheduler con Warmup (10% de los pasos totales)\n",
    "    total_steps = len(train_loader) * EPOCHS_BERT\n",
    "    warmup_steps = int(total_steps * 0.1)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_fold_f1 = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS_BERT):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        all_train_preds, all_train_labels = [], []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (max_norm = 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step() # Actualizar learning rate\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            all_train_preds.extend(predicted.cpu().numpy())\n",
    "            all_train_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_val_preds, all_val_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                logits = model(input_ids, attention_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                all_val_preds.extend(predicted.cpu().numpy())\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='macro')\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS_BERT} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f} | Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        # Guardar el mejor modelo del Fold\n",
    "        if val_f1 > best_fold_f1:\n",
    "            best_fold_f1 = val_f1\n",
    "            best_model_state_for_this_fold = model.state_dict().copy()\n",
    "            \n",
    "    bert_fold_metrics.append(best_fold_f1)\n",
    "    print(f\"\\nMejor F1-Macro en Fold {fold+1}: {best_fold_f1:.4f}\\n\")\n",
    "    \n",
    "    if best_fold_f1 > best_bert_global_f1:\n",
    "        best_bert_global_f1 = best_fold_f1\n",
    "        best_bert_model_state = best_model_state_for_this_fold\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(f\"F1-Macro Promedio BERT (5 Folds): {np.mean(bert_fold_metrics):.4f}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba30b92-ccc8-4508-9b07-e69328b2613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluando el mejor modelo BERT en el conjunto de TEST...\")\n",
    "\n",
    "final_bert = BERTClassifier(MODEL_NAME, num_classes=2).to(device)\n",
    "final_bert.load_state_dict(best_bert_model_state)\n",
    "final_bert.eval()\n",
    "\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "from time import time\n",
    "start_time = time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in bert_test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        logits = final_bert(input_ids, attention_mask)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "inference_time = time() - start_time\n",
    "\n",
    "test_f1 = f1_score(test_labels, test_preds, average='macro')\n",
    "\n",
    "print(f\"\\nF1-Macro Final BERT (Test Set): {test_f1:.4f}\")\n",
    "print(f\"Tiempo de Inferencia total: {inference_time:.3f} segundos\")\n",
    "print(\"\\nReporte de Clasificación:\\n\", classification_report(test_labels, test_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
