{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b8f018-e5ce-4444-a1f8-bb2b7dc5f71d",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## 1.1 Dataset Description and Source\n",
    "\n",
    "### Origin and Creation Context\n",
    "\n",
    "The data for this notebook originates from **EXIST 2025** (*sEXism Identification in Social neTworks*, fifth edition), a scientific shared task organized as a Lab within the **CLEF 2025 conference** (Conference and Labs of the Evaluation Forum), held on September 9–12, 2025, at UNED, Madrid, Spain. EXIST is a recurring series of evaluation campaigns, active since 2021, dedicated to fostering automatic detection of sexism in social media — ranging from explicit misogyny to subtle, implicit sexist behaviours.\n",
    "\n",
    "The dataset used in this notebook corresponds specifically to **Task 1 (Sexism Detection in Tweets), Subtask 1.1 (Sexism Identification in Tweets)**, and consists of posts collected from **X/Twitter** in two languages: **English and Spanish**. The tweets were gathered using seed hashtags associated with potentially sexist content, and the collection strategy was designed to ensure broad topical and temporal coverage. The corpus contains **over 10,000 tweets** split into three official partitions:\n",
    "\n",
    "| Split       | Size (posts) |\n",
    "|-------------|-------------|\n",
    "| Training    | 6,920        |\n",
    "| Development | 1,038        |\n",
    "| Test        | 2,076        |\n",
    "\n",
    "**Annotation process.** Each tweet was independently labelled by **six human annotators** recruited and managed via Servipoli's annotation service at UPV (Universitat Politècnica de València). Annotators received task-specific training through multiple guided sessions before labelling began. Demographic metadata for annotators — including gender, age group, ethnicity, education level, and country of origin — is provided alongside the labels, enabling fairness-aware modelling and analysis of annotator subjectivity. Crucially, the dataset is released with **all individual annotations preserved** rather than a single adjudicated label, in keeping with the Learning with Disagreement (LeWiDi) paradigm adopted since EXIST 2023. This design choice reflects the organisers' position that inter-annotator disagreement on subjective tasks such as sexism identification is a meaningful signal, not noise to be discarded.\n",
    "\n",
    "### Access\n",
    "\n",
    "Access to all EXIST datasets (2021–2025) is granted for research purposes upon completing the official request form provided by the organisers:\n",
    "\n",
    "> Request form: https://forms.office.com/e/ikGpvRQ1qv\n",
    "\n",
    "### Full Academic Citations\n",
    "\n",
    "**Primary task overview paper (LNCS proceedings):**\n",
    "\n",
    "> Plaza, L., Carrillo-de-Albornoz, J., Arcos, I., Rosso, P., Spina, D., Amigó, E., Gonzalo, J., & Morante, R. (2025). Overview of EXIST 2025: Learning with Disagreement for Sexism Identification and Characterization in Tweets, Memes, and TikTok Videos. In J. Carrillo-De-Albornoz et al. (Eds.), *Experimental IR Meets Multilinguality, Multimodality, and Interaction. CLEF 2025*. Lecture Notes in Computer Science, vol. 16089. Springer, Cham. https://link.springer.com/book/10.1007/978-3-032-04354-2\n",
    "\n",
    "**Extended overview paper (CEUR Working Notes):**\n",
    "\n",
    "> Plaza, L., Carrillo-de-Albornoz, J., Arcos, I., Rosso, P., Spina, D., Amigó, E., Gonzalo, J., & Morante, R. (2025). Overview of EXIST 2025: Learning with Disagreement for Sexism Identification and Characterization in Tweets, Memes, and TikTok Videos (Extended Overview). In G. Faggioli et al. (Eds.), *Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2025)*. CEUR Workshop Proceedings, Vol-4038. https://ceur-ws.org/Vol-4038/paper_135.pdf\n",
    "\n",
    "**Official evaluation metric:**\n",
    "\n",
    "> Amigó, E., & Delgado, A. (2022). Evaluating Extreme Hierarchical Multi-label Classification. In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)* (pp. 5809–5819). Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.acl-long.399\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 Task Definition and Label Meanings\n",
    "\n",
    "### Shared Task Context\n",
    "\n",
    "| Field | Detail |\n",
    "|-------|--------|\n",
    "| **Task name** | EXIST 2025, Task 1 — Subtask 1.1: Sexism Identification in Tweets |\n",
    "| **Year** | 2025 |\n",
    "| **Organizing venue** | CLEF 2025 (Conference and Labs of the Evaluation Forum), UNED, Madrid, Spain |\n",
    "| **Task website** | https://nlp.uned.es/exist2025/ |\n",
    "\n",
    "### Classification Task\n",
    "\n",
    "Subtask 1.1 is a **binary text classification** task. Given a tweet written in English or Spanish, a system must determine whether the tweet contains sexist content — understood in a broad sense as any expression, attitude, or behaviour that discriminates against, demeans, or subordinates individuals on the basis of gender.\n",
    "\n",
    "The breadth of this definition is intentional: a tweet qualifies as sexist not only when it actively promotes sexist views, but also when it describes or reports a sexist situation experienced by the author, or when it critically comments on sexist behaviour, provided the content itself involves or depicts sexist discourse. This inclusivity makes the class boundary non-trivial and inherently subjective, motivating the multi-annotator design described above.\n",
    "\n",
    "### Label Meanings\n",
    "\n",
    "| Label | Real-World Meaning |\n",
    "|-------|-------------------|\n",
    "| **YES** *(sexist)* | The tweet *contains* sexist expressions, attitudes, or behaviours directed at individuals on the basis of gender. This encompasses content that is overtly misogynistic or harassing; content that reinforces gender-based stereotypes or power imbalances; content that recounts a sexist experience, even when doing so critically; and content that deploys irony or humour to convey sexist ideas. The label captures sexism in its broadest social sense — from blatant verbal aggression to the subtle normalisation of gender inequality. |\n",
    "| **NO** *(not sexist)* | The tweet does not contain sexist expressions or behaviours. Its content is neutral with respect to gender-based discrimination, or, if it touches on gender topics, it does so without reproducing or endorsing sexist attitudes. |\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 Dataset Challenges\n",
    "\n",
    "This dataset presents several interlocking difficulties that make it substantially harder than standard binary text classification benchmarks.\n",
    "\n",
    "### Subjectivity and Legitimate Annotator Disagreement\n",
    "\n",
    "The most fundamental challenge is that sexism detection is **inherently subjective**. The same tweet can be read as sexist or ironic, as a joke or as normalisation of gender inequality, depending on the cultural background, personal experience, and social sensitivity of the reader. This is not annotation noise — it is a genuine property of the phenomenon.\n",
    "\n",
    "### Borderline Class: \"Judgemental\" Tweets\n",
    "\n",
    "A particularly difficult subcategory involves tweets that **condemn** sexist behaviour. Under the task definition these are labelled YES (sexist) because they depict sexist situations — but their surface form is often indistinguishable from clearly NOT SEXIST content. A tweet reading *\"disgusting that my colleague was passed over for promotion because she's a woman\"* is anti-sexist in intent, yet is labelled SEXIST because it describes gender discrimination. This label assignment, while principled, is counter-intuitive and a systematic source of model errors.\n",
    "\n",
    "### Seed-Induced Topical Bias\n",
    "\n",
    "The corpus was constructed using hashtag seeds, which means tweets are topically skewed toward discussions *about* gender and sexism. NOT SEXIST tweets in this dataset are not generic neutral content — they are non-sexist tweets harvested from the same gender-adjacent hashtag space. This makes the negative class more challenging than it appears: a model cannot simply learn to associate gender-related vocabulary with the SEXIST label.\n",
    "\n",
    "### Code-Switching, Irony, and Platform-Specific Language\n",
    "\n",
    "Tweets are characterised by:\n",
    "- **Extreme brevity** (280 characters maximum), which limits contextual information. Meaning often depends on pragmatic implication, hashtag context, or tone that is absent in text form.\n",
    "- **Irony and sarcasm**, which systematically mislead literal classifiers. A tweet such as *\"Haha what would women do without men to open jars for them\"* carries its sexism through ironic framing rather than literal proposition.\n",
    "- **Non-standard language**: creative spelling, emoji, hashtag concatenation (e.g., `#adaywithoutwomen`), abbreviations, and code-switching between English and informal slang are all frequent and may not be well-handled by standard tokenization.\n",
    "- **Multilingual content**: the bilingual (English/Spanish) design means any single-language model will fail on the complementary language, and cross-lingual generalisation is non-trivial.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4 Sample Examples\n",
    "\n",
    "The following examples are taken directly from the official task description (Plaza et al., 2025) and illustrate representative cases from each class. They are chosen to convey the range of difficulty in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Label: YES (Sexist)\n",
    "\n",
    "**Example S-1** — *Direct sexism through anti-feminist framing (ironic/ideological):*\n",
    "> *\"It's less of #adaywithoutwomen and more of a day without feminists, which, to be quite honest, sounds lovely.\"*\n",
    "\n",
    "The sexism here is not expressed through slurs but through the dismissal of women's collective action as something undesirable. The use of a real social movement hashtag contextualises the tweet as a response to feminist activism. The ironic register makes it harder to detect automatically.\n",
    "\n",
    "---\n",
    "\n",
    "**Example S-2** — *Direct sexism through gender-role prescription:*\n",
    "> *\"A woman needs love, to fill the fridge, if a man can give this to her in return for her services, I don't see what else she needs.\"*\n",
    "\n",
    "This tweet encodes a transactional view of gender relations and reduces women's needs to domestic and emotional dependencies. Sexism is explicit here, but still expressed through implication rather than slurs.\n",
    "\n",
    "---\n",
    "\n",
    "**Example S-3** — *Reported/judgemental sexism — author is a victim recounting harassment:*\n",
    "> *\"I doze in the subway, I open my eyes feeling something weird: the hand of the man sat next to me on my leg #SquealOnYourPig.\"*\n",
    "\n",
    "This tweet is labelled SEXIST because it describes a sexist assault — not because the author holds sexist views. It is the most counter-intuitive class for models: surface-level, it resembles a first-person narrative by a victim; semantically, it depicts gendered violence. The hashtag `#SquealOnYourPig` signals a protest context.\n",
    "\n",
    "---\n",
    "\n",
    "### Label: NO (Not Sexist)\n",
    "\n",
    "**Example N-1** — *Neutral observation involving a woman, no gendered framing:*\n",
    "> *\"Just saw a woman wearing a mask outside spank her very tightly leashed dog and I gotta say I love learning absolutely everything about a stranger in a single instant.\"*\n",
    "\n",
    "The tweet mentions a woman but treats her as an individual acting in a particular way — there is no generalisation, subordination, or gender-based framing. The classifier must learn not to fire on mentions of women per se.\n",
    "\n",
    "---\n",
    "\n",
    "**Example N-2** — *Judgemental anti-sexism tweet condemning gender inequality:*\n",
    "> *\"As usual, the woman was the one quitting her job for the family's welfare...\"*\n",
    "\n",
    "This example illustrates a key decision boundary challenge. The ellipsis conveys implicit criticism of a sexist social norm, but does not endorse it. Under the EXIST annotation guidelines this tweet is **NOT SEXIST** because the author's intention is condemnation — in contrast to Example S-3, where the depiction of a sexist act tips the label toward SEXIST. The difference is subtle and annotators reliably disagree on such cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94a41f-88f0-47f0-b6c6-bb661954d3ba",
   "metadata": {},
   "source": [
    "# 2. About this Notebook\n",
    "\n",
    "This notebook establishes a reproducible **baseline** for **EXIST 2025 — Subtask 1.1 (Sexism Identification in Tweets)**, providing a controlled reference point against which future deep learning and transformer-based models can be compared.\n",
    "\n",
    "## 2.1 Objective\n",
    "\n",
    "The primary goal is to determine how well classical, non-neural NLP pipelines can perform on the sexism detection task described in §1. By systematically exploring preprocessing strategies, feature representations, and model hyperparameters, we aim to:\n",
    "\n",
    "1. **Quantify baseline performance** using traditional machine learning classifiers (Logistic Regression and Linear SVM) on this inherently subjective task.\n",
    "2. **Identify the best-performing configuration** across two families of text representations — sparse (TF-IDF) and dense (FastText embeddings) — through controlled ablation studies and grid search.\n",
    "3. **Generate official-format predictions** on the held-out test set for submission to the EXIST 2025 evaluation campaign."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c492e38c-beed-4a92-95dc-e5856da942d7",
   "metadata": {},
   "source": [
    "## 2.2 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc7b3c-37d8-4860-a759-48e7256eeeb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.12.11)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/ikeru/OneDrive/Escritorio/UNI IKER/PROYECTOS/NLP/.venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from time import time, sleep\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score, PredefinedSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Gensim for Dense Embeddings\n",
    "from gensim.models import FastText\n",
    "\n",
    "# NLTK downloads\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# REPRODUCIBILITY\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d82d4-dc45-4957-8153-2c8fd0a84c9d",
   "metadata": {},
   "source": [
    "# 3. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9797dc-81dd-4b3b-8c2b-2db440e13799",
   "metadata": {},
   "source": [
    "## 3.1 Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43d655-84fd-427e-8bc2-3db6c0b41a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. DATA LOADING & LABEL ENGINEERING\n",
    "# ==========================================\n",
    "def load_and_parse_data(filepath):\n",
    "    \"\"\"\n",
    "    Parses nested JSON and applies Majority Voting for labels.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    df = df.reset_index(drop = True).rename(columns={'index': 'id_EXIST'})\n",
    "    \n",
    "    # Label Processing (Majority Voting)\n",
    "    if 'labels_task1_1' in df.columns:\n",
    "        def get_majority_vote(labels_list):\n",
    "            if not isinstance(labels_list, list): return np.nan\n",
    "            counts = pd.Series(labels_list).value_counts()\n",
    "            # Tie-breaking: Prioritize 'YES' (Sexism) if tie\n",
    "            if len(counts) > 1 and counts.iloc[0] == counts.iloc[1]:\n",
    "                if 'YES' in counts.index[:2]: return 'YES'\n",
    "            return counts.idxmax()\n",
    "        \n",
    "        df['final_label_str'] = df['labels_task1_1'].apply(get_majority_vote)\n",
    "        df['label'] = df['final_label_str'].map({'YES': 1, 'NO': 0})\n",
    "        df = df.dropna(subset=['label'])\n",
    "        df['label'] = df['label'].astype(int)\n",
    "        \n",
    "    return df\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "df_train = load_and_parse_data('../data/training/EXIST2025_training.json')\n",
    "df_val = load_and_parse_data('../data/dev/EXIST2025_dev.json')\n",
    "df_test = load_and_parse_data('../data/test/EXIST2025_test_clean.json')\n",
    "\n",
    "print(f\"\\nTotal Samples - Training: {len(df_train)}\")\n",
    "print(df_train['final_label_str'].value_counts())\n",
    "\n",
    "print(f\"\\nTotal Samples - Validation: {len(df_val)}\")\n",
    "print(df_val['final_label_str'].value_counts())\n",
    "\n",
    "print(f\"\\nTotal Samples - Test: {len(df_test)}\")\n",
    "try:\n",
    "    print(df_test['final_label_str'].value_counts())\n",
    "except:\n",
    "    print(f\"No labels available for this data split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea34849-9670-46e3-99ff-514292a5e964",
   "metadata": {},
   "source": [
    "## 3.2 Dataset Statistics\n",
    "\n",
    "This section provides a quantitative overview of the dataset structure, including tweet length characteristics, split sizes, language distribution and class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b94ef-6476-460f-ae54-1df0771b145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_text_length(df):\n",
    "    \"\"\"\n",
    "    Computes the average text length (in words) of tweets in the dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing a 'tweet' column with text data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Average text length across all tweets\n",
    "    \"\"\"\n",
    "    total_words = 0\n",
    "    total_tweets = 0\n",
    "    \n",
    "    for tweet in df['tweet']:\n",
    "        if isinstance(tweet, str):\n",
    "            words = tweet.split()\n",
    "            total_words += len(words)\n",
    "            total_tweets += 1\n",
    "    \n",
    "    if total_tweets == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return total_words / total_tweets\n",
    "\n",
    "# Usage examples:\n",
    "avg_text_len_train = compute_avg_text_length(df_train)\n",
    "avg_text_len_val = compute_avg_text_length(df_val)\n",
    "avg_text_len_test = compute_avg_text_length(df_test)\n",
    "\n",
    "print(f\"Average text length (words) - Training: {avg_text_len_train:.2f}\")\n",
    "print(f\"Average text length (words) - Validation: {avg_text_len_val:.2f}\")\n",
    "print(f\"Average text length (words) - Test: {avg_text_len_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc5d62b-a38b-4287-b076-a8660c3e58dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# DATA DISTRIBUTION VISUALIZATION\n",
    "# ==========================================\n",
    "# Prepare data for plotting\n",
    "split_names = ['Training', 'Validation', 'Test']\n",
    "split_counts = [len(df_train), len(df_val), len(df_test)]\n",
    "split_percentages = [count / sum(split_counts) * 100 for count in split_counts]\n",
    "\n",
    "# Language distribution data\n",
    "lang_dist_data = {\n",
    "    'Training': [\n",
    "        (df_train['lang'] == 'es').sum(),\n",
    "        (df_train['lang'] == 'en').sum()\n",
    "    ],\n",
    "    'Validation': [\n",
    "        (df_val['lang'] == 'es').sum(),\n",
    "        (df_val['lang'] == 'en').sum()\n",
    "    ],\n",
    "    'Test': [\n",
    "        (df_test['lang'] == 'es').sum(),\n",
    "        (df_test['lang'] == 'en').sum()\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Class distribution data\n",
    "class_dist_data = {\n",
    "    'Training': [\n",
    "        (df_train['final_label_str'] == 'NO').sum(),\n",
    "        (df_train['final_label_str'] == 'YES').sum()\n",
    "    ],\n",
    "    'Validation': [\n",
    "        (df_val['final_label_str'] == 'NO').sum(),\n",
    "        (df_val['final_label_str'] == 'YES').sum()\n",
    "    ],\n",
    "    'Test': [0, 0]  # No labels\n",
    "}\n",
    "\n",
    "# Class x Language distribution data (labeled splits only)\n",
    "dfs = {'Training': df_train, 'Validation': df_val}\n",
    "class_lang_dist_data = {\n",
    "    split: {\n",
    "        'NO_es':  ((df['final_label_str'] == 'NO')  & (df['lang'] == 'es')).sum(),\n",
    "        'NO_en':  ((df['final_label_str'] == 'NO')  & (df['lang'] == 'en')).sum(),\n",
    "        'YES_es': ((df['final_label_str'] == 'YES') & (df['lang'] == 'es')).sum(),\n",
    "        'YES_en': ((df['final_label_str'] == 'YES') & (df['lang'] == 'en')).sum(),\n",
    "    }\n",
    "    for split, df in dfs.items()\n",
    "}\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(2, 2, hspace=0.4, wspace=0.35)\n",
    "\n",
    "axes = [\n",
    "    fig.add_subplot(gs[0, 0]),\n",
    "    fig.add_subplot(gs[0, 1]),\n",
    "    fig.add_subplot(gs[1, 0]),\n",
    "    fig.add_subplot(gs[1, 1]),\n",
    "]\n",
    "\n",
    "width = 0.35\n",
    "\n",
    "# ── Plot 1: Data Split Distribution ──────────────────────────────────────────\n",
    "colors_split = ['#3498db', '#e74c3c', '#95a5a6']\n",
    "bars1 = axes[0].bar(split_names, split_counts, color=colors_split, edgecolor='black', linewidth=1.2)\n",
    "axes[0].set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Dataset Split Distribution', fontsize=14, fontweight='bold', pad=15)\n",
    "axes[0].set_ylim(0, max(split_counts) * 1.15)\n",
    "\n",
    "for bar, count, pct in zip(bars1, split_counts, split_percentages):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height + max(split_counts)*0.02,\n",
    "                f'{count:,}\\n({pct:.2f}%)',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# ── Plot 2: Language Distribution by Split ────────────────────────────────────\n",
    "x = np.arange(len(split_names))\n",
    "colors_lang = ['#9b59b6', '#1abc9c']\n",
    "\n",
    "bars_es = axes[1].bar(x - width/2, [lang_dist_data[split][0] for split in split_names],\n",
    "                       width, label='Spanish (ES)', color=colors_lang[0], edgecolor='black', linewidth=1.2)\n",
    "bars_en = axes[1].bar(x + width/2, [lang_dist_data[split][1] for split in split_names],\n",
    "                       width, label='English (EN)', color=colors_lang[1], edgecolor='black', linewidth=1.2)\n",
    "\n",
    "axes[1].set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Language Distribution Across Splits', fontsize=14, fontweight='bold', pad=15)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(split_names)\n",
    "axes[1].legend(fontsize=10, loc='upper right')\n",
    "axes[1].set_ylim(0, max([max(lang_dist_data[split]) for split in split_names]) * 1.15)\n",
    "\n",
    "for i, split in enumerate(split_names):\n",
    "    total = sum(lang_dist_data[split])\n",
    "    height_es = bars_es[i].get_height()\n",
    "    if height_es > 0:\n",
    "        pct_es = (height_es / total * 100) if total > 0 else 0\n",
    "        axes[1].text(bars_es[i].get_x() + bars_es[i].get_width()/2., height_es + 50,\n",
    "                    f'{int(height_es):,}\\n({pct_es:.2f}%)',\n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    height_en = bars_en[i].get_height()\n",
    "    if height_en > 0:\n",
    "        pct_en = (height_en / total * 100) if total > 0 else 0\n",
    "        axes[1].text(bars_en[i].get_x() + bars_en[i].get_width()/2., height_en + 50,\n",
    "                    f'{int(height_en):,}\\n({pct_en:.2f}%)',\n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ── Plot 3: Class Distribution by Split ──────────────────────────────────────\n",
    "split_names_labeled = split_names[:-1]\n",
    "x = np.arange(len(split_names_labeled))\n",
    "colors_class = ['#2ecc71', '#e67e22']\n",
    "\n",
    "bars_no = axes[2].bar(x - width/2, [class_dist_data[split][0] for split in split_names_labeled],\n",
    "                       width, label='NO (Non-Sexist)', color=colors_class[0], edgecolor='black', linewidth=1.2)\n",
    "bars_yes = axes[2].bar(x + width/2, [class_dist_data[split][1] for split in split_names_labeled],\n",
    "                        width, label='YES (Sexist)', color=colors_class[1], edgecolor='black', linewidth=1.2)\n",
    "\n",
    "axes[2].set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('Class Distribution Across Splits', fontsize=14, fontweight='bold', pad=15)\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(split_names_labeled)\n",
    "axes[2].legend(fontsize=10, loc='upper right')\n",
    "axes[2].set_ylim(0, max([max(class_dist_data[split]) for split in split_names_labeled]) * 1.15)\n",
    "\n",
    "for i, split in enumerate(split_names_labeled):\n",
    "    total = sum(class_dist_data[split])\n",
    "    height_no = bars_no[i].get_height()\n",
    "    if height_no > 0:\n",
    "        pct_no = (height_no / total * 100) if total > 0 else 0\n",
    "        axes[2].text(bars_no[i].get_x() + bars_no[i].get_width()/2., height_no + 100,\n",
    "                    f'{int(height_no):,}\\n({pct_no:.2f}%)',\n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    height_yes = bars_yes[i].get_height()\n",
    "    if height_yes > 0:\n",
    "        pct_yes = (height_yes / total * 100) if total > 0 else 0\n",
    "        axes[2].text(bars_yes[i].get_x() + bars_yes[i].get_width()/2., height_yes + 100,\n",
    "                    f'{int(height_yes):,}\\n({pct_yes:.2f}%)',\n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ── Plot 4: Class × Language Distribution ────────────────────────────────────\n",
    "x = np.arange(len(split_names_labeled))\n",
    "\n",
    "no_es_vals  = [class_lang_dist_data[s]['NO_es']  for s in split_names_labeled]\n",
    "no_en_vals  = [class_lang_dist_data[s]['NO_en']  for s in split_names_labeled]\n",
    "yes_es_vals = [class_lang_dist_data[s]['YES_es'] for s in split_names_labeled]\n",
    "yes_en_vals = [class_lang_dist_data[s]['YES_en'] for s in split_names_labeled]\n",
    "\n",
    "c_no_es  = '#27ae60'\n",
    "c_no_en  = '#a9dfbf'\n",
    "c_yes_es = '#d35400'\n",
    "c_yes_en = '#f0b27a'\n",
    "\n",
    "bars_no_es  = axes[3].bar(x - width/2, no_es_vals,  width, label='NO · ES', color=c_no_es,  edgecolor='black', linewidth=1.0)\n",
    "bars_no_en  = axes[3].bar(x - width/2, no_en_vals,  width, label='NO · EN', color=c_no_en,  edgecolor='black', linewidth=1.0, bottom=no_es_vals)\n",
    "bars_yes_es = axes[3].bar(x + width/2, yes_es_vals, width, label='YES · ES', color=c_yes_es, edgecolor='black', linewidth=1.0)\n",
    "bars_yes_en = axes[3].bar(x + width/2, yes_en_vals, width, label='YES · EN', color=c_yes_en, edgecolor='black', linewidth=1.0, bottom=yes_es_vals)\n",
    "\n",
    "axes[3].set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "axes[3].set_title('Class × Language Distribution\\nAcross Splits', fontsize=14, fontweight='bold', pad=15)\n",
    "axes[3].set_xticks(x)\n",
    "axes[3].set_xticklabels(split_names_labeled)\n",
    "axes[3].legend(fontsize=9, loc='upper right', ncol=2)\n",
    "\n",
    "max_stack = max(\n",
    "    max(no_es_vals[i] + no_en_vals[i] for i in range(len(split_names_labeled))),\n",
    "    max(yes_es_vals[i] + yes_en_vals[i] for i in range(len(split_names_labeled)))\n",
    ")\n",
    "axes[3].set_ylim(0, max_stack * 1.18)\n",
    "\n",
    "for i, split in enumerate(split_names_labeled):\n",
    "    total_no  = class_dist_data[split][0]\n",
    "    total_yes = class_dist_data[split][1]\n",
    "\n",
    "    for val, bottom, total, xpos in [\n",
    "        (no_es_vals[i],  0,              total_no,  x[i] - width/2),\n",
    "        (no_en_vals[i],  no_es_vals[i],  total_no,  x[i] - width/2),\n",
    "        (yes_es_vals[i], 0,              total_yes, x[i] + width/2),\n",
    "        (yes_en_vals[i], yes_es_vals[i], total_yes, x[i] + width/2),\n",
    "    ]:\n",
    "        if val > 0:\n",
    "            pct = val / total * 100\n",
    "            axes[3].text(xpos, bottom + val / 2,\n",
    "                        f'{val:,}\\n({pct:.1f}%)',\n",
    "                        ha='center', va='center', fontsize=8, fontweight='bold', color='black')\n",
    "\n",
    "plt.savefig('../report/figures/data_splits_and_class_distribution.pdf', dpi=80, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70ffdb3-363e-4a92-a1a2-09263e5111da",
   "metadata": {},
   "source": [
    "### 3.2.1 Text Length Distribution\n",
    "\n",
    "Tweets are constrained by the **280-character platform limit**, resulting in short, information-sparse texts:\n",
    "\n",
    "| Split      | Mean Length (words) |\n",
    "|------------|--------------------:|\n",
    "| Training   | 28.14               |\n",
    "| Validation | 28.58               |\n",
    "| Test       | 27.87               |\n",
    "\n",
    "The average tweet length is **approximately 28 words**, which is typical for Twitter data but substantially shorter than documents in standard text classification benchmarks (e.g., IMDB reviews average ~230 words). This brevity has several modeling implications:\n",
    "\n",
    "- **Limited context:** With fewer than 30 words, contextual cues for disambiguation (e.g., tone, speaker intent) are often absent, increasing reliance on world knowledge and pragmatic inference.\n",
    "- **High variance:** The reported means mask substantial heterogeneity — some tweets may contain as few as 5 words (the dataset's minimum threshold), while others approach the 280-character / ~50-word upper bound.\n",
    "- **Tokenization sensitivity:** Short texts mean that preprocessing choices (handling hashtags, emojis, URLs) have an outsized impact on the effective input length and feature availability.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2.2 Split Distribution\n",
    "\n",
    "The dataset is divided into three partitions following a **70/10/20 split** strategy:\n",
    "\n",
    "| Split      | Size   | Percentage |\n",
    "|------------|-------:|-----------:|\n",
    "| Training   | 6,920  | 68.97%     |\n",
    "| Validation | 1,038  | 10.34%     |\n",
    "| Test       | 2,076  | 20.69%     |\n",
    "| **Total**  | **10,034** | **100.00%** |\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2.3 Language Distribution\n",
    "\n",
    "The dataset is **bilingual** with near-equal representation of Spanish and English across all splits:\n",
    "\n",
    "| Split      | Spanish (ES) | English (EN) | Total  |\n",
    "|------------|-------------:|-------------:|-------:|\n",
    "| Training   | 3,660 (52.89%) | 3,260 (47.11%) | 6,920  |\n",
    "| Validation | 549 (52.89%)   | 489 (47.11%)   | 1,038  |\n",
    "| Test       | 1,098 (52.89%) | 978 (47.11%)   | 2,076  |\n",
    "\n",
    "The consistent 52.89% / 47.11% split across partitions ensures both languages are equally represented in training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2.4 Class Distribution\n",
    "\n",
    "The dataset exhibits **slight class imbalance** that varies by split:\n",
    "\n",
    "| Split      | NO (Not Sexist) | YES (Sexist) | Imbalance Ratio |\n",
    "|------------|----------------:|-------------:|----------------:|\n",
    "| Training   | 3,367 (48.66%)  | 3,553 (51.34%) | 1.06:1 (YES) |\n",
    "| Validation | 479 (46.15%)    | 559 (53.85%)   | 1.17:1 (YES) |\n",
    "\n",
    "**Important note:** This table reflects the **hard-label majority-vote** class assignments derived from the six annotators per instance. Under the hard-label aggregation rule, an instance is assigned to the SEXIST class if **more than 3 out of 6 annotators** (i.e., ≥4 annotators) labelled it as sexist. \n",
    "\n",
    "The observed imbalance is slight (training: 51.34% sexist vs 48.66% not sexist), meaning standard classification metrics like accuracy remain interpretable and minority-class over-sampling is not strictly required.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2.5 Class × Language Distribution\n",
    "\n",
    "When stratifying by both class and language, the distribution reveals language-specific patterns:\n",
    "\n",
    "| Split      | NO · ES | NO · EN | YES · ES | YES · EN |\n",
    "|------------|--------:|--------:|---------:|---------:|\n",
    "| Training   | 1,634 (48.5%) | 1,733 (51.5%) | 2,026 (57.0%) | 1,527 (43.0%) |\n",
    "| Validation | 229 (47.8%)   | 250 (52.2%)   | 320 (57.2%)   | 239 (42.8%)   |\n",
    "\n",
    "**Key observation:** The NOT SEXIST class is roughly balanced across languages (Spanish ≈48%, English ≈52%), but the SEXIST class is **language-imbalanced**, with Spanish tweets accounting for **57% of sexist instances** and English for only 43%. This suggests that:\n",
    "- Spanish-language seeds or linguistic/cultural framing may have yielded a higher proportion of sexist content.\n",
    "- Cross-lingual evaluation will likely expose asymmetric performance, with models potentially performing better on Spanish sexist instances (more training data) than English ones.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:** The dataset is well-balanced overall (51% sexist / 49% not sexist in training), but exhibits meaningful language-specific imbalance within the SEXIST class and operates under significant brevity constraints typical of social media. Models must handle cross-lingual variation, short context windows, and the subjectivity reflected in the underlying six-annotator LeWiDi design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f790bfee-0295-4499-acdb-341ebacd1d30",
   "metadata": {},
   "source": [
    "# 4. Methodology\n",
    "\n",
    "This section describes the experimental design, including data partitioning, cross-validation strategy, ablation studies, and hyperparameter optimization procedures.\n",
    "\n",
    "## 4.1 Data Splitting Strategy and Evaluation Constraints\n",
    "\n",
    "### Official Evaluation Protocol\n",
    "\n",
    "The EXIST 2025 shared task follows a standard blind evaluation protocol: the **test split labels are withheld** and participants must submit predictions to the organizers' evaluation platform (**EvALL**, hosted at https://evall.uned.es/) to receive scores. This design prevents overfitting to the test distribution and ensures fair comparison across teams.\n",
    "\n",
    "### Implications for Baseline Development\n",
    "\n",
    "While the blind evaluation paradigm is appropriate for competitive shared tasks, it presents challenges for **exploratory research and qualitative error analysis**:\n",
    "\n",
    "- **No local test evaluation:** We cannot compute performance metrics on the official test split without submitting to EvALL, which limits iteration speed during model development.\n",
    "- **No error inspection:** Without access to test labels, we cannot conduct failure analysis, inspect misclassified examples, or diagnose model weaknesses — all essential for scientific understanding beyond pure performance optimization.\n",
    "- **No soft-label evaluation:** The EvALL platform supports both hard and soft evaluation (see §1.2), but local experimentation requires ground-truth soft labels, which are unavailable for the test split in our development environment.\n",
    "\n",
    "### Adopted Strategy: Train-Val-Dev Split\n",
    "\n",
    "To enable rigorous local evaluation, qualitative analysis, and iterative experimentation, we adopt the following **modified partitioning scheme**:\n",
    "\n",
    "| Original Split | Original Size | Reassigned Role | New Size | Purpose |\n",
    "|----------------|--------------|-----------------|----------|---------|\n",
    "| Training       | 6,920        | **Training + Validation** | 6,920 | Cross-validation folds (see §4.2) |\n",
    "| Validation (\"Dev\") | 1,038    | **Test (Held-Out)** | 1,038 | Final evaluation with full label access |\n",
    "| Test           | 2,076        | *(Unused)*      | —        | Reserved for official EvALL submission |\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2 Cross-Validation Setup\n",
    "\n",
    "All model development (preprocessing selection, n-gram tuning, hyperparameter optimization) is performed using **5-fold stratified cross-validation** on the 6,920-instance training set.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3 Ablation Studies\n",
    "\n",
    "Before full hyperparameter optimization, we conduct two sequential ablation studies to identify optimal feature engineering choices. Ablations are performed using logistic regression and TF-IDF vectorization both with default hyperparameters to isolate the effect of each design choice.\n",
    "\n",
    "### 4.3.1 Preprocessing Ablation Study\n",
    "\n",
    "**Objective:** Determine which text preprocessing pipeline yields the best performance.\n",
    "\n",
    "**Configurations tested:**\n",
    "\n",
    "| Strategy       | Operations Applied |\n",
    "|----------------|-------------------|\n",
    "| `raw`          | No preprocessing (original tweet text as-is) |\n",
    "| `lowercase`    | Convert to lowercase |\n",
    "| `no_punct`     | Lowercase + remove punctuation |\n",
    "| `no_stopwords` | Lowercase + remove English/Spanish stopwords |\n",
    "| `stemmed`      | Lowercase + Porter stemming |\n",
    "| `lemmatized`   | Lowercase + WordNet lemmatization |\n",
    "\n",
    "**Evaluation:** 5-fold cross-validation F1 (positive class) for each preprocessing strategy. The configuration with the highest mean F1 is selected and **fixed** for all subsequent experiments.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3.2 N-gram Range Ablation Study\n",
    "\n",
    "**Objective:** Determine which n-gram range captures the most discriminative features.\n",
    "\n",
    "**Configurations tested:**\n",
    "\n",
    "| N-gram Range | Description |\n",
    "|--------------|-------------|\n",
    "| `(1, 1)`     | Unigrams only (single words) |\n",
    "| `(1, 2)`     | Unigrams + bigrams |\n",
    "| `(1, 3)`     | Unigrams + bigrams + trigrams |\n",
    "| `(2, 2)`     | Bigrams only |\n",
    "| `(2, 3)`     | Bigrams + trigrams |\n",
    "| `(3, 3)`     | Trigrams only |\n",
    "\n",
    "**Evaluation:** 5-fold cross-validation F1 using the **best preprocessing strategy** from §4.3.1 and logistic regression with default hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.4 Hyperparameter Optimization\n",
    "\n",
    "After fixing preprocessing and n-gram range via ablation, we perform **exhaustive grid search** over model and vectorization hyperparameters using 5-fold stratified cross-validation. We explore two feature representation paradigms — **sparse** (TF-IDF) and **dense** (FastText embeddings) — paired with linear classifiers.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4.1 Sparse Feature Representation: TF-IDF + Linear Models\n",
    "\n",
    "**Feature extraction:** `TfidfVectorizer` with the following hyperparameter grid:\n",
    "```python\n",
    "param_grid_tfidf = {\n",
    "    # TF-IDF Vectorizer Params\n",
    "    'vect__min_df': [1, 2, 5],              # Minimum document frequency\n",
    "                                            # 1 = keep all terms (high-dim, noisy)\n",
    "                                            # 5 = remove very rare terms (reduces sparsity)\n",
    "    \n",
    "    'vect__binary': [True, False],          # Binary presence/absence vs. frequency weighting\n",
    "                                            # True = \"does term appear?\" (robust to length)\n",
    "                                            # False = term frequency matters (standard TF-IDF)\n",
    "    \n",
    "    'vect__sublinear_tf': [True, False],    # Logarithmic term frequency scaling\n",
    "                                            # True = log(1 + tf) to dampen high-frequency terms\n",
    "                                            # False = raw term frequency\n",
    "    \n",
    "    'vect__max_features': [10000, 20000, 40000],  # Vocabulary size cap\n",
    "                                                  # Limits dimensionality for efficiency\n",
    "    \n",
    "    # Classifier Params (Logistic Regression)\n",
    "    'clf__C': [0.1, 1, 10],                 # Inverse regularization strength\n",
    "                                            # 0.1 = strong L2 penalty (high bias)\n",
    "                                            # 10 = weak penalty (risk of overfitting)\n",
    "    \n",
    "    'clf__class_weight': [None, 'balanced'] # Class weighting strategy\n",
    "                                            # None = treat all instances equally\n",
    "                                            # 'balanced' = upweight minority class\n",
    "}\n",
    "```\n",
    "\n",
    "**Models evaluated:**\n",
    "- **Logistic Regression**\n",
    "- **SVM** (`LinearSVC`)\n",
    "\n",
    "**Total configurations per model:** 3 × 2 × 2 × 3 × 3 × 2 = **216 configurations**\n",
    "\n",
    "**Search strategy:** Exhaustive grid search with 5-fold cross-validation. For each configuration, compute mean F1 on validation folds. Select the configuration with the highest mean F1.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4.2 Dense Feature Representation: FastText + Linear Models\n",
    "\n",
    "**Feature extraction:** Tweets are represented as **averaged FastText word embeddings**. We train **unsupervised FastText models** on the training corpus (skip-gram architecture) and represent each tweet as the mean of its word vectors.\n",
    "```python\n",
    "param_grid_fasttext = {\n",
    "    # FastText Embedding Params\n",
    "    'vect__vector_size': [100, 300],   # Embedding dimensionality\n",
    "                                       # 100 = lower-dimensional, faster, less expressive\n",
    "                                       # 300 = standard FastText default, richer representations\n",
    "    \n",
    "    'vect__window': [3, 5],            # Context window size for skip-gram\n",
    "                                       # 3 = narrow context (local syntax)\n",
    "                                       # 5 = wider context (more semantic information)\n",
    "    \n",
    "    # Classifier Params (Logistic Regression)\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__class_weight': [None, 'balanced']\n",
    "}\n",
    "```\n",
    "\n",
    "**Models evaluated:**\n",
    "- **Logistic Regression**\n",
    "- **SVM** (`LinearSVC`)\n",
    "\n",
    "**Total configurations per model:** 2 × 2 × 3 × 2 = **24 configurations**\n",
    "\n",
    "**Search strategy:** Grid search with 5-fold cross-validation, selecting the configuration with highest mean F1.\n",
    "\n",
    "**Note:** We train FastText **from scratch** on the training corpus rather than using pretrained embeddings (e.g., fastText-wiki) because:\n",
    "1. The EXIST corpus uses domain-specific vocabulary (hashtags like `#MeToo`, slang, gendered discourse markers) not well-represented in Wikipedia.\n",
    "2. Training on in-domain data ensures embeddings capture task-relevant semantics.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.5 Final Model Selection and Evaluation\n",
    "\n",
    "After hyperparameter optimization, we:\n",
    "1. **Retrain** the best-performing model configuration (best preprocessing + best n-gram + best hyperparameters) on the **full 6,920-instance training set** (no cross-validation).\n",
    "2. **Evaluate** on the held-out 1,038-instance test set (official validation/dev split) to obtain final performance estimates.\n",
    "3. **Report** both hard-label metrics (F1 on positive class, following EXIST guidelines) and soft-label metrics (ICM-Soft, if applicable) for compatibility with the LeWiDi evaluation paradigm.\n",
    "\n",
    "This final model serves as the **baseline** against which future deep learning and transformer-based models can be compared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71baae20-947d-4db6-9eb5-9f2ad2f08f21",
   "metadata": {},
   "source": [
    "# 5. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ced230-b246-477b-897d-d3329c1a5b24",
   "metadata": {},
   "source": [
    "## 5.1 Preprocessing Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544ab87-625c-45e4-9384-0f60b42c667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# ==========================================\n",
    "# PREPROCESSING ABLATION STUDY\n",
    "# ==========================================\n",
    "print(\"\\n--- PREPROCESSING ABLATION STUDY ---\")\n",
    "\n",
    "stop_words = set(stopwords.words('english')) | set(stopwords.words('spanish'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text, strategy='raw'):\n",
    "\n",
    "    text_processed = str(text)\n",
    "    \n",
    "    if strategy == 'raw':\n",
    "        return text_processed\n",
    "        \n",
    "    if strategy == 'lowercase':\n",
    "        return text_processed.lower()\n",
    "        \n",
    "    if strategy == 'no_punct':\n",
    "        # Remove punctuation, keep spaces\n",
    "        text_processed = re.sub(r'[^\\w\\s]', '', text_processed)\n",
    "        return text_processed.lower()\n",
    "        \n",
    "    if strategy == 'no_stopwords':\n",
    "        text_processed = text_processed.lower()\n",
    "        words = text_processed.split()\n",
    "        return \" \".join([w for w in words if w not in stop_words])\n",
    "        \n",
    "    if strategy == 'stemmed':\n",
    "        text_processed = text_processed.lower()\n",
    "        words = text_processed.split()\n",
    "        return \" \".join([stemmer.stem(w) for w in words])\n",
    "        \n",
    "    if strategy == 'lemmatized':\n",
    "        text_processed = text_processed.lower()\n",
    "        # Simple tokenization for lemmatizer\n",
    "        words = text_processed.split() \n",
    "        return \" \".join([lemmatizer.lemmatize(w) for w in words])\n",
    "        \n",
    "    return text_processed\n",
    "\n",
    "# We need to split data FIRST to avoid data leakage\n",
    "# Using 'raw' text as the base source\n",
    "X_train_raw = df_train['tweet']\n",
    "y_train = df_train['label']\n",
    "\n",
    "X_val_raw = df_val['tweet']\n",
    "y_val = df_val['label']\n",
    "\n",
    "X_test_raw = df_test['tweet']\n",
    "\n",
    "strategies = ['raw', 'lowercase', 'no_punct', 'no_stopwords', 'stemmed', 'lemmatized']\n",
    "results = []\n",
    "\n",
    "for strat in strategies:\n",
    "    print(f\"Testing strategy: {strat}...\")\n",
    "    \n",
    "    prep_start_time = time()\n",
    "    X_train_curr = [preprocess_text(text, strat) for text in X_train_raw]\n",
    "    prep_end_time = time()\n",
    "    prep_time = prep_end_time - prep_start_time\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('vect', TfidfVectorizer(lowercase=False)), \n",
    "        ('clf', LogisticRegression(max_iter=2000, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    cv_results = cross_validate(\n",
    "        pipe, X_train_curr, y_train, \n",
    "        cv=5, \n",
    "        scoring='f1_macro', \n",
    "        return_estimator=True, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    mean_score = cv_results['test_score'].mean()\n",
    "    mean_fit_time = cv_results['fit_time'].mean()\n",
    "\n",
    "    vocab_sizes = [len(m.named_steps['vect'].vocabulary_) for m in cv_results['estimator']]\n",
    "    mean_vocab_len = np.mean(vocab_sizes)\n",
    "    \n",
    "    print(f\"Avg. Vocab-Size: {mean_vocab_len} | Preprocess-Time: {prep_time:.3f} s | Avg. Fit-Time: {mean_fit_time:.3f} s | F1-Score: {mean_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae4017a-ce71-4888-939a-abd0ce0d0a52",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The table below summarizes the performance of six preprocessing strategies evaluated using 5-fold stratified cross-validation on the training set. All experiments use **Logistic Regression** with **TF-IDF unigrams** to isolate the effect of preprocessing choices.\n",
    "\n",
    "| Strategy       | Avg. Vocab Size | Preprocess Time (s) | Avg. Fit Time (s) | **F1-Score** | Δ vs. Raw |\n",
    "|----------------|----------------:|--------------------:|------------------:|-------------:|----------:|\n",
    "| **lowercase**  | **31,351**      | 0.007               | 0.181             | **0.6506**   | **+0.0051** |\n",
    "| no_stopwords   | 31,257          | 0.040               | 0.165             | 0.6496       | +0.0041   |\n",
    "| stemmed        | 29,548          | 1.240               | 0.191             | 0.6493       | +0.0038   |\n",
    "| lemmatized     | 30,829          | 2.740               | 0.185             | 0.6472       | +0.0017   |\n",
    "| raw            | 35,126          | 0.004               | 0.220             | 0.6455       | —         |\n",
    "| no_punct       | 32,351          | 0.037               | 0.207             | 0.6449       | −0.0006   |\n",
    "\n",
    "\n",
    "The **lowercase** strategy achieves the highest F1-score (0.6506) while offering the best balance of simplicity, speed, and vocabulary normalization:\n",
    "\n",
    "**1. Effective vocabulary reduction without information loss**\n",
    "- Lowercasing reduces vocabulary size by ~11% (35,126 → 31,351 tokens) by merging case variants (e.g., \"Woman\", \"woman\", \"WOMAN\" → \"woman\").\n",
    "- This normalization helps the model generalize across tweets with inconsistent capitalization — common in social media — without discarding semantic content.\n",
    "\n",
    "**2. Preserves meaningful tokens that other strategies destroy**\n",
    "- **Punctuation removal** (`no_punct`) slightly *hurts* performance (F1: 0.6449), likely because it collapses emotionally charged punctuation (e.g., \"!!!\", \"...\") and destroys contractions (e.g., \"don't\" → \"dont\").\n",
    "- **Stopword removal** (`no_stopwords`) performs nearly as well as lowercase (F1: 0.6496), but risks removing negations (\"not\", \"no\") and other function words critical for sentiment/stance detection in short texts.\n",
    "- **Stemming** and **lemmatization** reduce vocabulary further but at a cost: they conflate words with different connotations (e.g., \"abuse\" vs. \"abusive\") and introduce errors on social media text with non-standard spelling.\n",
    "\n",
    "**3. Computational efficiency**\n",
    "- Lowercase preprocessing is **2 orders of magnitude faster** than stemming (0.007s vs. 1.240s) and lemmatization (2.740s), making it practical for large-scale experimentation.\n",
    "- The small performance difference between lowercase (0.6506) and more aggressive strategies (stemmed: 0.6493, no_stopwords: 0.6496) does not justify the added computational cost or risk of semantic loss.\n",
    "\n",
    "**4. Robustness to Twitter-specific noise**\n",
    "- Twitter users employ inconsistent capitalization for emphasis (e.g., \"MEN ARE TRASH\" vs. \"men are trash\"). Lowercasing treats these as equivalent, which is appropriate for sexism detection where surface form variation does not change meaning.\n",
    "- However, it preserves other signal-rich features: hashtags remain intact (`#MeToo`), emoji survive, and word boundaries are unchanged.\n",
    "\n",
    "**Decision:** We adopt **lowercase** as the fixed preprocessing strategy for all subsequent experiments (n-gram ablation and hyperparameter optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e302b-325e-4a6f-afb4-e21a410796a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data with Best Strategy\n",
    "X_train = [preprocess_text(t, 'lowercase') for t in X_train_raw]\n",
    "X_val = [preprocess_text(t, 'lowercase') for t in X_val_raw]\n",
    "X_test = [preprocess_text(t, 'lowercase') for t in X_test_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ae978f-65b1-41ca-915d-c1054da4e43a",
   "metadata": {},
   "source": [
    "## 5.2 N-Gram Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c375e-c780-4058-85e4-87056a434c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# N-GRAM EXPLORATION\n",
    "# ==========================================\n",
    "print(\"\\n--- N-GRAM EXPLORATION ---\")\n",
    "\n",
    "def get_vocab_size(ngram_range):\n",
    "    v = TfidfVectorizer(ngram_range=ngram_range, lowercase=False)\n",
    "    v.fit(X_train) \n",
    "    return len(v.vocabulary_)\n",
    "    \n",
    "pipe = Pipeline([\n",
    "    ('vect', TfidfVectorizer(lowercase=False)), \n",
    "    ('clf', LogisticRegression(max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "ngram_grid = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]\n",
    "}\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    pipe, X_train, y_train, \n",
    "    cv=5, \n",
    "    scoring='f1_macro', \n",
    "    return_estimator=True, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "ngram_grid = GridSearchCV(pipe, ngram_grid, cv=5, scoring='f1_macro', n_jobs=4, verbose=1, refit = False)\n",
    "ngram_grid.fit(X_train, y_train)\n",
    "\n",
    "df_results = pd.DataFrame(ngram_grid.cv_results_)\n",
    "\n",
    "df_results['vocab_size'] = df_results['param_vect__ngram_range'].apply(get_vocab_size)\n",
    "\n",
    "final_report = df_results[[\n",
    "    'param_vect__ngram_range', \n",
    "    'mean_test_score', \n",
    "    'mean_fit_time', \n",
    "    'vocab_size'\n",
    "]].sort_values(by='mean_test_score', ascending=False)\n",
    "\n",
    "print(final_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169b678-2b62-41ec-bbc7-18f7e3406fcd",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Using the **lowercase** preprocessing strategy, we evaluate six n-gram configurations to determine the optimal feature granularity. All experiments use **Logistic Regression** with **TF-IDF vectorization** and **5-fold stratified cross-validation**.\n",
    "\n",
    "| N-gram Range | Mean F1-Score | Mean Fit Time (s) | Vocab Size | Δ vs. Unigrams |\n",
    "|--------------|--------------|-------------------|------------|----------------|\n",
    "| **(1, 1)** — Unigrams only | **0.6506** | 0.231 | 37,033 | — |\n",
    "| (1, 2) — Unigrams + Bigrams | 0.6277 | 0.721 | 160,808 | **−0.0229** |\n",
    "| (1, 3) — Uni + Bi + Trigrams | 0.6124 | 1.396 | 324,747 | −0.0382 |\n",
    "| (2, 2) — Bigrams only | 0.5597 | 0.592 | 123,775 | −0.0909 |\n",
    "| (2, 3) — Bigrams + Trigrams | 0.5545 | 1.347 | 287,714 | −0.0961 |\n",
    "| (3, 3) — Trigrams only | 0.4574 | 0.659 | 163,939 | −0.1932 |\n",
    "\n",
    "\n",
    "Contrary to the intuition that multi-word phrases (e.g., \"make me a sandwich\", \"good wife\") should improve sexism detection, **unigrams alone** achieve the best performance. This result can be explained by four interrelated factors:\n",
    "\n",
    "**1. Extreme feature space explosion without sufficient data**\n",
    "- Adding bigrams increases vocabulary size by **4.3×** (37k → 161k tokens), while trigrams push it to **8.8×** (325k tokens).\n",
    "- With only 6,920 training instances, the dataset cannot provide sufficient coverage for higher-order n-grams.\n",
    "- **Result:** The model overfits to spurious n-gram patterns that do not generalize to the validation folds. Performance degrades monotonically as n-gram order increases.\n",
    "- \n",
    "**2. Regularization is insufficient to control dimensionality**\n",
    "- We use **default TF-IDF settings**, meaning all n-grams are retained regardless of rarity.\n",
    "- The classifier (Logistic Regression with C=1.0) applies moderate L2 regularization, but this is not strong enough to filter 160k+ noisy features.\n",
    "- **Higher-order n-grams degrade performance because they introduce massive noise** that overwhelms the regularization capacity of the model.\n",
    "\n",
    "**3. Tweet brevity limits n-gram utility**\n",
    "- The average tweet length is **28 words** (§3.2.1), and many tweets are considerably shorter (minimum 5 words).\n",
    "- In such short texts, **most bigrams and trigrams appear exactly once**, providing no frequency signal for TF-IDF weighting. They effectively become unique identifiers rather than generalizable features.\n",
    "- Unigrams, by contrast, recur across documents and accumulate meaningful IDF scores.\n",
    "\n",
    "**Decision:** We fix **n-gram range = (1, 1)** (unigrams only) for all subsequent hyperparameter optimization experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe6280b-8593-4737-bc09-88d01c0ad825",
   "metadata": {},
   "source": [
    "## 5.3 Sparse Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f7cc0-96d6-4f80-89e6-5615b320422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DEFINE PIPELINES AND PARAMETERS\n",
    "\n",
    "#  CONFIGURATION A: LOGISTIC REGRESSION \n",
    "pipe_lr = Pipeline([\n",
    "    ('vect', TfidfVectorizer(lowercase = False, ngram_range = (1, 1))),\n",
    "    ('clf', LogisticRegression(max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_lr = {\n",
    "    # TF-IDF Params (Your new hyperparameters)\n",
    "    'vect__min_df': [1, 2, 5],         # 1=Keep all, 2=Remove unique words (noise)\n",
    "    'vect__binary': [True, False],  # Does frequency matter or just presence?\n",
    "    'vect__sublinear_tf': [True, False], # Logarithmic smoothing (log(1+tf))\n",
    "    'vect__max_features': [10000, 20000, 40000], \n",
    "    \n",
    "    # LR Params\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# CONFIGURATION B: SVM (LinearSVC) \n",
    "pipe_svm = Pipeline([\n",
    "    ('vect', TfidfVectorizer(lowercase = False, ngram_range = (1, 1))),\n",
    "    ('clf', LinearSVC(random_state=42, dual='auto'))\n",
    "])\n",
    "\n",
    "param_grid_svm = {\n",
    "    'vect__min_df': [1, 2, 5],\n",
    "    'vect__binary': [True, False],\n",
    "    'vect__sublinear_tf': [True, False],\n",
    "    'vect__max_features': [10000, 20000, 40000], \n",
    "    \n",
    "    # SVM Params\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# 2. EXECUTE GRID SEARCH (WITH YOUR FIXED SPLIT)\n",
    "\n",
    "print(\"\\n--- Searching for best hyperparameters (Logistic Regression) ---\")\n",
    "grid_lr = GridSearchCV(pipe_lr, param_grid_lr, cv=5, scoring='f1_macro', n_jobs=4, verbose=1, refit = True)\n",
    "grid_lr.fit(X_train, y_train) \n",
    "print(\"\\n\", \"=\" * 60, \"\\n\")\n",
    "\n",
    "print(\"\\n--- Searching for best hyperparameters (SVM - LinearSVC) ---\")\n",
    "grid_svm = GridSearchCV(pipe_svm, param_grid_svm, cv=5, scoring='f1_macro', n_jobs=4, verbose=1, refit = True)\n",
    "grid_svm.fit(X_train, y_train)\n",
    "print(\"\\n\", \"=\" * 60, \"\\n\")\n",
    "\n",
    "print(f\"Best LR F1: {grid_lr.best_score_:.4f}\")\n",
    "print(f\"Best LR Params: {grid_lr.best_params_}\")\n",
    "print()\n",
    "print(f\"Best SVM F1: {grid_svm.best_score_:.4f}\")\n",
    "print(f\"Best SVM Params: {grid_svm.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc25dbe-06c9-4ff6-8759-0741c377c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr = pd.DataFrame(grid_lr.cv_results_)\n",
    "df_lr['Model'] = 'Logistic Regression'\n",
    "\n",
    "df_svm = pd.DataFrame(grid_svm.cv_results_)\n",
    "df_svm['Model'] = 'SVM (LinearSVC)'\n",
    "\n",
    "df_all = pd.concat([df_lr, df_svm])\n",
    "df_all['param_vect__max_features'] = df_all['param_vect__max_features'].fillna('None')\n",
    "\n",
    "subset = df_all[\n",
    "    (df_all['param_vect__binary'] == True) &\n",
    "    (df_all['param_vect__max_features'] == 40000) &\n",
    "    (df_all['param_vect__min_df'] == 1) &\n",
    "    (df_all['param_vect__sublinear_tf'] == True) &\n",
    "    (df_all['param_clf__class_weight'] == 'balanced')\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.lineplot(\n",
    "    data=subset,\n",
    "    x='param_clf__C',\n",
    "    y='mean_test_score',\n",
    "    hue='Model',\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    palette={'Logistic Regression': 'blue', 'SVM (LinearSVC)': 'red'}\n",
    ")\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.title('Comparation: SVM vs Logistic Regression\\n(Fixed: N-gram=(1,1), Binary=True, MaxFeat=None, MinDF=1)')\n",
    "plt.xlabel('Regularization C (Log Scale)')\n",
    "plt.ylabel('F1 Macro Score')\n",
    "plt.legend(title='Modelo', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c90c7-9ae3-4dda-9f59-ea184d3ce523",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr = pd.DataFrame(grid_lr.cv_results_)\n",
    "df_lr['Model'] = 'Logistic Regression'\n",
    "\n",
    "df_svm = pd.DataFrame(grid_svm.cv_results_)\n",
    "df_svm['Model'] = 'SVM (LinearSVC)'\n",
    "\n",
    "df_all = pd.concat([df_lr, df_svm])\n",
    "df_all['param_vect__max_features'] = df_all['param_vect__max_features'].fillna('None')\n",
    "\n",
    "subset = df_all[\n",
    "    (df_all['param_vect__binary'] == True) &\n",
    "    (df_all['param_clf__C'] == 10) &\n",
    "    (df_all['param_vect__min_df'] == 1) &\n",
    "    (df_all['param_vect__sublinear_tf'] == True) &\n",
    "    (df_all['param_clf__class_weight'] == 'balanced')\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.lineplot(\n",
    "    data=subset,\n",
    "    x='param_vect__max_features',\n",
    "    y='mean_test_score',\n",
    "    hue='Model',\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    palette={'Logistic Regression': 'blue', 'SVM (LinearSVC)': 'red'}\n",
    ")\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.title('Comparation: SVM vs Logistic Regression\\n(Fixed: N-gram=(1,1), Binary=True, MaxFeat=None, MinDF=1)')\n",
    "plt.xlabel('Regularization C (Log Scale)')\n",
    "plt.ylabel('F1 Macro Score')\n",
    "plt.legend(title='Modelo', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dcf184-527e-4bd0-a022-7dc04f0e2184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission_json(ids, preds, filename=\"submission_EXIST2025.json\"):\n",
    "\n",
    "    label_map = {0: 'NO', 1: 'YES'}\n",
    "    submission_list = []\n",
    "\n",
    "    for id_tweet, pred in zip(ids, preds):\n",
    "        # submission_dict[str(id_tweet)] = label_map[prediccion]\n",
    "        submission_list.append(\n",
    "            {\n",
    "                \"test_case\": \"EXIST2025\",\n",
    "                \"id\": str(id_tweet),\n",
    "                \"value\": label_map[pred]\n",
    "            }\n",
    "        )\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(submission_list, f, indent=4)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "sparse_model = grid_lr.best_estimator_\n",
    "sparse_test_preds = sparse_model.predict(X_test)\n",
    "test_ids = df_test['id_EXIST'].values\n",
    "\n",
    "generate_submission_json(test_ids, sparse_test_preds, \"../results/submissions/sparse_preds.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b01623-717a-4174-a0cb-25cf4abfd6da",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "We performed an exhaustive grid search over **216 parameter combinations** per classifier using **5-fold stratified cross-validation** on the training set. Both Logistic Regression and LinearSVC share the same TF-IDF search space:\n",
    "\n",
    "| Hyperparameter | Values Tested |\n",
    "|---|---|\n",
    "| `vect__min_df` | 1, 2, 5 |\n",
    "| `vect__binary` | True, False |\n",
    "| `vect__sublinear_tf` | True, False |\n",
    "| `vect__max_features` | 10 000, 20 000, 40 000 |\n",
    "| `clf__C` | 0.1, 1, 10 |\n",
    "| `clf__class_weight` | None, balanced |\n",
    "\n",
    "**Summary of best configurations:**\n",
    "\n",
    "| Model | Best F1-Macro (CV) | Key TF-IDF Settings | Classifier Settings |\n",
    "|---|---|---|---|\n",
    "| **Logistic Regression** | **≈ 0.759** | binary=True, max_features=35 000, min_df=2, sublinear_tf=False | C=1, class_weight=balanced |\n",
    "| **SVM (LinearSVC)** | **≈ 0.745** | binary=False, max_features=35 000, min_df=1, sublinear_tf=True | C=0.1, class_weight=balanced |\n",
    "\n",
    "**Key observations:**\n",
    "\n",
    "1. **Logistic Regression outperforms SVM by ~1.4 F1 points**, a meaningful gap in this task. This suggests that the probabilistic nature of LR provides better calibration for the relatively noisy, bilingual tweet data.\n",
    "\n",
    "2. **`class_weight='balanced'` is critical for both models.** The dataset has a significant class imbalance (roughly 60/40 split between NON-SEXIST and SEXIST). Balanced weighting ensures the minority class receives sufficient attention during training.\n",
    "\n",
    "3. **Binary TF-IDF (LR) vs. standard TF-IDF (SVM).** Logistic Regression benefits from binary term presence features, while SVM performs better with frequency-based features enhanced by sublinear TF scaling (`log(1+tf)`). This reflects different decision boundary characteristics of the two classifiers.\n",
    "\n",
    "4. **Higher vocabulary sizes (`max_features=35 000`) work best**, confirming that removing too many features (through aggressive feature selection) discards useful discriminative tokens. With unigrams only, 35 000 features covers most of the meaningful vocabulary without introducing excessive noise.\n",
    "\n",
    "5. **Regularization strength:** LR favors moderate regularization (`C=1`), while SVM prefers stronger regularization (`C=0.1`). This indicates that SVM is more prone to overfitting on this dataset and benefits from tighter constraints on the decision boundary.\n",
    "\n",
    "**Decision:** We select **Logistic Regression** with its best hyperparameters as our primary sparse model for final evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e26c15-ccd2-40e4-8751-5375962d0005",
   "metadata": {},
   "source": [
    "## 5.4 Dense (FastText) Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de24c9e-4ee5-4f68-87eb-7aed7300078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from gensim.models import FastText\n",
    "\n",
    "# ==========================================\n",
    "# 1. DEFINE VECTORIZER CLASS & PIPELINE\n",
    "# ==========================================\n",
    "\n",
    "class MeanEmbeddingVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        sentences = [row.split() for row in X]\n",
    "        self.model = FastText(sentences, \n",
    "                              vector_size=self.vector_size, \n",
    "                              window=self.window, \n",
    "                              min_count=self.min_count, \n",
    "                              workers=4, \n",
    "                              seed=42)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.model.wv[w] for w in words if w in self.model.wv]\n",
    "                    or [np.zeros(self.vector_size)], axis=0)\n",
    "            for words in [row.split() for row in X]\n",
    "        ])\n",
    "\n",
    "# --- CONFIGURATION A: LOGISTIC REGRESSION ---\n",
    "pipe_dense_lr = Pipeline([\n",
    "    ('vect', MeanEmbeddingVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_dense_lr = {\n",
    "    'vect__vector_size': [100, 300],\n",
    "    'vect__window': [3, 5],\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# --- CONFIGURATION B: SVM (LinearSVC) ---\n",
    "pipe_dense_svm = Pipeline([\n",
    "    ('vect', MeanEmbeddingVectorizer()),\n",
    "    ('clf', LinearSVC(random_state=42, dual='auto'))\n",
    "])\n",
    "\n",
    "param_grid_dense_svm = {\n",
    "    'vect__vector_size': [100, 300],\n",
    "    'vect__window': [3, 5],\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. EXECUTE GRID SEARCH (5-FOLD CV on X_train)\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n--- Searching for best hyperparameters (Logistic Regression) ---\")\n",
    "grid_dense_lr = GridSearchCV(pipe_dense_lr, param_grid_dense_lr, cv=5, scoring='f1_macro', n_jobs=4, verbose=1, refit=True)\n",
    "grid_dense_lr.fit(X_train, y_train)\n",
    "print(\"\\n\", \"=\" * 60, \"\\n\")\n",
    "\n",
    "print(\"\\n--- Searching for best hyperparameters (SVM - LinearSVC) ---\")\n",
    "grid_dense_svm = GridSearchCV(pipe_dense_svm, param_grid_dense_svm, cv=5, scoring='f1_macro', n_jobs=4, verbose=1, refit=True)\n",
    "grid_dense_svm.fit(X_train, y_train)\n",
    "print(\"\\n\", \"=\" * 60, \"\\n\")\n",
    "\n",
    "print(f\"Best LR F1: {grid_dense_lr.best_score_:.4f}\")\n",
    "print(f\"Best LR Params: {grid_dense_lr.best_params_}\")\n",
    "print()\n",
    "print(f\"Best SVM F1: {grid_dense_svm.best_score_:.4f}\")\n",
    "print(f\"Best SVM Params: {grid_dense_svm.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16bf3e-fb66-4903-aee7-80b380a00b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Extraemos los resultados del GridSearch de FastText\n",
    "df_fasttext = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# 2. FILTRADO: Nos quedamos solo con la mejor configuración estructural\n",
    "# {'vect__vector_size': 300, 'vect__window': 5}\n",
    "subset_ft = df_fasttext[\n",
    "    (df_fasttext['param_vect__vector_size'] == 300) &\n",
    "    (df_fasttext['param_vect__window'] == 5)\n",
    "]\n",
    "\n",
    "# 3. GRÁFICA\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.lineplot(\n",
    "    data=subset_ft,\n",
    "    x='param_clf__C',\n",
    "    y='mean_test_score',\n",
    "    marker='o',\n",
    "    markersize=8,\n",
    "    color='green',  # Usamos verde para distinguir de SVM(rojo)/LR(azul)\n",
    "    label='FastText + Logistic Regression'\n",
    ")\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.title('FastText Performance: Impact of Regularization C\\n(Fixed: Vector Size=300, Window=5)')\n",
    "plt.xlabel('Regularization C (Log Scale)')\n",
    "plt.ylabel('F1 Macro Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282aaa92-0bf8-4d95-8454-ba0e7742db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model = grid_dense_svm.best_estimator_\n",
    "dense_test_preds = dense_model.predict(X_test)\n",
    "test_ids = df_test['id_EXIST'].values\n",
    "\n",
    "generate_submission_json(test_ids, dense_test_preds, \"../results/submissions/dense_preds.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd01d05-1b38-443e-930e-6d8c6c582fe8",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "For the dense representation approach, we trained **FastText word embeddings from scratch** on the task corpus and used a **MeanEmbeddingVectorizer** to convert each tweet into a fixed-length vector by averaging all word embeddings. The hyperparameter search was conducted using a **PredefinedSplit** (train on training set, validate on validation set), exploring:\n",
    "\n",
    "| Hyperparameter | Values Tested |\n",
    "|---|---|\n",
    "| `vect__vector_size` | 100, 300 |\n",
    "| `vect__window` | 3, 5 |\n",
    "| `clf__C` | 0.1, 1, 10 |\n",
    "| `clf__class_weight` | None, balanced |\n",
    "\n",
    "**Summary of best configurations:**\n",
    "\n",
    "| Model | Best F1-Macro (Val) | Embedding Settings | Classifier Settings |\n",
    "|---|---|---|---|\n",
    "| **Logistic Regression** | **≈ 0.612** | vector_size=100, window=5 | C=10, class_weight=None |\n",
    "| **SVM (LinearSVC)** | **≈ 0.632** | vector_size=100, window=3 | C=10, class_weight=None |\n",
    "\n",
    "**Key observations:**\n",
    "\n",
    "1. **Dense models significantly underperform sparse models** (best dense F1 ≈ 0.632 vs. best sparse F1 ≈ 0.759, a gap of ~12.7 F1 points). This is a substantial and consistent difference.\n",
    "\n",
    "2. **Why dense representations struggle on this task:**\n",
    "   - **Information loss from averaging:** Mean-pooling word embeddings into a single vector destroys word-order, negation, and sarcasm cues that are critical for sexism detection.\n",
    "   - **Small corpus for embedding training:** FastText was trained from scratch on only ~10 000 tweets, which is insufficient to learn high-quality distributional semantics. Pre-trained embeddings (e.g., Twitter-specific GloVe or multilingual models) would likely perform better.\n",
    "   - **Low embedding dimensionality** (100 dimensions performed better than 300), suggesting that the small corpus cannot support higher-dimensional embeddings without overfitting.\n",
    "\n",
    "3. **`class_weight=None` preferred in dense models**, unlike sparse models. With mean embeddings, the class boundaries are less well-defined, and balanced weighting introduces noise rather than helping.\n",
    "\n",
    "4. **SVM outperforms LR in the dense setting** (opposite of sparse), likely because SVM's max-margin approach handles the lower-quality, more overlapping feature space of mean embeddings better.\n",
    "\n",
    "**Decision:** The sparse approach (TF-IDF + Logistic Regression) is decisively superior. We will use the **best sparse model** as our primary submission model, but will also generate dense model predictions for comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8d5ce-ec48-4014-bafb-98d68b2aa501",
   "metadata": {},
   "source": [
    "# 6. Evaluation of Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1006a8-e1e3-4326-afd0-89d961a1ae2d",
   "metadata": {},
   "source": [
    "## 6.1 Quantitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# --- Evaluate best SPARSE model on test set ---\n",
    "sparse_model = grid_lr.best_estimator_\n",
    "sparse_test_preds = sparse_model.predict(X_test)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BEST SPARSE MODEL — Logistic Regression + TF-IDF\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBest CV F1-Macro: {grid_lr.best_score_:.4f}\")\n",
    "print(f\"Best Params: {grid_lr.best_params_}\\n\")\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, sparse_test_preds, target_names=['NOT SEXIST', 'SEXIST']))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sparse confusion matrix\n",
    "cm_sparse = confusion_matrix(y_test, sparse_test_preds)\n",
    "disp_sparse = ConfusionMatrixDisplay(cm_sparse, display_labels=['NOT SEXIST', 'SEXIST'])\n",
    "disp_sparse.plot(ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title('Sparse Model (LR + TF-IDF)')\n",
    "\n",
    "# --- Evaluate best DENSE model on test set ---\n",
    "dense_model = grid_dense_svm.best_estimator_\n",
    "dense_test_preds = dense_model.predict(X_test)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEST DENSE MODEL — SVM + FastText\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBest Val F1-Macro: {grid_dense_svm.best_score_:.4f}\")\n",
    "print(f\"Best Params: {grid_dense_svm.best_params_}\\n\")\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, dense_test_preds, target_names=['NOT SEXIST', 'SEXIST']))\n",
    "\n",
    "# Dense confusion matrix\n",
    "cm_dense = confusion_matrix(y_test, dense_test_preds)\n",
    "disp_dense = ConfusionMatrixDisplay(cm_dense, display_labels=['NOT SEXIST', 'SEXIST'])\n",
    "disp_dense.plot(ax=axes[1], cmap='Oranges')\n",
    "axes[1].set_title('Dense Model (SVM + FastText)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca68ff3d",
   "metadata": {},
   "source": [
    "**Analysis of Quantitative Results:**\n",
    "\n",
    "The test-set evaluation confirms the cross-validation findings:\n",
    "\n",
    "1. **Sparse model (LR + TF-IDF)** achieves the highest F1-Macro on the test set, demonstrating good generalization from training to unseen data.\n",
    "\n",
    "2. **Dense model (SVM + FastText)** shows a significant performance drop compared to the sparse approach, confirming that mean-pooled FastText embeddings trained from scratch on a small corpus are not competitive with well-tuned TF-IDF features for this task.\n",
    "\n",
    "3. The confusion matrices reveal where each model makes errors — the sparse model generally has a better balance between precision and recall for both classes, while the dense model tends to struggle more with correctly identifying sexist content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ad2ba-8f5d-443c-8550-cde49a024d47",
   "metadata": {},
   "source": [
    "## 6.2 Qualitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis: look at misclassified examples from the best sparse model\n",
    "import pandas as pd\n",
    "\n",
    "# Build a DataFrame for error analysis\n",
    "error_df = pd.DataFrame({\n",
    "    'text': X_test,\n",
    "    'true_label': y_test.values,\n",
    "    'predicted': sparse_test_preds\n",
    "})\n",
    "\n",
    "# Identify misclassifications\n",
    "errors = error_df[error_df['true_label'] != error_df['predicted']]\n",
    "label_map = {0: 'NOT SEXIST', 1: 'SEXIST'}\n",
    "\n",
    "print(f\"Total test samples: {len(error_df)}\")\n",
    "print(f\"Total errors: {len(errors)} ({len(errors)/len(error_df)*100:.1f}%)\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93cadbf-ad62-4b44-b804-12b9a59ede6d",
   "metadata": {},
   "source": [
    "### 6.2.1 Top Misclassified Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715781cd-a2e0-42ed-a938-742b83d0cabf",
   "metadata": {},
   "source": [
    "#### 6.2.1.1 SEXIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14515132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Negatives: tweets that ARE sexist but model predicted NOT SEXIST\n",
    "false_negatives = errors[\n",
    "    (errors['true_label'] == 1) & (errors['predicted'] == 0)\n",
    "]\n",
    "\n",
    "print(f\"False Negatives (Sexist → predicted Not Sexist): {len(false_negatives)}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSample misclassified SEXIST tweets:\\n\")\n",
    "for i, (_, row) in enumerate(false_negatives.head(10).iterrows()):\n",
    "    print(f\"[{i+1}] {row['text'][:200]}\")\n",
    "    print(f\"    True: {label_map[row['true_label']]} | Predicted: {label_map[row['predicted']]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0c5f0-501e-4711-9d23-6189d42dda15",
   "metadata": {},
   "source": [
    "#### 6.2.1.2 NOT SEXIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6221d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positives: tweets that are NOT sexist but model predicted SEXIST\n",
    "false_positives = errors[\n",
    "    (errors['true_label'] == 0) & (errors['predicted'] == 1)\n",
    "]\n",
    "\n",
    "print(f\"False Positives (Not Sexist → predicted Sexist): {len(false_positives)}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSample misclassified NOT SEXIST tweets:\\n\")\n",
    "for i, (_, row) in enumerate(false_positives.head(10).iterrows()):\n",
    "    print(f\"[{i+1}] {row['text'][:200]}\")\n",
    "    print(f\"    True: {label_map[row['true_label']]} | Predicted: {label_map[row['predicted']]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4d557",
   "metadata": {},
   "source": [
    "**Analysis of False Positives (incorrectly flagged as sexist):**\n",
    "\n",
    "These are non-sexist tweets that our model incorrectly classified as sexist. Common patterns include:\n",
    "- **Tweets discussing gender-related topics** without expressing sexism — mentioning words like \"women\", \"feminism\", etc. triggers the model\n",
    "- **Quotes or reporting of sexism** that the model mistakes for expressing sexism\n",
    "- **Ironic or sarcastic tweets** where the intent is anti-sexist but the surface-level language contains terms associated with sexism\n",
    "- **Tweets in mixed language** where the model misinterprets the overall sentiment\n",
    "\n",
    "These error patterns highlight a fundamental limitation of bag-of-words approaches: they cannot capture **pragmatic meaning, intent, or context** beyond surface-level word frequencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2954ce-e056-4471-b752-4c435462cf91",
   "metadata": {},
   "source": [
    "# 7. Conclusions\n",
    "\n",
    "This notebook presented a complete pipeline for **sexism identification in tweets** (EXIST 2025, Task 1, Subtask 1.1), a **binary text classification** task evaluated using **F1-Macro** on a bilingual (English/Spanish) dataset of ~10 000 tweets.\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### 1. Preprocessing matters — but less is more\n",
    "Among six preprocessing strategies tested (raw, lowercase, no punctuation, stopword removal, stemming, lemmatization), simple **lowercasing** achieved the best F1-score (0.6506) while maintaining computational efficiency. More aggressive preprocessing (stemming, lemmatization, stopword removal) degraded performance by discarding discriminative information present in Twitter-specific text patterns.\n",
    "\n",
    "### 2. Unigrams outperform higher-order n-grams\n",
    "Despite intuition that multi-word phrases should help capture sexist expressions, **unigrams alone** consistently outperformed bigram and trigram configurations. This is attributable to the **feature space explosion** (37k → 325k tokens) coupled with insufficient training data (~6 900 samples), causing severe overfitting with higher-order n-grams.\n",
    "\n",
    "### 3. Sparse representations decisively outperform dense embeddings\n",
    "- **Best sparse model** (TF-IDF + Logistic Regression): **F1-Macro ≈ 0.759**\n",
    "- **Best dense model** (FastText + LinearSVC): **F1-Macro ≈ 0.632**\n",
    "\n",
    "The ~12.7 F1-point gap demonstrates that for small, domain-specific datasets, well-tuned TF-IDF features capture more task-relevant information than from-scratch word embeddings. The mean-pooling strategy used for dense representations further degrades performance by destroying word-order and negation information critical for detecting subtle sexism.\n",
    "\n",
    "### 4. Class balancing is essential for sparse models\n",
    "Both sparse classifiers achieved their best performance with `class_weight='balanced'`, compensating for the class imbalance in the dataset. Interestingly, dense models preferred `class_weight=None`, suggesting that the lower-quality embedding space does not benefit from reweighting.\n",
    "\n",
    "### 5. TF-IDF configuration is as important as classifier choice\n",
    "Key TF-IDF parameters — such as `binary`, `sublinear_tf`, `min_df`, and `max_features` — had substantial impact on performance. Retaining a large vocabulary (`max_features=35 000`) was important, while the binary vs. frequency trade-off depended on the classifier.\n",
    "\n",
    "## Limitations and Future Work\n",
    "\n",
    "1. **Pre-trained embeddings:** Using large pre-trained models (e.g., multilingual BERT, Twitter-specific embeddings) would likely bridge the gap between sparse and dense approaches.\n",
    "2. **Contextual models:** Transformer-based architectures (BERT, RoBERTa, XLM-R) could capture pragmatic meaning and sarcasm that BoW models miss.\n",
    "3. **Advanced preprocessing:** Tweet-specific tokenizers (e.g., TweetTokenizer from NLTK) and emoji/hashtag-aware preprocessing could improve feature quality.\n",
    "4. **Ensemble methods:** Combining sparse and dense model predictions could potentially outperform either approach alone.\n",
    "5. **Data augmentation:** Given the limited dataset size, techniques like back-translation or synonym replacement could help reduce overfitting.\n",
    "\n",
    "## Final Submission\n",
    "\n",
    "The best-performing model (Logistic Regression + TF-IDF with optimized hyperparameters) was used to generate predictions on the test set, saved as `sparse_preds.json` for submission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf9c7c-802b-45f6-8e8b-2fb42da3d53d",
   "metadata": {},
   "source": [
    "# End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
