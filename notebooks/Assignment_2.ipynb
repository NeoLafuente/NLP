{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787e2655",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79248cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from time import time, sleep\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score, PredefinedSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Gensim for Dense Embeddings\n",
    "from gensim.models import FastText\n",
    "\n",
    "# NLTK downloads\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e83ee85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "\n",
      "Total Samples - Training: 6920\n",
      "final_label_str\n",
      "YES    3553\n",
      "NO     3367\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total Samples - Validation: 1038\n",
      "final_label_str\n",
      "YES    559\n",
      "NO     479\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total Samples - Test: 2076\n",
      "No labels available for this data split\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. DATA LOADING & LABEL ENGINEERING\n",
    "# ==========================================\n",
    "def load_and_parse_data(filepath):\n",
    "    \"\"\"\n",
    "    Parses nested JSON and applies Majority Voting for labels.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    df = df.reset_index(drop = True).rename(columns={'index': 'id_EXIST'})\n",
    "    \n",
    "    # Label Processing (Majority Voting)\n",
    "    if 'labels_task1_1' in df.columns:\n",
    "        def get_majority_vote(labels_list):\n",
    "            if not isinstance(labels_list, list): return np.nan\n",
    "            counts = pd.Series(labels_list).value_counts()\n",
    "            # Tie-breaking: Prioritize 'YES' (Sexism) if tie\n",
    "            if len(counts) > 1 and counts.iloc[0] == counts.iloc[1]:\n",
    "                if 'YES' in counts.index[:2]: return 'YES'\n",
    "            return counts.idxmax()\n",
    "        \n",
    "        df['final_label_str'] = df['labels_task1_1'].apply(get_majority_vote)\n",
    "        df['label'] = df['final_label_str'].map({'YES': 1, 'NO': 0})\n",
    "        df = df.dropna(subset=['label'])\n",
    "        df['label'] = df['label'].astype(int)\n",
    "        \n",
    "    return df\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "df_train = load_and_parse_data('../data/training/EXIST2025_training.json')\n",
    "df_val = load_and_parse_data('../data/dev/EXIST2025_dev.json')\n",
    "df_test = load_and_parse_data('../data/test/EXIST2025_test_clean.json')\n",
    "\n",
    "print(f\"\\nTotal Samples - Training: {len(df_train)}\")\n",
    "if 'final_label_str' in df_train.columns:\n",
    "    print(df_train['final_label_str'].value_counts())\n",
    "\n",
    "print(f\"\\nTotal Samples - Validation: {len(df_val)}\")\n",
    "if 'final_label_str' in df_val.columns:\n",
    "    print(df_val['final_label_str'].value_counts())\n",
    "\n",
    "print(f\"\\nTotal Samples - Test: {len(df_test)}\")\n",
    "try:\n",
    "    print(df_test['final_label_str'].value_counts())\n",
    "except:\n",
    "    print(f\"No labels available for this data split\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "850844e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "stop_words = set(stopwords.words('english')) | set(stopwords.words('spanish'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text, strategy='raw'):\n",
    "\n",
    "    text_processed = str(text)\n",
    "    \n",
    "    if strategy == 'raw':\n",
    "        return text_processed\n",
    "        \n",
    "    if strategy == 'lowercase':\n",
    "        return text_processed.lower()\n",
    "        \n",
    "    if strategy == 'no_punct':\n",
    "        # Remove punctuation, keep spaces\n",
    "        text_processed = re.sub(r'[^\\w\\s]', '', text_processed)\n",
    "        return text_processed.lower()\n",
    "        \n",
    "    if strategy == 'no_stopwords':\n",
    "        text_processed = text_processed.lower()\n",
    "        words = text_processed.split()\n",
    "        return \" \".join([w for w in words if w not in stop_words])\n",
    "        \n",
    "    if strategy == 'stemmed':\n",
    "        text_processed = text_processed.lower()\n",
    "        words = text_processed.split()\n",
    "        return \" \".join([stemmer.stem(w) for w in words])\n",
    "        \n",
    "    if strategy == 'lemmatized':\n",
    "        text_processed = text_processed.lower()\n",
    "        # Simple tokenization for lemmatizer\n",
    "        words = text_processed.split() \n",
    "        return \" \".join([lemmatizer.lemmatize(w) for w in words])\n",
    "        \n",
    "    return text_processed\n",
    "\n",
    "# Preprocess the Tweet columns\n",
    "df_train['text_clean'] = df_train['tweet'].apply(lambda x: preprocess_text(x, 'lowercase'))\n",
    "df_val['text_clean'] = df_val['tweet'].apply(lambda x: preprocess_text(x, 'lowercase'))\n",
    "df_test['text_clean'] = df_test['tweet'].apply(lambda x: preprocess_text(x, 'lowercase'))\n",
    "\n",
    "X_train_raw = df_train['tweet']\n",
    "y_train = df_train['label']\n",
    "\n",
    "X_val_raw = df_val['tweet']\n",
    "y_val = df_val['label']\n",
    "\n",
    "X_test_raw = df_test['tweet']\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=2):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.min_freq = min_freq\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 2\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                if frequencies[word] == self.min_freq:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "    def tokenize(self, text):\n",
    "        return re.findall(r'\\w+', text)\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokenized_text]\n",
    "\n",
    "vocab = Vocabulary(min_freq=2)\n",
    "vocab.build_vocabulary(df_train['text_clean'].tolist())\n",
    "\n",
    "class EXISTDataset(Dataset):\n",
    "    def __init__(self, df, vocab, max_len=64):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.iloc[index]['text_clean']\n",
    "        # Check if label exists (it might not for test set)\n",
    "        if 'label' in self.df.columns:\n",
    "            label = self.df.iloc[index]['label']\n",
    "        else:\n",
    "            label = -1 # Placeholder\n",
    "        \n",
    "        tokens = self.vocab.numericalize(text)\n",
    "        \n",
    "        # Padding/Truncating\n",
    "        if len(tokens) < self.max_len:\n",
    "            tokens.extend([self.vocab.stoi[\"<PAD>\"]] * (self.max_len - len(tokens)))\n",
    "        else:\n",
    "            tokens = tokens[:self.max_len]\n",
    "            \n",
    "        return torch.tensor(tokens), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Datasets and Loaders\n",
    "train_dataset = EXISTDataset(df_train, vocab)\n",
    "val_dataset = EXISTDataset(df_val, vocab)\n",
    "test_dataset = EXISTDataset(df_test, vocab)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcec046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTMClassifier(\n",
      "  (embedding): Embedding(11013, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 3. Bidirectional LSTM Model\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n",
    "                           bidirectional=True, batch_first=True, dropout=0.5 if n_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # output: [batch_size, seq_len, hidden_dim * 2]\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Max pooling over the sequence dimension\n",
    "        pooled = torch.max(output, dim=1)[0]\n",
    "        \n",
    "        return self.fc(self.dropout(pooled))\n",
    "# Initialization\n",
    "VOCAB_SIZE = len(vocab.stoi)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 2\n",
    "N_LAYERS = 2\n",
    "model = BiLSTMClassifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f454638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuración de dispositivo (GPU si está disponible)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# 2. Pérdida y Optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 3. Bucle de entrenamiento\n",
    "EPOCHS = 10\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"Entrenando en: {device}\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for texts, labels in train_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Métrica de accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "    # Validación\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}:')\n",
    "    print(f'  Train Loss: {train_loss/len(train_loader):.4f} | Accuracy: {100.*correct/total:.2f}%')\n",
    "    print(f'  Val   Loss: {val_loss/len(val_loader):.4f} | Accuracy: {100.*val_correct/val_total:.2f}%')\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Guardar el mejor modelo\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(\"¡Modelo guardado!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
