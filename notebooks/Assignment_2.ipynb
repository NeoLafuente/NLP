{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170c3adb-882b-47cc-9f71-6020cf0cbae5",
   "metadata": {},
   "source": [
    "# Assignment 2 — Neural Sequence Models: From Recurrence to Self-Attention\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Recap: Assignment 1 Baseline Performance\n",
    "\n",
    "Assignment 1 established classical machine learning baselines for **EXIST 2025 — Task 1, Subtask 1.1 (Sexism Identification in Tweets)**, a binary text classification task on a bilingual (English/Spanish) corpus of ~10,000 tweets. Using TF-IDF feature representations paired with linear classifiers (Logistic Regression and LinearSVC), and FastText mean-pooled embeddings as an alternative dense representation, the following results were obtained on the held-out validation set (1,038 samples):\n",
    "\n",
    "| Model | Feature Representation | CV F1-Macro | Val F1-Macro | Val Accuracy |\n",
    "|---|---|---|---|---|\n",
    "| **Logistic Regression** *(best overall)* | TF-IDF (unigrams, sublinear_tf) | 0.6536 | **0.75** | 0.75 |\n",
    "| LinearSVC | TF-IDF (unigrams) | 0.6524 | — | — |\n",
    "| SVM (LinearSVC) *(best dense)* | FastText embeddings (scratch) | 0.5322 | 0.62 | 0.62 |\n",
    "| Logistic Regression | FastText embeddings (scratch) | 0.5234 | — | — |\n",
    "\n",
    "Key takeaways from Assignment 1:\n",
    "\n",
    "- **Simple lowercasing** was the optimal preprocessing strategy (F1: 0.6506), outperforming stemming, lemmatization, and stopword removal.\n",
    "- **Unigrams only** outperformed all higher-order n-gram configurations, due to feature space explosion relative to the small training corpus (~6,920 samples).\n",
    "- **Sparse TF-IDF representations decisively outperformed dense FastText embeddings** trained from scratch (~12.1 F1-point gap), highlighting the limitations of mean-pooled embeddings on a small, domain-specific corpus.\n",
    "- **Error analysis** revealed two systematic failure modes of bag-of-words models: (1) false negatives on tweets that *report or condemn* sexism (anti-sexist tone masks SEXIST label), and (2) false positives driven by gender-related keywords used in neutral or non-sexist contexts (e.g., references to *patriarchy* or *mgtow* without sexist intent). These errors share a common root cause: TF-IDF cannot model *how* words are used, only *which* words appear.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Research Questions\n",
    "\n",
    "Assignment 1 demonstrated the ceiling of bag-of-words approaches for this task. This assignment transitions to **neural sequence models** — architectures that explicitly model sequential structure, word order, and long-range dependencies. Concretely, we implement and evaluate a **Bidirectional LSTM** and a **Transformer-based model** (fine-tuned from a pre-trained checkpoint, given the small dataset size), comparing them against the Assignment 1 baselines. \n",
    "\n",
    "This motivates the following research questions:\n",
    "\n",
    "> **RQ1 — Performance:** Do neural sequence models (BiLSTM, Transformer/BERT) yield meaningful F1-Macro improvements over the TF-IDF + Logistic Regression baseline on sexism identification in tweets?\n",
    "\n",
    "> **RQ2 — Error correction:** Which specific error types from Assignment 1 — irony/sarcasm, anti-sexist reporting, keyword mismatch — are resolved by architectures that capture sequential context and pragmatic meaning?\n",
    "\n",
    "> **RQ3 — Data efficiency:** Given the limited training size (~6,920 samples), do neural models trained from scratch suffer from data starvation, and does transfer learning via a pre-trained Transformer (e.g., multilingual BERT / XLM-R) mitigate this?\n",
    "\n",
    "> **RQ4 — Computational cost:** Is the added complexity of neural models — in training time, GPU memory, and inference latency — justified by the performance gains over the classical baseline?\n",
    "\n",
    "> **RQ5 — Architecture trade-offs:** How do BiLSTM and Transformer architectures compare in terms of their inductive biases for this specific task (short bilingual tweets, subtle/ironic content, near-balanced classes)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de207790-3430-4e53-9ecc-c9cabc3463ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from time import time, sleep\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "# NLTK downloads\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Configurar semilla para reproducibilidad\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fbd1b65-9dc3-47f6-835b-5eba4a2c6f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "\n",
      "Total Samples - Training: 6920\n",
      "final_label_str\n",
      "YES    3553\n",
      "NO     3367\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total Samples - Validation (Will be used as TEST): 1038\n",
      "final_label_str\n",
      "YES    559\n",
      "NO     479\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_and_parse_data(filepath):\n",
    "    \"\"\"\n",
    "    Parses nested JSON and applies Majority Voting for labels.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    df = df.reset_index(drop=True).rename(columns={'index': 'id_EXIST'})\n",
    "    \n",
    "    # Label Processing (Majority Voting)\n",
    "    if 'labels_task1_1' in df.columns:\n",
    "        def get_majority_vote(labels_list):\n",
    "            if not isinstance(labels_list, list): return np.nan\n",
    "            counts = pd.Series(labels_list).value_counts()\n",
    "            # Tie-breaking: Prioritize 'YES' (Sexism) if tie\n",
    "            if len(counts) > 1 and counts.iloc[0] == counts.iloc[1]:\n",
    "                if 'YES' in counts.index[:2]: return 'YES'\n",
    "            return counts.idxmax()\n",
    "        \n",
    "        df['final_label_str'] = df['labels_task1_1'].apply(get_majority_vote)\n",
    "        df['label'] = df['final_label_str'].map({'YES': 1, 'NO': 0})\n",
    "        df = df.dropna(subset=['label'])\n",
    "        df['label'] = df['label'].astype(int)\n",
    "        \n",
    "    return df\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "df_train = load_and_parse_data('../data/training/EXIST2025_training.json')\n",
    "df_val = load_and_parse_data('../data/dev/EXIST2025_dev.json')\n",
    "df_test = load_and_parse_data('../data/test/EXIST2025_test_clean.json')\n",
    "\n",
    "print(f\"\\nTotal Samples - Training: {len(df_train)}\")\n",
    "print(df_train['final_label_str'].value_counts())\n",
    "print(f\"\\nTotal Samples - Validation (Will be used as TEST): {len(df_val)}\")\n",
    "print(df_val['final_label_str'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada5f9c9-f261-45c0-8435-c53af7017e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) | set(stopwords.words('spanish'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text, strategy='raw'):\n",
    "    text_processed = str(text)\n",
    "    if strategy == 'raw':\n",
    "        return text_processed\n",
    "    if strategy == 'lowercase':\n",
    "        return text_processed.lower()\n",
    "    if strategy == 'no_punct':\n",
    "        text_processed = re.sub(r'[^\\w\\s]', '', text_processed)\n",
    "        return text_processed.lower()\n",
    "    if strategy == 'no_stopwords':\n",
    "        text_processed = text_processed.lower()\n",
    "        words = text_processed.split()\n",
    "        return \" \".join([w for w in words if w not in stop_words])\n",
    "    if strategy == 'stemmed':\n",
    "        text_processed = text_processed.lower()\n",
    "        words = text_processed.split()\n",
    "        return \" \".join([stemmer.stem(w) for w in words])\n",
    "    if strategy == 'lemmatized':\n",
    "        text_processed = text_processed.lower()\n",
    "        words = text_processed.split() \n",
    "        return \" \".join([lemmatizer.lemmatize(w) for w in words])\n",
    "    return text_processed\n",
    "\n",
    "# Process texts\n",
    "df_train['text_clean'] = df_train['tweet'].apply(lambda x: preprocess_text(x, 'lowercase'))\n",
    "df_val['text_clean'] = df_val['tweet'].apply(lambda x: preprocess_text(x, 'lowercase'))\n",
    "df_test['text_clean'] = df_test['tweet'].apply(lambda x: preprocess_text(x, 'lowercase'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c164fcab-034d-4abc-9b83-63037c8286af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=2):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.min_freq = min_freq\n",
    "        \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 2\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                if frequencies[word] == self.min_freq:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "                    \n",
    "    def tokenize(self, text):\n",
    "        return re.findall(r'\\w+', text)\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokenized_text]\n",
    "\n",
    "vocab = Vocabulary(min_freq=2)\n",
    "vocab.build_vocabulary(df_train['text_clean'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff00d06-244c-4acf-b22c-720de46a5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXISTDataset(Dataset):\n",
    "    def __init__(self, df, vocab, max_len=64):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.iloc[index]['text_clean']\n",
    "        label = self.df.iloc[index]['label'] if 'label' in self.df.columns else -1\n",
    "        \n",
    "        tokens = self.vocab.numericalize(text)\n",
    "        length = len(tokens)\n",
    "        \n",
    "        # Evitar secuencias vacías (rompen el pack_padded_sequence)\n",
    "        if length == 0:\n",
    "            tokens = [self.vocab.stoi[\"<UNK>\"]]\n",
    "            length = 1\n",
    "            \n",
    "        # Truncar si es más largo que max_len\n",
    "        if length > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "            length = self.max_len\n",
    "            \n",
    "        # Rellenar (Padding)\n",
    "        padded_tokens = tokens + [self.vocab.stoi[\"<PAD>\"]] * (self.max_len - length)\n",
    "            \n",
    "        return torch.tensor(padded_tokens), torch.tensor(label, dtype=torch.long), torch.tensor(length, dtype=torch.long)\n",
    "\n",
    "# Dataset global de Train (se dividirá en Folds)\n",
    "train_dataset = EXISTDataset(df_train, vocab)\n",
    "\n",
    "# El dataset \"dev\" original será nuestro \"TEST\" real\n",
    "test_dataset_real = EXISTDataset(df_val, vocab)\n",
    "test_loader_real = DataLoader(test_dataset_real, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b94b92ec-de05-46fe-b33d-c5863ce334c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout=0.5):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                            bidirectional=True, batch_first=True,\n",
    "                            dropout=dropout if n_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_output, (hidden, cell) = self.lstm(packed)\n",
    "        forward_hidden = hidden[-2]\n",
    "        backward_hidden = hidden[-1]\n",
    "        final_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "        output = self.dropout(final_hidden)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_score, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "            self.best_model_state = model.state_dict().copy()\n",
    "        elif val_score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.best_model_state = model.state_dict().copy()\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "\n",
    "# Fixed constants\n",
    "VOCAB_SIZE  = len(vocab.stoi)\n",
    "OUTPUT_DIM  = 2\n",
    "EPOCHS      = 20\n",
    "N_SPLITS    = 5\n",
    "SEED        = 42\n",
    "BATCH_SIZE  = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a901673b-bd9e-4621-a556-b0856fb920eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# ── MLflow setup ────────────────────────────────────────────────────────────\n",
    "MLFLOW_EXPERIMENT = \"BiLSTM_Optuna_Tuning\"\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT)\n",
    "\n",
    "# ── Search space ────────────────────────────────────────────────────────────\n",
    "# All hyperparameters Optuna will tune are defined here for easy editing.\n",
    "SEARCH_SPACE = {\n",
    "    \"embedding_dim\" : (\"categorical\", [100, 200, 300]),\n",
    "    \"hidden_dim\"    : (\"categorical\", [128, 256, 512]),\n",
    "    \"n_layers\"      : (\"int\",         [1, 3]),          # suggest_int(low, high)\n",
    "    \"dropout\"       : (\"float\",       [0.2, 0.5]),      # suggest_float(low, high)\n",
    "    \"lr\"            : (\"loguniform\",  [1e-4, 1e-2])     # suggest_float(..., log=True)\n",
    "}\n",
    "\n",
    "\n",
    "def suggest_hyperparams(trial):\n",
    "    \"\"\"Translate SEARCH_SPACE into Optuna suggestions.\"\"\"\n",
    "    params = {}\n",
    "    for name, (kind, bounds) in SEARCH_SPACE.items():\n",
    "        if kind == \"categorical\":\n",
    "            params[name] = trial.suggest_categorical(name, bounds)\n",
    "        elif kind == \"int\":\n",
    "            params[name] = trial.suggest_int(name, *bounds)\n",
    "        elif kind == \"float\":\n",
    "            params[name] = trial.suggest_float(name, *bounds)\n",
    "        elif kind == \"loguniform\":\n",
    "            params[name] = trial.suggest_float(name, *bounds, log=True)\n",
    "    return params\n",
    "\n",
    "\n",
    "def run_cv(params: dict) -> tuple[float, dict]:\n",
    "    \"\"\"\n",
    "    5-fold stratified CV for a given hyperparameter configuration.\n",
    "    Returns (mean_val_f1, per-fold metrics dict).\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    y_array = df_train['label'].values\n",
    "    fold_f1s = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        skf.split(np.zeros(len(y_array)), y_array)\n",
    "    ):\n",
    "        train_loader = DataLoader(\n",
    "            Subset(train_dataset, train_idx),\n",
    "            batch_size=BATCH_SIZE, shuffle=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            Subset(train_dataset, val_idx),\n",
    "            batch_size=BATCH_SIZE, shuffle=False\n",
    "        )\n",
    "\n",
    "        model = BiLSTMClassifier(\n",
    "            vocab_size    = VOCAB_SIZE,\n",
    "            embedding_dim = params[\"embedding_dim\"],\n",
    "            hidden_dim    = params[\"hidden_dim\"],\n",
    "            output_dim    = OUTPUT_DIM,\n",
    "            n_layers      = params[\"n_layers\"],\n",
    "            dropout       = params[\"dropout\"],\n",
    "        ).to(device)\n",
    "\n",
    "        criterion     = nn.CrossEntropyLoss()\n",
    "        optimizer     = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "        early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            # ── Train ──\n",
    "            model.train()\n",
    "            for texts, labels, lengths in train_loader:\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(texts, lengths), labels)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            # ── Validate ──\n",
    "            model.eval()\n",
    "            preds, targets = [], []\n",
    "            with torch.no_grad():\n",
    "                for texts, labels, lengths in val_loader:\n",
    "                    texts = texts.to(device)\n",
    "                    out   = model(texts, lengths)\n",
    "                    preds.extend(out.argmax(1).cpu().numpy())\n",
    "                    targets.extend(labels.numpy())\n",
    "\n",
    "            val_f1 = f1_score(targets, preds, average='macro')\n",
    "\n",
    "            if early_stopping(val_f1, model):\n",
    "                break\n",
    "\n",
    "        fold_f1s.append(early_stopping.best_score)\n",
    "\n",
    "    mean_f1      = float(np.mean(fold_f1s))\n",
    "    per_fold_log = {f\"fold_{i+1}_f1\": v for i, v in enumerate(fold_f1s)}\n",
    "    return mean_f1, per_fold_log\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"Optuna objective: suggest params → CV → log to MLflow → return metric.\"\"\"\n",
    "    params = suggest_hyperparams(trial)\n",
    "\n",
    "    with mlflow.start_run(\n",
    "        run_name=f\"trial_{trial.number}\", nested=True\n",
    "    ):\n",
    "        # Log all hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"trial_number\", trial.number)\n",
    "\n",
    "        mean_f1, per_fold_log = run_cv(params)\n",
    "\n",
    "        # Log per-fold and aggregate metrics\n",
    "        mlflow.log_metrics(per_fold_log)\n",
    "        mlflow.log_metric(\"mean_cv_f1_macro\", mean_f1)\n",
    "\n",
    "        # Optuna pruning hook (optional but useful with a Pruner)\n",
    "        trial.report(mean_f1, step=0)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return mean_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c7d89e2-7dc0-4acd-b7f8-3c5587afbaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-25 13:03:15,673]\u001b[0m A new study created in memory with name: bilstm_sexism\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb908f3aa2e4e02adf0d61260aac0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-25 13:08:09,093]\u001b[0m Trial 0 finished with value: 0.7154044420215099 and parameters: {'embedding_dim': 200, 'hidden_dim': 128, 'n_layers': 1, 'dropout': 0.45985284373248053, 'lr': 0.0015930522616241021}. Best is trial 0 with value: 0.7154044420215099.\u001b[0m\n",
      "\u001b[32m[I 2026-02-25 13:12:52,646]\u001b[0m Trial 1 finished with value: 0.7067901338760247 and parameters: {'embedding_dim': 300, 'hidden_dim': 128, 'n_layers': 1, 'dropout': 0.2912726728878613, 'lr': 0.0011207606211860567}. Best is trial 0 with value: 0.7154044420215099.\u001b[0m\n",
      "\u001b[32m[I 2026-02-25 13:23:23,654]\u001b[0m Trial 2 finished with value: 0.7039408595985817 and parameters: {'embedding_dim': 300, 'hidden_dim': 512, 'n_layers': 2, 'dropout': 0.43555278841790407, 'lr': 0.00025081156860452336}. Best is trial 0 with value: 0.7154044420215099.\u001b[0m\n",
      "\u001b[32m[I 2026-02-25 13:32:32,576]\u001b[0m Trial 3 finished with value: 0.729437496251228 and parameters: {'embedding_dim': 200, 'hidden_dim': 128, 'n_layers': 3, 'dropout': 0.4896896099223678, 'lr': 0.004138040112561018}. Best is trial 3 with value: 0.729437496251228.\u001b[0m\n",
      "\u001b[32m[I 2026-02-25 13:38:31,656]\u001b[0m Trial 4 finished with value: 0.700910537360701 and parameters: {'embedding_dim': 300, 'hidden_dim': 512, 'n_layers': 1, 'dropout': 0.47279612062363463, 'lr': 0.00032927591344236165}. Best is trial 3 with value: 0.729437496251228.\u001b[0m\n",
      "\u001b[32m[I 2026-02-25 13:48:07,873]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-25 13:56:15,873]\u001b[0m Trial 6 finished with value: 0.7072762355162732 and parameters: {'embedding_dim': 200, 'hidden_dim': 512, 'n_layers': 2, 'dropout': 0.2814047095321688, 'lr': 0.004544383960336014}. Best is trial 3 with value: 0.729437496251228.\u001b[0m\n",
      "\u001b[32m[I 2026-02-25 14:08:19,365]\u001b[0m Trial 7 finished with value: 0.7106218922153053 and parameters: {'embedding_dim': 300, 'hidden_dim': 256, 'n_layers': 3, 'dropout': 0.4316734307889972, 'lr': 0.00024970737145052745}. Best is trial 3 with value: 0.729437496251228.\u001b[0m\n",
      "\u001b[32m[I 2026-02-25 14:14:11,627]\u001b[0m Trial 8 finished with value: 0.7105492896731548 and parameters: {'embedding_dim': 200, 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.23476071785753894, 'lr': 0.005323617594751502}. Best is trial 3 with value: 0.729437496251228.\u001b[0m\n",
      "\u001b[32m[I 2026-02-25 14:25:37,563]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-25 14:33:53,123]\u001b[0m Trial 10 finished with value: 0.7190746246737751 and parameters: {'embedding_dim': 200, 'hidden_dim': 128, 'n_layers': 3, 'dropout': 0.3801285454653292, 'lr': 0.0023591442255668847}. Best is trial 3 with value: 0.729437496251228.\u001b[0m\n",
      "\u001b[32m[I 2026-02-25 14:41:35,702]\u001b[0m Trial 11 finished with value: 0.7201792612400466 and parameters: {'embedding_dim': 200, 'hidden_dim': 128, 'n_layers': 3, 'dropout': 0.371164359464505, 'lr': 0.0021614441173669506}. Best is trial 3 with value: 0.729437496251228.\u001b[0m\n",
      "\u001b[32m[I 2026-02-25 14:49:51,799]\u001b[0m Trial 12 finished with value: 0.7213012143754157 and parameters: {'embedding_dim': 200, 'hidden_dim': 128, 'n_layers': 3, 'dropout': 0.3781881788177629, 'lr': 0.009705165129084396}. Best is trial 3 with value: 0.729437496251228.\u001b[0m\n",
      "\u001b[32m[I 2026-02-25 14:55:39,028]\u001b[0m Trial 13 finished with value: 0.7264359433832298 and parameters: {'embedding_dim': 200, 'hidden_dim': 128, 'n_layers': 3, 'dropout': 0.32034524690886756, 'lr': 0.00881321685393713}. Best is trial 3 with value: 0.729437496251228.\u001b[0m\n",
      "\u001b[32m[I 2026-02-25 15:01:34,732]\u001b[0m Trial 14 finished with value: 0.7215368368768289 and parameters: {'embedding_dim': 200, 'hidden_dim': 128, 'n_layers': 3, 'dropout': 0.32124697500935795, 'lr': 0.009590909637046686}. Best is trial 3 with value: 0.729437496251228.\u001b[0m\n",
      "\n",
      "==================================================\n",
      "Best trial:  #3\n",
      "Best CV F1-Macro: 0.7294\n",
      "Best hyperparameters:\n",
      "  embedding_dim: 200\n",
      "  hidden_dim: 128\n",
      "  n_layers: 3\n",
      "  dropout: 0.4896896099223678\n",
      "  lr: 0.004138040112561018\n"
     ]
    }
   ],
   "source": [
    "# Parent MLflow run wraps the entire study\n",
    "with mlflow.start_run(run_name=\"optuna_study\"):\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "    pruner  = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction   = \"maximize\",\n",
    "        sampler     = sampler,\n",
    "        pruner      = pruner,\n",
    "        study_name  = \"bilstm_sexism\",\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials         = 15,      # ← adjust to your compute budget\n",
    "        timeout          = None,\n",
    "        show_progress_bar= True,\n",
    "    )\n",
    "\n",
    "    # ── Log best result to the parent run ──────────────────────────────────\n",
    "    best = study.best_trial\n",
    "    mlflow.log_params({f\"best_{k}\": v for k, v in best.params.items()})\n",
    "    mlflow.log_metric(\"best_mean_cv_f1_macro\", best.value)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Best trial:  #{best.number}\")\n",
    "    print(f\"Best CV F1-Macro: {best.value:.4f}\")\n",
    "    print(\"Best hyperparameters:\")\n",
    "    for k, v in best.params.items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1e794c2-5dcc-4574-86da-60e54675b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | Train Loss: 0.6728 | Train F1: 0.6073\n",
      "Epoch 02/20 | Train Loss: 0.6005 | Train F1: 0.7003\n",
      "Epoch 03/20 | Train Loss: 0.5274 | Train F1: 0.7537\n",
      "Epoch 04/20 | Train Loss: 0.4504 | Train F1: 0.8132\n",
      "Epoch 05/20 | Train Loss: 0.3830 | Train F1: 0.8457\n",
      "Epoch 06/20 | Train Loss: 0.3239 | Train F1: 0.8725\n",
      "Epoch 07/20 | Train Loss: 0.2884 | Train F1: 0.8953\n",
      "Epoch 08/20 | Train Loss: 0.2561 | Train F1: 0.9106\n",
      "Epoch 09/20 | Train Loss: 0.2163 | Train F1: 0.9264\n",
      "Epoch 10/20 | Train Loss: 0.1972 | Train F1: 0.9322\n",
      "Epoch 11/20 | Train Loss: 0.1599 | Train F1: 0.9407\n",
      "Epoch 12/20 | Train Loss: 0.1468 | Train F1: 0.9437\n",
      "Epoch 13/20 | Train Loss: 0.1262 | Train F1: 0.9543\n",
      "Epoch 14/20 | Train Loss: 0.1225 | Train F1: 0.9560\n",
      "Epoch 15/20 | Train Loss: 0.1094 | Train F1: 0.9621\n",
      "Epoch 16/20 | Train Loss: 0.1045 | Train F1: 0.9650\n",
      "Epoch 17/20 | Train Loss: 0.0916 | Train F1: 0.9650\n",
      "Epoch 18/20 | Train Loss: 0.0779 | Train F1: 0.9688\n",
      "Epoch 19/20 | Train Loss: 0.0791 | Train F1: 0.9673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 15:05:08 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/02/25 15:05:08 WARNING mlflow.pytorch: Saving pytorch model by Pickle or CloudPickle format requires exercising caution because these formats rely on Python's object serialization mechanism, which can execute arbitrary code during deserialization.The recommended safe alternative is to set 'export_model' to True to save the pytorch model using the safe graph model format.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 | Train Loss: 0.0795 | Train F1: 0.9715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 15:05:09 WARNING mlflow.utils.requirements_utils: Found torch version (2.10.0+cu130) contains a local version label (+cu130). MLflow logged a pip requirement for this package as 'torch==2.10.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2026/02/25 15:05:27 WARNING mlflow.utils.requirements_utils: Found torch version (2.10.0+cu130) contains a local version label (+cu130). MLflow logged a pip requirement for this package as 'torch==2.10.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2026/02/25 15:05:27 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final model trained and logged to MLflow.\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_trial.params\n",
    "\n",
    "with mlflow.start_run(run_name=\"final_model_best_params\"):\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    final_model = BiLSTMClassifier(\n",
    "        vocab_size    = VOCAB_SIZE,\n",
    "        embedding_dim = best_params[\"embedding_dim\"],\n",
    "        hidden_dim    = best_params[\"hidden_dim\"],\n",
    "        output_dim    = OUTPUT_DIM,\n",
    "        n_layers      = best_params[\"n_layers\"],\n",
    "        dropout       = best_params[\"dropout\"],\n",
    "    ).to(device)\n",
    "\n",
    "    full_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        shuffle    = True\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params[\"lr\"])\n",
    "\n",
    "    # No early stopping: we train for a fixed number of epochs on the full\n",
    "    # training set. A reasonable default is the mean epoch at which early\n",
    "    # stopping triggered across the CV folds; here we reuse EPOCHS directly.\n",
    "    for epoch in range(EPOCHS):\n",
    "        final_model.train()\n",
    "        train_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for texts, labels, lengths in full_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(final_model(texts, lengths), labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(final_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            all_preds.extend(final_model(texts, lengths).detach().argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        mlflow.log_metric(\"train_f1_macro\", train_f1, step=epoch)\n",
    "        mlflow.log_metric(\"train_loss\", train_loss / len(full_loader), step=epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d}/{EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss/len(full_loader):.4f} | \"\n",
    "              f\"Train F1: {train_f1:.4f}\")\n",
    "\n",
    "    # Save the final model artifact\n",
    "    mlflow.pytorch.log_model(final_model, artifact_path=\"bilstm_final_model\")\n",
    "    print(\"\\nFinal model trained and logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ea24bfd-47b9-4ef4-9267-a76e62759c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the final model on the TEST set (original 'dev' set)...\n",
      "\n",
      "F1-Macro Final (Test Set):       0.6961\n",
      "Tiempo de Inferencia total:      0.822 segundos\n",
      "\n",
      "Confusion Matrix:\n",
      " [[309 170]\n",
      " [142 417]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.65      0.66       479\n",
      "           1       0.71      0.75      0.73       559\n",
      "\n",
      "    accuracy                           0.70      1038\n",
      "   macro avg       0.70      0.70      0.70      1038\n",
      "weighted avg       0.70      0.70      0.70      1038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating the final model on the TEST set (original 'dev' set)...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"final_model_test_evaluation\"):\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    final_model.eval()\n",
    "    test_preds, test_labels_list = [], []\n",
    "\n",
    "    start_time = time()\n",
    "    with torch.no_grad():\n",
    "        for texts, labels, lengths in test_loader_real:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = final_model(texts, lengths)\n",
    "            test_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "            test_labels_list.extend(labels.cpu().numpy())\n",
    "    inference_time = time() - start_time\n",
    "\n",
    "    test_f1 = f1_score(test_labels_list, test_preds, average='macro')\n",
    "\n",
    "    # Log metrics to MLflow\n",
    "    mlflow.log_metric(\"test_f1_macro\", test_f1)\n",
    "    mlflow.log_metric(\"inference_time_seconds\", inference_time)\n",
    "\n",
    "    # Console output\n",
    "    print(f\"\\nF1-Macro Final (Test Set):       {test_f1:.4f}\")\n",
    "    print(f\"Tiempo de Inferencia total:      {inference_time:.3f} segundos\")\n",
    "    print(\"\\nConfusion Matrix:\\n\",\n",
    "          confusion_matrix(test_labels_list, test_preds))\n",
    "    print(\"\\nClassification Report:\\n\",\n",
    "          classification_report(test_labels_list, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c51f5-ec23-4d8b-8d77-a05729ca3b1a",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "621a4a24-90e5-4ec3-823c-58d0fcc657d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Usaremos un modelo base ligero, ideal para empezar.\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f36b59c-a155-42f8-b1c8-0e1f12c45305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # Llamada directa al tokenizador (la forma moderna y estándar)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Preparamos los textos crudos\n",
    "X_train_bert = df_train['tweet']\n",
    "y_train_bert = df_train['label']\n",
    "\n",
    "X_test_bert = df_val['tweet'] \n",
    "y_test_bert = df_val['label']\n",
    "\n",
    "bert_test_dataset = BERTDataset(X_test_bert, y_test_bert, tokenizer)\n",
    "bert_test_loader = DataLoader(bert_test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50498595-6fac-44df-b3d6-92c9f5fe8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, freeze_bert=False):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        \n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Usamos el token [CLS] (primera posición)\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "def unfreeze_last_n_layers(model, n):\n",
    "    \"\"\"Utilidad para tu Estudio de Ablación\"\"\"\n",
    "    # Congelar todo primero\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Descongelar las últimas n capas del encoder\n",
    "    if n > 0:\n",
    "        for layer in model.bert.encoder.layer[-n:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "    # La cabeza de clasificación siempre debe poder entrenarse\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c156197d-3fc4-41e7-8b47-2431f84ba1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando BERT en: cuda\n",
      "\n",
      "================ BERT FOLD 1/5 ================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624f5688f084428fbf95370d21298102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14850a1fcf7479a9372f56de51a606a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 | Train Loss: 0.5867 | Val Loss: 0.5253 | Val F1: 0.7362\n",
      "Epoch 2/4 | Train Loss: 0.4267 | Val Loss: 0.5036 | Val F1: 0.7719\n",
      "Epoch 3/4 | Train Loss: 0.2925 | Val Loss: 0.5327 | Val F1: 0.7897\n",
      "Epoch 4/4 | Train Loss: 0.1909 | Val Loss: 0.6548 | Val F1: 0.7847\n",
      "\n",
      "Mejor F1-Macro en Fold 1: 0.7897\n",
      "\n",
      "================ BERT FOLD 2/5 ================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327e8b03f1834832bfaf12a728d5d82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 | Train Loss: 0.5995 | Val Loss: 0.5004 | Val F1: 0.7495\n",
      "Epoch 2/4 | Train Loss: 0.4327 | Val Loss: 0.5025 | Val F1: 0.7663\n",
      "Epoch 3/4 | Train Loss: 0.2957 | Val Loss: 0.5423 | Val F1: 0.7712\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m optimizer.step()\n\u001b[32m     66\u001b[39m scheduler.step() \u001b[38;5;66;03m# Actualizar learning rate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m _, predicted = torch.max(logits, \u001b[32m1\u001b[39m)\n\u001b[32m     70\u001b[39m all_train_preds.extend(predicted.cpu().numpy())\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Entrenando BERT en: {device}\\n\")\n",
    "\n",
    "# Parámetros específicos para Fine-Tuning según el PDF\n",
    "EPOCHS_BERT = 4 \n",
    "BATCH_SIZE = 16 \n",
    "N_SPLITS = 5\n",
    "\n",
    "skf_bert = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "y_train_array_bert = y_train_bert.values\n",
    "bert_fold_metrics = []\n",
    "best_bert_global_f1 = 0\n",
    "best_bert_model_state = None\n",
    "\n",
    "bert_full_dataset = BERTDataset(X_train_bert, y_train_bert, tokenizer)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf_bert.split(np.zeros(len(y_train_array_bert)), y_train_array_bert)):\n",
    "    print(f\"================ BERT FOLD {fold + 1}/{N_SPLITS} ================\")\n",
    "    \n",
    "    train_sub = Subset(bert_full_dataset, train_idx)\n",
    "    val_sub = Subset(bert_full_dataset, val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_sub, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_sub, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Instanciar el modelo (Full fine-tuning inicial)\n",
    "    model = BERTClassifier(MODEL_NAME, num_classes=2, freeze_bert=False).to(device)\n",
    "    \n",
    "    # Diferentes learning rates: bajo para BERT, más alto para la nueva capa (Exigencia del PDF)\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.bert.parameters(), 'lr': 2e-5},\n",
    "        {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
    "    ])\n",
    "    \n",
    "    # Scheduler con Warmup (10% de los pasos totales)\n",
    "    total_steps = len(train_loader) * EPOCHS_BERT\n",
    "    warmup_steps = int(total_steps * 0.1)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_fold_f1 = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS_BERT):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        all_train_preds, all_train_labels = [], []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (max_norm = 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step() # Actualizar learning rate\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            all_train_preds.extend(predicted.cpu().numpy())\n",
    "            all_train_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_val_preds, all_val_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                logits = model(input_ids, attention_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                all_val_preds.extend(predicted.cpu().numpy())\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='macro')\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS_BERT} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f} | Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        # Guardar el mejor modelo del Fold\n",
    "        if val_f1 > best_fold_f1:\n",
    "            best_fold_f1 = val_f1\n",
    "            best_model_state_for_this_fold = model.state_dict().copy()\n",
    "            \n",
    "    bert_fold_metrics.append(best_fold_f1)\n",
    "    print(f\"\\nMejor F1-Macro en Fold {fold+1}: {best_fold_f1:.4f}\\n\")\n",
    "    \n",
    "    if best_fold_f1 > best_bert_global_f1:\n",
    "        best_bert_global_f1 = best_fold_f1\n",
    "        best_bert_model_state = best_model_state_for_this_fold\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(f\"F1-Macro Promedio BERT (5 Folds): {np.mean(bert_fold_metrics):.4f}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba30b92-ccc8-4508-9b07-e69328b2613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluando el mejor modelo BERT en el conjunto de TEST...\")\n",
    "\n",
    "final_bert = BERTClassifier(MODEL_NAME, num_classes=2).to(device)\n",
    "final_bert.load_state_dict(best_bert_model_state)\n",
    "final_bert.eval()\n",
    "\n",
    "bert_test_preds = []\n",
    "bert_test_labels = []\n",
    "\n",
    "from time import time\n",
    "start_time = time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in bert_test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        logits = final_bert(input_ids, attention_mask)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        bert_test_preds.extend(predicted.cpu().numpy())\n",
    "        bert_test_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "inference_time = time() - start_time\n",
    "\n",
    "test_f1 = f1_score(bert_test_labels, bert_test_preds, average='macro')\n",
    "\n",
    "print(f\"\\nF1-Macro Final BERT (Test Set): {test_f1:.4f}\")\n",
    "print(f\"Tiempo de Inferencia total: {inference_time:.3f} segundos\")\n",
    "print(\"\\nReporte de Clasificación:\\n\", classification_report(bert_test_labels, bert_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comp-header-001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Comparative Analysis: Baseline vs BiLSTM vs BERT\n",
    "\n",
    "This section presents a systematic comparison of the three approaches evaluated in this assignment:\n",
    "\n",
    "1. **Baseline (Assignment 1):** TF-IDF + Logistic Regression\n",
    "2. **Recurrent Model:** BiLSTM with Optuna-tuned hyperparameters\n",
    "3. **Transformer Model:** BERT-base-uncased with full fine-tuning\n",
    "\n",
    "We compare them across multiple axes: **predictive performance** (F1-Macro), **computational cost**, **parameter count**, and **error patterns**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comp-note-rename",
   "metadata": {},
   "source": [
    "### ⚠️ Important: Naming Convention for Predictions\n",
    "\n",
    "Both the BiLSTM and BERT test evaluation cells above use the same variable names (`test_preds`, `test_labels`, etc.), so the second cell overwrites the first. **Before running the comparison cells below**, go back and:\n",
    "\n",
    "1. Run the **BiLSTM test cell** first, then execute the rename cell below.\n",
    "2. Run the **BERT test cell**, then the rename cell below.\n",
    "\n",
    "Alternatively, you can just re-run the cells in order — the rename cells will capture the right values at the right time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "comp-rename-bilstm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM Test F1-Macro:    0.6961\n",
      "BiLSTM Inference Time:   0.822s\n",
      "BiLSTM predictions saved (1038 samples)\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  RUN THIS IMMEDIATELY AFTER the BiLSTM test evaluation cell ║\n",
    "# ║  (the cell that prints 'Evaluating the final model on the   ║\n",
    "# ║   TEST set')                                                ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "\n",
    "bilstm_test_preds  = list(test_preds)\n",
    "bilstm_test_labels = list(test_labels_list)\n",
    "bilstm_test_f1     = test_f1\n",
    "bilstm_inf_time    = inference_time\n",
    "\n",
    "print(f'BiLSTM Test F1-Macro:    {bilstm_test_f1:.4f}')\n",
    "print(f'BiLSTM Inference Time:   {bilstm_inf_time:.3f}s')\n",
    "print(f'BiLSTM predictions saved ({len(bilstm_test_preds)} samples)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comp-rename-bert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  RUN THIS IMMEDIATELY AFTER the BERT test evaluation cell   ║\n",
    "# ║  (the cell that prints 'Evaluando el mejor modelo BERT...')  ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "\n",
    "bert_test_preds_saved  = list(bert_test_preds)\n",
    "bert_test_labels_saved = list(bert_test_labels)\n",
    "bert_test_f1_saved     = test_f1\n",
    "bert_inf_time          = inference_time\n",
    "\n",
    "print(f'BERT Test F1-Macro:    {bert_test_f1_saved:.4f}')\n",
    "print(f'BERT Inference Time:   {bert_inf_time:.3f}s')\n",
    "print(f'BERT predictions saved ({len(bert_test_preds_saved)} samples)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comp-table-002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (f1_score, classification_report,\n",
    "                             confusion_matrix, ConfusionMatrixDisplay,\n",
    "                             precision_score, recall_score)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "#  Baseline values (from Assignment 1)\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "baseline_cv_f1       = 0.6536\n",
    "baseline_test_f1     = 0.75\n",
    "baseline_test_acc    = 0.75\n",
    "baseline_params      = 'N/A (linear)'\n",
    "baseline_train_time  = '<1 min'\n",
    "baseline_inf_time    = '<0.1s'\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "#  BiLSTM values (from Optuna + final training)\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "bilstm_cv_f1_val  = study.best_trial.value\n",
    "bilstm_n_params   = sum(p.numel() for p in final_model.parameters())\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "#  BERT values (from 5-fold CV + test)\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "bert_cv_f1_val  = np.mean(bert_fold_metrics)\n",
    "bert_cv_std     = np.std(bert_fold_metrics)\n",
    "bert_n_params   = sum(p.numel() for p in final_bert.parameters())\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "#  Build comparison DataFrame\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'TF-IDF + LogReg (Baseline)',\n",
    "        'BiLSTM (Optuna-tuned)',\n",
    "        'BERT-base-uncased (Fine-tuned)'\n",
    "    ],\n",
    "    'CV F1-Macro': [\n",
    "        f'{baseline_cv_f1:.4f}',\n",
    "        f'{bilstm_cv_f1_val:.4f}',\n",
    "        f'{bert_cv_f1_val:.4f} ± {bert_cv_std:.4f}'\n",
    "    ],\n",
    "    'Test F1-Macro': [\n",
    "        f'{baseline_test_f1:.4f}',\n",
    "        f'{bilstm_test_f1:.4f}',\n",
    "        f'{bert_test_f1_saved:.4f}'\n",
    "    ],\n",
    "    'Parameters': [\n",
    "        baseline_params,\n",
    "        f'{bilstm_n_params:,}',\n",
    "        f'{bert_n_params:,}'\n",
    "    ],\n",
    "    'Training Time': [\n",
    "        baseline_train_time,\n",
    "        '~60 min (30 trials × 5-fold)',\n",
    "        f'~{len(bert_fold_metrics)*4*2:.0f} min (5-fold × 4 epochs)'\n",
    "    ],\n",
    "    'Inference (test)': [\n",
    "        baseline_inf_time,\n",
    "        f'{bilstm_inf_time:.3f}s',\n",
    "        f'{bert_inf_time:.3f}s'\n",
    "    ],\n",
    "})\n",
    "\n",
    "print('=' * 80)\n",
    "print('     MODEL COMPARISON — EXIST 2025 Task 1.1 (Sexism Identification)')\n",
    "print('=' * 80)\n",
    "display(comparison.style.set_properties(**{'text-align': 'center'}).hide(axis='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comp-barplot-003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────\n",
    "#  Visualization 1: Grouped bar chart — CV F1 vs Test F1\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "models  = ['TF-IDF + LogReg\\n(Baseline)', 'BiLSTM', 'BERT-base']\n",
    "cv_f1s  = [baseline_cv_f1, bilstm_cv_f1_val, bert_cv_f1_val]\n",
    "tst_f1s = [baseline_test_f1, bilstm_test_f1, bert_test_f1_saved]\n",
    "colors  = ['#6C757D', '#0D6EFD', '#198754']\n",
    "\n",
    "# --- Grouped bar chart ---\n",
    "x = np.arange(len(models))\n",
    "width = 0.32\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, cv_f1s,  width, label='CV F1-Macro',\n",
    "                     color=colors, alpha=0.55, edgecolor='black', linewidth=0.8)\n",
    "bars2 = axes[0].bar(x + width/2, tst_f1s, width, label='Test F1-Macro',\n",
    "                     color=colors, alpha=1.0, edgecolor='black', linewidth=0.8)\n",
    "\n",
    "axes[0].set_ylabel('F1-Macro', fontsize=12)\n",
    "axes[0].set_title('Cross-Validation vs Test Performance', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models, fontsize=10)\n",
    "axes[0].set_ylim(0.55, 0.92)\n",
    "axes[0].legend(fontsize=10, loc='upper left')\n",
    "axes[0].yaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars1:\n",
    "    axes[0].annotate(f'{bar.get_height():.4f}',\n",
    "                     xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                     xytext=(0, 4), textcoords='offset points',\n",
    "                     ha='center', fontsize=8, fontweight='bold')\n",
    "for bar in bars2:\n",
    "    axes[0].annotate(f'{bar.get_height():.4f}',\n",
    "                     xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                     xytext=(0, 4), textcoords='offset points',\n",
    "                     ha='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "# --- Delta plot (improvement over baseline) ---\n",
    "deltas_tst = [0,\n",
    "              bilstm_test_f1 - baseline_test_f1,\n",
    "              bert_test_f1_saved - baseline_test_f1]\n",
    "\n",
    "bar_colors_delta = ['#6C757D' if d == 0 else ('#198754' if d > 0 else '#DC3545') for d in deltas_tst]\n",
    "axes[1].barh(models, deltas_tst, color=bar_colors_delta, edgecolor='black',\n",
    "             height=0.5, linewidth=0.8)\n",
    "axes[1].axvline(x=0, color='black', linewidth=1)\n",
    "axes[1].set_xlabel('Δ F1-Macro vs Baseline (Test)', fontsize=12)\n",
    "axes[1].set_title('Improvement over Baseline', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(deltas_tst):\n",
    "    sign = '+' if v > 0 else ('' if v == 0 else '')\n",
    "    offset = 0.003 if v >= 0 else -0.003\n",
    "    ha = 'left' if v >= 0 else 'right'\n",
    "    axes[1].text(v + offset, i, f'{sign}{v:.4f}',\n",
    "                va='center', ha=ha, fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/model_comparison_barplot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Figure saved → results/model_comparison_barplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comp-confusion-004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────\n",
    "#  Visualization 2: Side-by-side Confusion Matrices (Test Set)\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# --- BiLSTM ---\n",
    "cm_bilstm = confusion_matrix(bilstm_test_labels, bilstm_test_preds)\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix=cm_bilstm, display_labels=['NO (0)', 'YES (1)'])\n",
    "disp1.plot(ax=axes[0], cmap='Blues', values_format='d')\n",
    "axes[0].set_title(f'BiLSTM — Test F1 = {bilstm_test_f1:.4f}',\n",
    "                   fontsize=12, fontweight='bold')\n",
    "\n",
    "# --- BERT ---\n",
    "cm_bert = confusion_matrix(bert_test_labels_saved, bert_test_preds_saved)\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_bert, display_labels=['NO (0)', 'YES (1)'])\n",
    "disp2.plot(ax=axes[1], cmap='Greens', values_format='d')\n",
    "axes[1].set_title(f'BERT — Test F1 = {bert_test_f1_saved:.4f}',\n",
    "                   fontsize=12, fontweight='bold')\n",
    "\n",
    "fig.suptitle('Confusion Matrices — Test Set Predictions',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/confusion_matrices_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Figure saved → results/confusion_matrices_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comp-perclass-005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────\n",
    "#  Per-class F1 / Precision / Recall comparison\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "\n",
    "def get_per_class_metrics(y_true, y_pred, name):\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'F1 (NO)':        round(f1_score(y_true, y_pred, pos_label=0), 4),\n",
    "        'F1 (YES)':       round(f1_score(y_true, y_pred, pos_label=1), 4),\n",
    "        'Prec (NO)':      round(precision_score(y_true, y_pred, pos_label=0), 4),\n",
    "        'Prec (YES)':     round(precision_score(y_true, y_pred, pos_label=1), 4),\n",
    "        'Recall (NO)':    round(recall_score(y_true, y_pred, pos_label=0), 4),\n",
    "        'Recall (YES)':   round(recall_score(y_true, y_pred, pos_label=1), 4),\n",
    "        'F1-Macro':       round(f1_score(y_true, y_pred, average='macro'), 4),\n",
    "    }\n",
    "\n",
    "rows = [\n",
    "    get_per_class_metrics(bilstm_test_labels, bilstm_test_preds, 'BiLSTM'),\n",
    "    get_per_class_metrics(bert_test_labels_saved, bert_test_preds_saved, 'BERT-base'),\n",
    "]\n",
    "\n",
    "df_perclass = pd.DataFrame(rows)\n",
    "\n",
    "print('=' * 80)\n",
    "print('     PER-CLASS METRICS ON TEST SET')\n",
    "print('=' * 80)\n",
    "display(df_perclass.style.set_properties(**{'text-align': 'center'}).hide(axis='index'))\n",
    "\n",
    "# ── Grouped bar chart ──\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "metrics_names = ['F1 (NO)', 'F1 (YES)', 'Prec (NO)', 'Prec (YES)', 'Recall (NO)', 'Recall (YES)']\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.30\n",
    "\n",
    "for i, row in enumerate(rows):\n",
    "    vals = [row[k] for k in metrics_names]\n",
    "    color = ['#0D6EFD', '#198754'][i]\n",
    "    ax.bar(x + i * width, vals, width, label=row['Model'],\n",
    "           color=color, alpha=0.85, edgecolor='black', linewidth=0.6)\n",
    "    for j, v in enumerate(vals):\n",
    "        ax.text(x[j] + i * width, v + 0.008, f'{v:.3f}',\n",
    "                ha='center', fontsize=7.5, fontweight='bold')\n",
    "\n",
    "ax.set_xticks(x + width / 2)\n",
    "ax.set_xticklabels(metrics_names, fontsize=10, rotation=15)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Per-Class Metrics — BiLSTM vs BERT', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/per_class_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Figure saved → results/per_class_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comp-params-006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────\n",
    "#  Model Complexity & Efficiency Summary\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "\n",
    "def count_params(model):\n",
    "    total     = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "t_bilstm, tr_bilstm = count_params(final_model)\n",
    "t_bert,   tr_bert   = count_params(final_bert)\n",
    "\n",
    "complexity_df = pd.DataFrame({\n",
    "    'Model':            ['BiLSTM', 'BERT-base-uncased'],\n",
    "    'Total Params':     [f'{t_bilstm:,}', f'{t_bert:,}'],\n",
    "    'Trainable Params': [f'{tr_bilstm:,}', f'{tr_bert:,}'],\n",
    "    'Test F1-Macro':    [f'{bilstm_test_f1:.4f}', f'{bert_test_f1_saved:.4f}'],\n",
    "    'Inference (s)':    [f'{bilstm_inf_time:.3f}', f'{bert_inf_time:.3f}'],\n",
    "    'F1 / M params':    [f'{bilstm_test_f1/(t_bilstm/1e6):.4f}',\n",
    "                         f'{bert_test_f1_saved/(t_bert/1e6):.4f}'],\n",
    "})\n",
    "\n",
    "print('╔' + '═' * 64 + '╗')\n",
    "print('║  MODEL COMPLEXITY & EFFICIENCY SUMMARY' + ' ' * 25 + '║')\n",
    "print('╚' + '═' * 64 + '╝')\n",
    "display(complexity_df.style.set_properties(**{'text-align': 'center'}).hide(axis='index'))\n",
    "\n",
    "ratio = t_bert / t_bilstm\n",
    "print(f'\\n→ BERT has {ratio:.1f}× more parameters than BiLSTM')\n",
    "print(f'→ BERT inference is {bert_inf_time/bilstm_inf_time:.1f}× slower than BiLSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comp-analysis-007",
   "metadata": {},
   "source": [
    "## 4.1 Results Discussion\n",
    "\n",
    "### Answering the Research Questions\n",
    "\n",
    "**RQ1 — Performance:**  \n",
    "Neural sequence models achieve improvements over the TF-IDF baseline. The BiLSTM shows a meaningful improvement in CV F1 (0.7282 vs 0.6536), demonstrating that even simple sequential modeling captures useful word-order information that bag-of-words discards. BERT, leveraging pre-trained contextual representations, pushes performance further, confirming that transfer learning is beneficial even on this relatively small dataset.\n",
    "\n",
    "**RQ2 — Error Correction:**  \n",
    "The confusion matrices above reveal which error types are resolved by each architecture. BERT handles better the *anti-sexist reporting* error mode (tweets that discuss sexism without being sexist), since its bidirectional contextualized representations can distinguish *intent* from *topic*. The BiLSTM partially addresses this but remains limited by its learned-from-scratch embeddings on a small corpus.\n",
    "\n",
    "**RQ3 — Data Efficiency:**  \n",
    "The BiLSTM, trained entirely from scratch on ~6,920 samples, shows moderate gains — its best CV F1 (0.7282) is above the baseline but still constrained by limited data. BERT's pre-trained weights effectively mitigate data starvation: despite having ~100× more parameters, fine-tuning with just ~5,500 training samples per fold yields strong generalization, demonstrating the power of transfer learning.\n",
    "\n",
    "**RQ4 — Computational Cost:**  \n",
    "The baseline trains in seconds on CPU. BiLSTM requires ~1 hour for the full Optuna search (30 trials × 5-fold CV on GPU). BERT fine-tuning is the most expensive (~40 min for 5 folds × 4 epochs on GPU), but yields the best results. Inference-time differences are less dramatic — both neural models process the test set in a few seconds.\n",
    "\n",
    "**RQ5 — Architecture Trade-offs:**  \n",
    "BiLSTM offers moderate improvement at reasonable cost, but struggles with the bilingual nature of the corpus (EN + ES) since its vocabulary is trained from scratch. BERT's subword tokenization and pre-training on massive corpora handle cross-lingual patterns more gracefully. For short texts like tweets, BERT's full self-attention mechanism is particularly effective — the entire sequence fits within the attention window, avoiding the information bottleneck of LSTM's fixed-size hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comp-conclusions-008",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Conclusions\n",
    "\n",
    "1. **Progressive improvement:** We observe a clear progression from classical BoW models → recurrent models → pre-trained Transformers, aligning with the broader trajectory of the NLP field.\n",
    "\n",
    "2. **Transfer learning is crucial for small datasets:** With only ~6,920 training samples, models trained from scratch (BiLSTM) show moderate gains over the well-tuned baseline. Pre-trained models (BERT) leverage knowledge from massive corpora, making them significantly more data-efficient.\n",
    "\n",
    "3. **The baseline is surprisingly competitive:** TF-IDF + Logistic Regression achieves 0.75 F1-Macro on the test set — a strong result that underscores the importance of well-engineered baselines before deploying complex architectures.\n",
    "\n",
    "4. **Cost-performance trade-off:** BERT delivers the best absolute performance but requires GPU acceleration and significantly more training time. For resource-constrained scenarios, the BiLSTM or even the baseline may be preferable.\n",
    "\n",
    "5. **Future directions:**\n",
    "   - Use a **multilingual Transformer** (XLM-RoBERTa) to better handle the bilingual corpus\n",
    "   - Apply **data augmentation** (back-translation, paraphrasing) to increase effective training size\n",
    "   - Conduct an **ablation study** on BERT layer freezing using `unfreeze_last_n_layers()` to find the optimal fine-tuning depth\n",
    "   - Explore **ensemble methods** combining predictions from all three model families\n",
    "   - Investigate **knowledge distillation** from BERT to a smaller student model for deployment efficiency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
