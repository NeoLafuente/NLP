{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356ac96b-5176-478b-9454-97e4d6c3a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Gensim for Dense Embeddings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# NLTK downloads for preprocessing\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# --- MANDATORY METHODOLOGY: REPRODUCIBILITY ---\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b8b3b9-6cfd-4dd7-81f2-4c7e0ce491d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'EXIST2025_training.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Load Data\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading Training Data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m df_train_full = \u001b[43mload_and_parse_data\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEXIST2025_training.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_test\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal Training Samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_train_full)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m display(df_train_full[[\u001b[33m'\u001b[39m\u001b[33mid_EXIST\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtweet\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfinal_label\u001b[39m\u001b[33m'\u001b[39m]].head())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mload_and_parse_data\u001b[39m\u001b[34m(filepath, is_test)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_and_parse_data\u001b[39m(filepath, is_test=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m         data = json.load(f)\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Convert dictionary of dictionaries to DataFrame\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/IXjAnHtYzIQ1ua8oeQpYQ/lib/python3.13/site-packages/IPython/core/interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'EXIST2025_training.json'"
     ]
    }
   ],
   "source": [
    "def load_and_parse_data(filepath, is_test=False):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Convert dictionary of dictionaries to DataFrame\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    \n",
    "    # Reset index to make the ID a column\n",
    "    df = df.reset_index().rename(columns={'index': 'id_EXIST'})\n",
    "    \n",
    "    # Label Processing (Only for Training Data)\n",
    "    if not is_test and 'labels_task1_1' in df.columns:\n",
    "        # Majority Vote Function\n",
    "        def get_majority_vote(labels_list):\n",
    "            # labels_list is like ['YES', 'YES', 'NO', 'NO', 'YES', 'NO']\n",
    "            # We count occurrences\n",
    "            counts = pd.Series(labels_list).value_counts()\n",
    "            # Return the label with the highest count\n",
    "            return counts.idxmax()\n",
    "        \n",
    "        df['final_label'] = df['labels_task1_1'].apply(get_majority_vote)\n",
    "        \n",
    "        # Binary encoding: YES=1, NO=0 (adjust based on your specific classes)\n",
    "        # Check unique classes first\n",
    "        print(f\"Classes found: {df['final_label'].unique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load Data\n",
    "print(\"Loading Training Data...\")\n",
    "df_train_full = load_and_parse_data('EXIST2025_training.json', is_test=False)\n",
    "\n",
    "print(f\"Total Training Samples: {len(df_train_full)}\")\n",
    "display(df_train_full[['id_EXIST', 'tweet', 'final_label']].head())\n",
    "\n",
    "# --- MANDATORY METHODOLOGY: DATA SPLITTING ---\n",
    "# Splitting the provided training file into an internal Train and Test set\n",
    "# to perform the final Error Analysis effectively.\n",
    "X = df_train_full['tweet']\n",
    "y = df_train_full['final_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, # Maintains class balance\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Internal Train Shape: {X_train.shape}\")\n",
    "print(f\"Internal Test Shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5200cead-3eef-43ed-b20d-31e0d960f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) | set(stopwords.words('spanish')) # Assuming mixed EN/ES data based on snippets\n",
    "\n",
    "def clean_text(text, usage='clean'):\n",
    "    # Basic Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    if usage == 'raw':\n",
    "        return text\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from hashtags\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = text.split()\n",
    "    \n",
    "    if usage == 'clean':\n",
    "        # Remove Stopwords and Lemmatize\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing strategies to create separate feature sets for ablation\n",
    "X_train_clean = X_train.apply(lambda x: clean_text(x, usage='clean'))\n",
    "X_test_clean = X_test.apply(lambda x: clean_text(x, usage='clean'))\n",
    "\n",
    "X_train_raw = X_train.apply(lambda x: clean_text(x, usage='raw'))\n",
    "X_test_raw = X_test.apply(lambda x: clean_text(x, usage='raw'))\n",
    "\n",
    "print(\"Example Raw:\", X_train_raw.iloc[0])\n",
    "print(\"Example Clean:\", X_train_clean.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740597f-e7bb-4066-99f0-99ac1703e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.word2vec = None\n",
    "        self.dim = vector_size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Train Word2Vec on the corpus provided (Domain Specific)\n",
    "        sentences = [row.split() for row in X]\n",
    "        self.word2vec = Word2Vec(sentences, \n",
    "                                 vector_size=self.vector_size, \n",
    "                                 window=self.window, \n",
    "                                 min_count=self.min_count, \n",
    "                                 workers=4,\n",
    "                                 seed=RANDOM_STATE)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in [row.split() for row in X]\n",
    "        ])\n",
    "\n",
    "print(\"Dense Vectorizer Class Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcbb4a4-2257-4e43-a6a7-38cf8a1c094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Results Dictionary\n",
    "results = {}\n",
    "\n",
    "# --- SETUP CROSS VALIDATION ---\n",
    "cv_strat = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# --- PIPELINE 1: SPARSE (TF-IDF + Logistic Regression) ---\n",
    "# Covers: N-gram Exploration, TF-IDF\n",
    "pipe_sparse = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Grid Search for Sparse\n",
    "# Covers: Hyperparameter Optimization (C, ngram_range)\n",
    "param_grid_sparse = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)], # Unigrams vs Bigrams\n",
    "    'clf__C': [0.1, 1, 10]                 # Regularization strength\n",
    "}\n",
    "\n",
    "print(\"Running Grid Search on Sparse (TF-IDF)...\")\n",
    "grid_sparse = GridSearchCV(pipe_sparse, param_grid_sparse, cv=cv_strat, scoring='f1_macro', n_jobs=-1)\n",
    "grid_sparse.fit(X_train_clean, y_train)\n",
    "\n",
    "results['Sparse_Best_Score'] = grid_sparse.best_score_\n",
    "results['Sparse_Best_Params'] = grid_sparse.best_params_\n",
    "print(f\"Best Sparse F1-Macro: {grid_sparse.best_score_:.4f}\")\n",
    "print(f\"Best Sparse Params: {grid_sparse.best_params_}\")\n",
    "\n",
    "\n",
    "# --- PIPELINE 2: DENSE (Word2Vec + Logistic Regression) ---\n",
    "pipe_dense = Pipeline([\n",
    "    ('vect', MeanEmbeddingVectorizer(vector_size=100)),\n",
    "    ('clf', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Grid Search for Dense\n",
    "param_grid_dense = {\n",
    "    'clf__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "print(\"\\nRunning Grid Search on Dense (Word2Vec)...\")\n",
    "grid_dense = GridSearchCV(pipe_dense, param_grid_dense, cv=cv_strat, scoring='f1_macro', n_jobs=-1)\n",
    "grid_dense.fit(X_train_clean, y_train)\n",
    "\n",
    "results['Dense_Best_Score'] = grid_dense.best_score_\n",
    "results['Dense_Best_Params'] = grid_dense.best_params_\n",
    "print(f\"Best Dense F1-Macro: {grid_dense.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b39bfc-04c2-4c09-b982-58488a72a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_sparse = grid_sparse.best_estimator_\n",
    "\n",
    "# Evaluate on CLEAN data (Already fitted above, but for clarity)\n",
    "clean_score = grid_sparse.best_score_\n",
    "\n",
    "# Evaluate on RAW data (Retraining best params on raw text)\n",
    "print(\"\\n--- Ablation Study: Raw vs Clean ---\")\n",
    "best_model_sparse.fit(X_train_raw, y_train)\n",
    "# Simple cross val score to compare\n",
    "from sklearn.model_selection import cross_val_score\n",
    "raw_scores = cross_val_score(best_model_sparse, X_train_raw, y_train, cv=cv_strat, scoring='f1_macro')\n",
    "raw_mean_score = raw_scores.mean()\n",
    "\n",
    "print(f\"F1-Macro with CLEANED text: {clean_score:.4f}\")\n",
    "print(f\"F1-Macro with RAW text:     {raw_mean_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d706e22-3213-452e-9b24-c4ccebe453fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best performing model overall (usually Sparse for text)\n",
    "final_model = grid_sparse.best_estimator_\n",
    "# Ensure it is fitted on the full internal train set (Clean version)\n",
    "final_model.fit(X_train_clean, y_train)\n",
    "\n",
    "# Predict on Internal Test Set\n",
    "y_pred = final_model.predict(X_test_clean)\n",
    "\n",
    "# 1. Quantitative Metrics\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 2. Confusion Matrix \n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=final_model.classes_, yticklabels=final_model.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "# 3. Discriminative Features (Top Weighted Words)\n",
    "# Extract feature names and coefficients\n",
    "vectorizer = final_model.named_steps['vect']\n",
    "classifier = final_model.named_steps['clf']\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefs = classifier.coef_[0] # Assuming binary classification\n",
    "\n",
    "# Create dataframe of terms and weights\n",
    "df_features = pd.DataFrame({'term': feature_names, 'weight': coefs})\n",
    "top_positive = df_features.sort_values(by='weight', ascending=False).head(10)\n",
    "top_negative = df_features.sort_values(by='weight', ascending=True).head(10)\n",
    "\n",
    "print(\"\\n--- Top Discriminative Features (Positive Class) ---\")\n",
    "print(top_positive)\n",
    "print(\"\\n--- Top Discriminative Features (Negative Class) ---\")\n",
    "print(top_negative)\n",
    "\n",
    "# 4. Qualitative Failure Analysis\n",
    "print(\"\\n--- Qualitative Failure Analysis (Misclassified Examples) ---\")\n",
    "results_df = pd.DataFrame({\n",
    "    'text': X_test, # Use original text for readability\n",
    "    'true': y_test,\n",
    "    'pred': y_pred\n",
    "})\n",
    "\n",
    "# Filter for errors\n",
    "errors = results_df[results_df['true'] != results_df['pred']]\n",
    "\n",
    "# Display 5 examples\n",
    "for i, row in errors.head(5).iterrows():\n",
    "    print(f\"Text: {row['text']}\")\n",
    "    print(f\"True: {row['true']} | Pred: {row['pred']}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
