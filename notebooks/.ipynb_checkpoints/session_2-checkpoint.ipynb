{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79eca8f0",
   "metadata": {},
   "source": [
    "# Session 2: Word Vectors and Distributional Semantics\n",
    "\n",
    "## From Discrete Symbols to Dense Representations\n",
    "\n",
    "**Machine Learning Master - UPNA**  \n",
    "**Academic Year 2025-2026**\n",
    "\n",
    "**Estimated Duration:** 2-3 hours\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction and Setup](#1-introduction-and-setup)\n",
    "2. [The Problem: Representing Meaning](#2-the-problem-representing-meaning)\n",
    "3. [Classical Approaches: Sparse Representations](#3-classical-approaches-sparse-representations)\n",
    "4. [The Distributional Hypothesis](#4-the-distributional-hypothesis)\n",
    "5. [Word2Vec: Learning Dense Embeddings](#5-word2vec-learning-dense-embeddings)\n",
    "6. [Alternative Approaches: GloVe and FastText](#6-alternative-approaches-glove-and-fasttext)\n",
    "7. [Evaluation Methods](#7-evaluation-methods)\n",
    "8. [Practical Applications](#8-practical-applications)\n",
    "9. [Limitations and Future Directions](#9-limitations-and-future-directions)\n",
    "10. [Final Exercises](#10-final-exercises)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you should be able to:\n",
    "\n",
    "- Explain the limitations of discrete word representations\n",
    "- Understand the distributional hypothesis and its implications\n",
    "- Implement and train Word2Vec models\n",
    "- Compare different embedding approaches (Word2Vec, GloVe, FastText)\n",
    "- Evaluate word embeddings using intrinsic and extrinsic methods\n",
    "- Apply embeddings to downstream NLP tasks\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction and Setup\n",
    "\n",
    "### 1.1 Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c110b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install gensim nltk scikit-learn matplotlib seaborn numpy scipy\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Word embeddings\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# NLP utilities\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Gensim version: {gensim.__version__}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74178020",
   "metadata": {},
   "source": [
    "### 1.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ddbbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vectors_2d(words, vectors, method='PCA', title='Word Embeddings Visualization'):\n",
    "    \"\"\"\n",
    "    Visualize word vectors in 2D space.\n",
    "\n",
    "    Args:\n",
    "        words: List of words\n",
    "        vectors: Numpy array of word vectors\n",
    "        method: Dimensionality reduction method ('PCA' or 'TSNE')\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    if method == 'PCA':\n",
    "        from sklearn.decomposition import PCA\n",
    "        reducer = PCA(n_components=2)\n",
    "    else:\n",
    "        from sklearn.manifold import TSNE\n",
    "        reducer = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "    vectors_2d = reducer.fit_transform(vectors)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.6)\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, xy=(vectors_2d[i, 0], vectors_2d[i, 1]),\n",
    "                    xytext=(5, 2), textcoords='offset points',\n",
    "                    ha='right', fontsize=10)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af408a5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Problem: Representing Meaning\n",
    "\n",
    "### 2.1 Theory: Why Traditional Representations Fail\n",
    "\n",
    "**Question 2.1:** Before we start coding, think about this:\n",
    "\n",
    "- How would you represent the word \"cat\" to a computer?\n",
    "- What information should be captured?\n",
    "- How can we capture similarity between \"cat\" and \"dog\"?\n",
    "\n",
    "_Write your thoughts here:_\n",
    "\n",
    "```\n",
    "[Your answer]\n",
    "```\n",
    "\n",
    "### Exercise 2.1: One-Hot Encoding Analysis\n",
    "\n",
    "**Objective:** Understand the limitations of one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36bb53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_vectors(vocabulary):\n",
    "    \"\"\"\n",
    "    Create one-hot encoded vectors for a vocabulary.\n",
    "\n",
    "    Args:\n",
    "        vocabulary (list): List of words\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary mapping words to one-hot vectors\n",
    "\n",
    "    Example:\n",
    "        >>> vocab = ['cat', 'dog', 'mouse']\n",
    "        >>> vectors = create_one_hot_vectors(vocab)\n",
    "        >>> vectors['cat']\n",
    "        array([1., 0., 0.])\n",
    "    \"\"\"\n",
    "    vocab_size = len(vocabulary)\n",
    "    one_hot_dict = {}\n",
    "\n",
    "    # TODO: Complete this function\n",
    "    # For each word in vocabulary, create a vector of zeros with length vocab_size\n",
    "    # Set the appropriate index to 1\n",
    "    # Hint: Use np.zeros() and enumerate()\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return one_hot_dict\n",
    "\n",
    "# Test the function\n",
    "vocabulary = ['cat', 'dog', 'mouse', 'lion', 'tiger']\n",
    "one_hot_vectors = create_one_hot_vectors(vocabulary)\n",
    "\n",
    "print(\"One-hot vectors:\")\n",
    "for word, vector in one_hot_vectors.items():\n",
    "    print(f\"{word}: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "        vec1, vec2: numpy arrays\n",
    "\n",
    "    Returns:\n",
    "        float: cosine similarity\n",
    "\n",
    "    Formula: cosine_sim = (a ¬∑ b) / (||a|| ||b||)\n",
    "    \"\"\"\n",
    "    # TODO: Complete this function\n",
    "    # Compute cosine similarity using the formula: (a ¬∑ b) / (||a|| ||b||)\n",
    "    # Hint: Use np.dot() and np.linalg.norm()\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    pass\n",
    "\n",
    "# Test similarity computation\n",
    "cat_vec = one_hot_vectors['cat']\n",
    "dog_vec = one_hot_vectors['dog']\n",
    "\n",
    "cat_cat_similarity = compute_cosine_similarity(cat_vec, cat_vec)\n",
    "cat_dog_similarity = compute_cosine_similarity(cat_vec, dog_vec)\n",
    "\n",
    "print(f\"\\nSimilarity(cat, cat): {cat_cat_similarity:.4f}\")\n",
    "print(f\"Similarity(cat, dog): {cat_dog_similarity:.4f}\")\n",
    "print(f\"Similarity(cat, mouse): {compute_cosine_similarity(cat_vec, one_hot_vectors['mouse']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072745f5",
   "metadata": {},
   "source": [
    "**Question 2.2:** What do you notice about the similarity between different words in one-hot encoding? Why is this a problem for capturing semantic relationships?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your observations here]\n",
    "\n",
    "Expected observations:\n",
    "- All different words have similarity of 0 (orthogonal vectors)\n",
    "- This means \"cat\" is as similar to \"dog\" as it is to \"mouse\" or any other word\n",
    "- No semantic information is captured\n",
    "- All words are equally distant from each other in the vector space\n",
    "```\n",
    "\n",
    "### Exercise 2.2: Vocabulary Size Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_vocabulary_growth(text_samples):\n",
    "    \"\"\"\n",
    "    Analyze how vocabulary size grows with corpus size.\n",
    "\n",
    "    Args:\n",
    "        text_samples (list): List of text samples\n",
    "\n",
    "    Returns:\n",
    "        list: Vocabulary sizes at different corpus sizes\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    vocab_sizes = []\n",
    "\n",
    "    for text in text_samples:\n",
    "        # TODO: Tokenize the text (simple split by whitespace)\n",
    "        # Add words to vocabulary set\n",
    "        # Record the vocabulary size\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        pass\n",
    "\n",
    "    return vocab_sizes\n",
    "\n",
    "# Example corpus (you can expand this)\n",
    "sample_texts = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog played in the park\",\n",
    "    \"a cat and a dog are friends\",\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"cats and dogs are common pets\",\n",
    "]\n",
    "\n",
    "vocab_sizes = analyze_vocabulary_growth(sample_texts)\n",
    "\n",
    "# Plot vocabulary growth\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(vocab_sizes) + 1), vocab_sizes, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Sentences')\n",
    "plt.ylabel('Vocabulary Size')\n",
    "plt.title('Vocabulary Growth with Corpus Size')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final vocabulary size: {vocab_sizes[-1]}\")\n",
    "print(f\"One-hot vector dimension: {vocab_sizes[-1]}\")\n",
    "print(f\"Memory for 1000 words (float32): {vocab_sizes[-1] * 1000 * 4 / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a6b52",
   "metadata": {},
   "source": [
    "**Question 2.3:** What happens to the dimensionality as the vocabulary grows? What are the implications for:\n",
    "\n",
    "- Memory usage\n",
    "- Computational efficiency\n",
    "- Sparsity of data\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Classical Approaches: Sparse Representations\n",
    "\n",
    "### 3.1 Theory: Bag of Words (BoW)\n",
    "\n",
    "Before Word2Vec, researchers used count-based methods. The simplest is **Bag of Words**: represent a document as the sum of its one-hot word vectors (i.e., word counts).\n",
    "\n",
    "**Properties:**\n",
    "\n",
    "- Disregards grammar and word order\n",
    "- Keeps multiplicity (counts)\n",
    "- Still sparse and high-dimensional\n",
    "\n",
    "### Exercise 3.1: Implementing Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_representation(documents, vocabulary=None):\n",
    "    \"\"\"\n",
    "    Create Bag of Words representation for documents.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of documents (strings)\n",
    "        vocabulary (list): Optional vocabulary to use\n",
    "\n",
    "    Returns:\n",
    "        np.array: BoW matrix (documents √ó vocabulary)\n",
    "        list: vocabulary\n",
    "\n",
    "    Example:\n",
    "        >>> docs = [\"I love NLP\", \"I love deep learning\"]\n",
    "        >>> bow_matrix, vocab = create_bow_representation(docs)\n",
    "        >>> bow_matrix.shape\n",
    "        (2, 5)  # 2 documents, 5 unique words\n",
    "    \"\"\"\n",
    "    # Build vocabulary if not provided\n",
    "    if vocabulary is None:\n",
    "        vocabulary = set()\n",
    "        for doc in documents:\n",
    "            words = doc.lower().split()\n",
    "            vocabulary.update(words)\n",
    "        vocabulary = sorted(list(vocabulary))\n",
    "\n",
    "    # Create word to index mapping\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "    # TODO: Complete this function\n",
    "    # Create a matrix of size (num_documents, vocab_size)\n",
    "    # For each document, count the occurrences of each word\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return bow_matrix, vocabulary\n",
    "\n",
    "# Test the function\n",
    "documents = [\n",
    "    \"I love natural language processing\",\n",
    "    \"I love deep learning\",\n",
    "    \"Natural language processing is amazing\"\n",
    "]\n",
    "\n",
    "bow_matrix, vocab = create_bow_representation(documents)\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"\\nBag of Words matrix:\")\n",
    "print(bow_matrix)\n",
    "print(f\"\\nMatrix shape: {bow_matrix.shape}\")\n",
    "print(f\"Sparsity: {np.sum(bow_matrix == 0) / bow_matrix.size * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf68a2",
   "metadata": {},
   "source": [
    "### 3.2 Theory: TF-IDF\n",
    "\n",
    "**TF-IDF** (Term Frequency - Inverse Document Frequency) weights terms by their importance, penalizing common words.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "- **Term Frequency (TF):** $\\text{tf}(t,d) = \\frac{\\text{count}(t,d)}{\\text{total\\_words}(d)}$\n",
    "- **Inverse Document Frequency (IDF):** $\\text{idf}(t, D) = \\log \\frac{N}{|\\{d \\in D : t \\in d\\}|}$\n",
    "- **TF-IDF:** $\\text{tfidf}(t,d,D) = \\text{tf}(t,d) \\times \\text{idf}(t, D)$\n",
    "\n",
    "### Exercise 3.2: Implementing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f529fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(documents):\n",
    "    \"\"\"\n",
    "    Compute TF-IDF representation for documents.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of documents (strings)\n",
    "\n",
    "    Returns:\n",
    "        np.array: TF-IDF matrix\n",
    "        list: vocabulary\n",
    "    \"\"\"\n",
    "    # First, get the BoW representation\n",
    "    bow_matrix, vocabulary = create_bow_representation(documents)\n",
    "\n",
    "    # TODO: Complete this function\n",
    "    # 1. Compute Term Frequency (TF): normalize BoW by document length\n",
    "    # 2. Compute Document Frequency (DF): number of documents containing each term\n",
    "    # 3. Compute Inverse Document Frequency (IDF): idf(t) = log(N / df(t))\n",
    "    # 4. Compute TF-IDF: tf_idf(t,d) = tf(t,d) * idf(t)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return tf_idf_matrix, vocabulary\n",
    "\n",
    "# Test the function\n",
    "test_documents = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"cats and dogs are enemies\",\n",
    "    \"the cat and the dog are friends\"\n",
    "]\n",
    "\n",
    "tf_idf_matrix, vocab = compute_tf_idf(test_documents)\n",
    "\n",
    "print(\"TF-IDF matrix:\")\n",
    "print(tf_idf_matrix)\n",
    "print(\"\\nTop weighted terms per document:\")\n",
    "for doc_idx in range(len(test_documents)):\n",
    "    top_indices = np.argsort(tf_idf_matrix[doc_idx])[-3:][::-1]\n",
    "    top_terms = [(vocab[idx], tf_idf_matrix[doc_idx, idx]) for idx in top_indices]\n",
    "    print(f\"Document {doc_idx + 1}: {top_terms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609fca71",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Comparing BoW and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3011bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bow_tfidf(documents, query_word):\n",
    "    \"\"\"\n",
    "    Compare BoW and TF-IDF representations for a specific word.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of documents\n",
    "        query_word (str): Word to analyze\n",
    "    \"\"\"\n",
    "    bow_matrix, vocab = create_bow_representation(documents)\n",
    "    tf_idf_matrix, _ = compute_tf_idf(documents)\n",
    "\n",
    "    # TODO: Complete this function\n",
    "    # Find the index of query_word in vocabulary\n",
    "    # Print its BoW and TF-IDF weights across documents\n",
    "    # Create a visualization comparing the two\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    pass\n",
    "\n",
    "# Test with different words\n",
    "compare_bow_tfidf(test_documents, \"the\")\n",
    "compare_bow_tfidf(test_documents, \"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184444bf",
   "metadata": {},
   "source": [
    "**Question 3.1:** Why does TF-IDF assign lower weights to common words like \"the\"? How does this help with information retrieval?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. The Distributional Hypothesis\n",
    "\n",
    "### 4.1 Theory: \"You Shall Know a Word by the Company It Keeps\"\n",
    "\n",
    "**J.R. Firth (1957):** Words that occur in similar contexts have similar meanings.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "$$\\text{If } C(w_i) \\approx C(w_j) \\quad \\Rightarrow \\quad \\text{semantics}(w_i) \\approx \\text{semantics}(w_j)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $C(w)$: Context of word $w$ (distribution of surrounding words)\n",
    "- $\\approx$: Similarity measure\n",
    "\n",
    "This is **one of the most successful ideas of modern statistical NLP!**\n",
    "\n",
    "### Exercise 4.1: Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b5927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(corpus, window_size=2):\n",
    "    \"\"\"\n",
    "    Build a word co-occurrence matrix from corpus.\n",
    "\n",
    "    Args:\n",
    "        corpus (list): List of sentences (strings)\n",
    "        window_size (int): Context window size\n",
    "\n",
    "    Returns:\n",
    "        np.array: Co-occurrence matrix\n",
    "        list: vocabulary\n",
    "\n",
    "    Example:\n",
    "        Given: \"the cat sat\" with window_size=1\n",
    "        For center word \"cat\":\n",
    "        - Context words: [\"the\", \"sat\"]\n",
    "        - Increment co-occurrence counts for (cat, the) and (cat, sat)\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    vocabulary = set()\n",
    "    for sentence in corpus:\n",
    "        words = sentence.lower().split()\n",
    "        vocabulary.update(words)\n",
    "    vocabulary = sorted(list(vocabulary))\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "    # TODO: Complete this function\n",
    "    # Initialize co-occurrence matrix with zeros\n",
    "    # For each sentence and each word position:\n",
    "    #   - Get context words within window_size\n",
    "    #   - Increment co-occurrence counts symmetrically\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return cooc_matrix, vocabulary\n",
    "\n",
    "# Test the function\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog played in the park\",\n",
    "    \"a cat and a dog are friends\"\n",
    "]\n",
    "\n",
    "cooc_matrix, vocab = build_cooccurrence_matrix(corpus, window_size=2)\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"\\nCo-occurrence matrix:\")\n",
    "print(cooc_matrix)\n",
    "\n",
    "# Visualize the matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cooc_matrix, xticklabels=vocab, yticklabels=vocab,\n",
    "            annot=True, fmt='g', cmap='YlOrRd')\n",
    "plt.title('Word Co-occurrence Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f260bd",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Analyzing Context Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe49c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_word_contexts(word, cooc_matrix, vocabulary, top_k=5):\n",
    "    \"\"\"\n",
    "    Analyze and display the top k context words for a given word.\n",
    "\n",
    "    Args:\n",
    "        word (str): Target word\n",
    "        cooc_matrix (np.array): Co-occurrence matrix\n",
    "        vocabulary (list): Vocabulary\n",
    "        top_k (int): Number of top context words to show\n",
    "    \"\"\"\n",
    "    # TODO: Complete this function\n",
    "    # Find the row for the target word\n",
    "    # Sort by co-occurrence count\n",
    "    # Display top k context words\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    pass\n",
    "\n",
    "# Analyze contexts for different words\n",
    "analyze_word_contexts(\"cat\", cooc_matrix, vocab, top_k=5)\n",
    "analyze_word_contexts(\"dog\", cooc_matrix, vocab, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e302b",
   "metadata": {},
   "source": [
    "**Question 4.1:** Looking at the contexts of \"cat\" and \"dog\", do they share similar context words? What does this tell us about their semantic relationship?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "### Exercise 4.3: Computing Similarity from Co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf926ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_context_similarity(word1, word2, cooc_matrix, vocabulary):\n",
    "    \"\"\"\n",
    "    Compute similarity between two words based on their context vectors.\n",
    "\n",
    "    Args:\n",
    "        word1, word2 (str): Words to compare\n",
    "        cooc_matrix (np.array): Co-occurrence matrix\n",
    "        vocabulary (list): Vocabulary\n",
    "\n",
    "    Returns:\n",
    "        float: Cosine similarity between context vectors\n",
    "    \"\"\"\n",
    "    # TODO: Complete this function\n",
    "    # Get context vectors for both words (rows from co-occurrence matrix)\n",
    "    # Compute cosine similarity between them\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    pass\n",
    "\n",
    "# Test similarity computation\n",
    "print(\"Context-based similarities:\")\n",
    "print(f\"Similarity(cat, dog): {compute_context_similarity('cat', 'dog', cooc_matrix, vocab):.4f}\")\n",
    "print(f\"Similarity(cat, mat): {compute_context_similarity('cat', 'mat', cooc_matrix, vocab):.4f}\")\n",
    "print(f\"Similarity(dog, park): {compute_context_similarity('dog', 'park', cooc_matrix, vocab):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd00db",
   "metadata": {},
   "source": [
    "**Question 4.2:** Compare these context-based similarities with the one-hot similarities from Exercise 2.1. What's the key difference?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Word2Vec: Learning Dense Embeddings\n",
    "\n",
    "### 5.1 Theory: The Revolution\n",
    "\n",
    "**Word2Vec (Mikolov et al. 2013)** learns embeddings by predicting context words from center words (or vice versa).\n",
    "\n",
    "**Key Idea:**\n",
    "\n",
    "1. Have a large corpus of text\n",
    "2. Every word represented by a vector\n",
    "3. Go through each position $t$ in text\n",
    "4. Use similarity of vectors to calculate probability of context given center\n",
    "5. Adjust vectors to maximize this probability\n",
    "\n",
    "**Two Variants:**\n",
    "\n",
    "- **Skip-gram:** Predict context words given center word\n",
    "- **CBOW:** Predict center word given context words\n",
    "\n",
    "**Objective Function (Skip-gram):**\n",
    "$$J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} \\log P(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "**Prediction Function:**\n",
    "$$P(o | c) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w \\in V} \\exp(u_w^T v_c)}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $v_c$: center word vector\n",
    "- $u_o$: context word vector\n",
    "\n",
    "### Exercise 5.1: Training Your First Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training corpus\n",
    "training_corpus = [\n",
    "    \"natural language processing is amazing\",\n",
    "    \"deep learning revolutionized nlp\",\n",
    "    \"word embeddings capture semantic meaning\",\n",
    "    \"neural networks learn representations\",\n",
    "    \"transformers are powerful models\",\n",
    "    \"bert and gpt are popular\",\n",
    "    \"language models generate text\",\n",
    "    \"semantic similarity measures relatedness\",\n",
    "    \"vector space models work well\",\n",
    "    \"word2vec learns from context\"\n",
    "]\n",
    "\n",
    "# TODO: Expand this corpus with more sentences\n",
    "# Add at least 20 more sentences on topics like:\n",
    "# - Machine learning\n",
    "# - Artificial intelligence\n",
    "# - Natural language processing\n",
    "# - Deep learning\n",
    "\n",
    "# YOUR SENTENCES HERE\n",
    "expanded_corpus = training_corpus + [\n",
    "    # Add your sentences here\n",
    "]\n",
    "\n",
    "# Tokenize corpus\n",
    "def tokenize_corpus(corpus):\n",
    "    \"\"\"Simple tokenization: lowercase and split.\"\"\"\n",
    "    return [sentence.lower().split() for sentence in corpus]\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(expanded_corpus)\n",
    "\n",
    "print(f\"Corpus size: {len(tokenized_corpus)} sentences\")\n",
    "print(f\"First 3 tokenized sentences:\")\n",
    "for sent in tokenized_corpus[:3]:\n",
    "    print(f\"  {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91754d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# TODO: Complete the Word2Vec training\n",
    "# Set appropriate hyperparameters:\n",
    "# - vector_size: embedding dimension (try 100)\n",
    "# - window: context window size (try 5)\n",
    "# - min_count: minimum word frequency (try 1 for small corpus)\n",
    "# - workers: number of threads (try 4)\n",
    "# - sg: 0 for CBOW, 1 for Skip-gram (try 1)\n",
    "\n",
    "model_w2v = Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    # YOUR HYPERPARAMETERS HERE\n",
    ")\n",
    "\n",
    "print(f\"\\nModel trained!\")\n",
    "print(f\"Vocabulary size: {len(model_w2v.wv)}\")\n",
    "print(f\"Vector size: {model_w2v.wv.vector_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d912f82",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Exploring Learned Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this function\n",
    "def explore_word_embedding(word, model, top_n=5):\n",
    "    \"\"\"\n",
    "    Explore the embedding learned for a specific word.\n",
    "\n",
    "    Args:\n",
    "        word (str): Word to explore\n",
    "        model: Trained Word2Vec model\n",
    "        top_n (int): Number of similar words to show\n",
    "    \"\"\"\n",
    "    if word not in model.wv:\n",
    "        print(f\"Word '{word}' not in vocabulary\")\n",
    "        return\n",
    "\n",
    "    # Get vector\n",
    "    vector = model.wv[word]\n",
    "    print(f\"\\nWord: '{word}'\")\n",
    "    print(f\"Vector shape: {vector.shape}\")\n",
    "    print(f\"Vector (first 10 dimensions): {vector[:10]}\")\n",
    "\n",
    "    # Find most similar words\n",
    "    # YOUR CODE HERE - use model.wv.most_similar()\n",
    "\n",
    "    pass\n",
    "\n",
    "# Test with different words\n",
    "explore_word_embedding(\"language\", model_w2v)\n",
    "explore_word_embedding(\"learning\", model_w2v)\n",
    "explore_word_embedding(\"model\", model_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60f17af",
   "metadata": {},
   "source": [
    "### Exercise 5.3: Vector Arithmetic and Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b46879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_analogy(model, a, b, c, top_n=1):\n",
    "    \"\"\"\n",
    "    Solve word analogy: a is to b as c is to ?\n",
    "\n",
    "    Example: king is to queen as man is to woman\n",
    "    Formula: result = vector(b) - vector(a) + vector(c)\n",
    "\n",
    "    Args:\n",
    "        model: Trained Word2Vec model\n",
    "        a, b, c (str): Analogy words\n",
    "        top_n (int): Number of results to return\n",
    "\n",
    "    Returns:\n",
    "        list: Most similar words to the result\n",
    "    \"\"\"\n",
    "    # TODO: Complete this function\n",
    "    # Use model.wv.most_similar() with positive and negative arguments\n",
    "    # positive=[b, c], negative=[a]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    pass\n",
    "\n",
    "# Test with analogies (these might not work well with small corpus)\n",
    "print(\"Testing analogies:\")\n",
    "print(\"deep : learning :: natural : ?\")\n",
    "result = solve_analogy(model_w2v, \"deep\", \"learning\", \"natural\")\n",
    "print(f\"Result: {result}\\n\")\n",
    "\n",
    "# TODO: Try creating your own analogies\n",
    "# Examples:\n",
    "# - \"neural : network :: word : ?\"\n",
    "# - \"bert : transformer :: word2vec : ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a6446",
   "metadata": {},
   "source": [
    "**Question 5.1:** Why might analogies not work well with our small training corpus? What would we need to improve them?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "### Exercise 5.4: Visualizing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad770a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(model, words=None, n_words=20):\n",
    "    \"\"\"\n",
    "    Visualize word embeddings in 2D space using PCA.\n",
    "\n",
    "    Args:\n",
    "        model: Trained Word2Vec model\n",
    "        words (list): Specific words to visualize (optional)\n",
    "        n_words (int): Number of words to visualize if words=None\n",
    "    \"\"\"\n",
    "    if words is None:\n",
    "        # Select most frequent words\n",
    "        words = [word for word, _ in model.wv.index_to_key[:n_words]]\n",
    "\n",
    "    # TODO: Complete this function\n",
    "    # 1. Get vectors for selected words\n",
    "    # 2. Apply PCA to reduce to 2 dimensions\n",
    "    # 3. Create a scatter plot with word labels\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    pass\n",
    "\n",
    "# Visualize embeddings\n",
    "visualize_embeddings(model_w2v, n_words=20)\n",
    "\n",
    "# Visualize specific semantic groups\n",
    "semantic_groups = [\n",
    "    \"language\", \"natural\", \"processing\",\n",
    "    \"learning\", \"deep\", \"neural\",\n",
    "    \"model\", \"network\", \"transformer\"\n",
    "]\n",
    "visualize_embeddings(model_w2v, words=semantic_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db19c9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Alternative Approaches: GloVe and FastText\n",
    "\n",
    "### 6.1 Theory: GloVe (Global Vectors)\n",
    "\n",
    "**GloVe (Pennington et al. 2014)** combines:\n",
    "\n",
    "- **Count-based methods:** Uses global co-occurrence statistics\n",
    "- **Predictive methods:** Learns embeddings via optimization\n",
    "\n",
    "**Key Insight:** Ratios of co-occurrence probabilities reveal semantic relationships.\n",
    "\n",
    "**Objective Function:**\n",
    "$$J = \\sum_{i,j=1}^{|V|} f(X_{ij}) \\left( w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j - \\log(X_{ij}) \\right)^2$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $X_{ij}$: Co-occurrence count of words $i$ and $j$\n",
    "- $f(X_{ij})$: Weighting function (prevents common words from dominating)\n",
    "\n",
    "### 6.2 Theory: FastText\n",
    "\n",
    "**FastText (Bojanowski et al. 2017)** extends Word2Vec with **subword information**.\n",
    "\n",
    "**Key Innovation:** Represent words as sums of character n-grams.\n",
    "\n",
    "**Example:** word \"where\" with n=3\n",
    "\n",
    "- N-grams: `<wh`, `whe`, `her`, `ere`, `re>`, `<where>`\n",
    "- Vector: $v_{where} = \\sum_{g \\in \\text{n-grams}} v_g$\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Handles **out-of-vocabulary (OOV)** words\n",
    "- Captures morphological information\n",
    "- Better for morphologically rich languages\n",
    "\n",
    "### Exercise 6.1: Training FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f80e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# TODO: Train a FastText model on the same corpus\n",
    "# Use similar hyperparameters as Word2Vec\n",
    "# Additional FastText parameters:\n",
    "# - min_n: minimum length of character n-grams (try 3)\n",
    "# - max_n: maximum length of character n-grams (try 6)\n",
    "\n",
    "model_ft = FastText(\n",
    "    sentences=tokenized_corpus,\n",
    "    # YOUR HYPERPARAMETERS HERE\n",
    ")\n",
    "\n",
    "print(f\"\\nFastText model trained!\")\n",
    "print(f\"Vocabulary size: {len(model_ft.wv)}\")\n",
    "print(f\"Vector size: {model_ft.wv.vector_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09b649",
   "metadata": {},
   "source": [
    "### Exercise 6.2: Testing OOV Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dee620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oov_handling(word, model_w2v, model_ft):\n",
    "    \"\"\"\n",
    "    Test how Word2Vec and FastText handle out-of-vocabulary words.\n",
    "\n",
    "    Args:\n",
    "        word (str): Word to test (should not be in training vocabulary)\n",
    "        model_w2v: Trained Word2Vec model\n",
    "        model_ft: Trained FastText model\n",
    "    \"\"\"\n",
    "    print(f\"\\nTesting OOV word: '{word}'\")\n",
    "\n",
    "    # Word2Vec\n",
    "    # TODO: Check if word is in Word2Vec vocabulary\n",
    "    # If not, print that it's OOV\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # FastText\n",
    "    # TODO: Try to get vector from FastText (should work via n-grams)\n",
    "    # Print the vector shape and find similar words\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    pass\n",
    "\n",
    "# Test with OOV words (words not in training corpus)\n",
    "test_words = [\n",
    "    \"running\",  # If \"run\" was in training\n",
    "    \"computation\",  # If \"compute\" was in training\n",
    "    \"networking\",  # If \"network\" was in training\n",
    "]\n",
    "\n",
    "for word in test_words:\n",
    "    test_oov_handling(word, model_w2v, model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273cb30c",
   "metadata": {},
   "source": [
    "**Question 6.1:** How does FastText generate vectors for OOV words? What are the advantages and disadvantages of this approach?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Evaluation Methods: Meaningful Assessment with Pretrained Embeddings\n",
    "\n",
    "> **Critical Insight**: Our custom-trained Word2Vec model (from Section 5) was trained on only ~30 sentences‚Äîfar too small to learn meaningful semantic relationships. Real embeddings require **massive corpora** (millions to billions of tokens) to capture the geometric regularities that make embeddings useful. In this section, we'll evaluate **pretrained embeddings** trained on large-scale data to see how embeddings _actually work in practice_.\n",
    "\n",
    "### 7.1 Why Small-Corpus Models Fail Evaluation\n",
    "\n",
    "Before we proceed, let's understand why our custom model gives poor results:\n",
    "\n",
    "| Evaluation Metric    | Custom Model (30 sentences) | Production Model (100B+ tokens) | Why the Difference?                              |\n",
    "| -------------------- | --------------------------- | ------------------------------- | ------------------------------------------------ |\n",
    "| Vocabulary coverage  | ~50 words                   | 3M+ words                       | Missing rare words breaks analogy tasks          |\n",
    "| Context diversity    | Limited patterns            | Billions of diverse contexts    | Insufficient statistics for robust relationships |\n",
    "| Vector stability     | High variance               | Converged representations       | Undertrained vectors lack geometric regularity   |\n",
    "| Semantic granularity | Crude clusters              | Fine-grained distinctions       | Dimensionality requires massive data to populate |\n",
    "\n",
    "> üí° **Key Takeaway**: _Never evaluate embedding quality on toy corpora._ The distributional hypothesis requires observing words across _thousands_ of diverse contexts to extract reliable semantic signals. Our small corpus illustrates the _mechanism_ of embedding learning‚Äînot its _capability_.\n",
    "\n",
    "#### Exercise 7.1: Loading Production-Quality Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570329e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TODO: Load Google News Word2Vec embeddings (300 dimensions, 3M words)\n",
    "# This model was trained on ~100 billion tokens from Google News articles\n",
    "# Hint: Use api.load('word2vec-google-news-300')\n",
    "print(\"üì• Downloading pretrained Word2Vec (Google News)...\")\n",
    "# YOUR CODE HERE\n",
    "_________________________\n",
    "\n",
    "print(f\"‚úì Successfully loaded {len(w2v_google):,} words with {w2v_google.vector_size} dimensions\\n\")\n",
    "\n",
    "# TODO: Perform sanity checks to verify model quality\n",
    "# 1. Check the analogy: 'king' - 'man' + 'woman' ‚âà ?\n",
    "# 2. Compute similarity between 'computer' and 'keyboard'\n",
    "# 3. Compute similarity between 'computer' and 'car'\n",
    "# Hint: Use w2v_google.most_similar() and w2v_google.similarity()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "print(\"üîç Sanity checks:\")\n",
    "result = _________________________\n",
    "print(f\"   ‚Ä¢ 'king' - 'man' + 'woman' ‚âà {result[0][0]}\")\n",
    "print(f\"   ‚Ä¢ Similarity('computer', 'keyboard') = {_________________________:.4f}\")\n",
    "print(f\"   ‚Ä¢ Similarity('computer', 'car') = {_________________________:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd78b5d",
   "metadata": {},
   "source": [
    "**Question 7.1**: Compare these results with what you observed from your custom-trained model. What differences do you notice? Why do you think pretrained embeddings perform better?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your observations here]\n",
    "```\n",
    "\n",
    "### 7.2 Intrinsic Evaluation I: Word Similarity Benchmarks\n",
    "\n",
    "#### Theory: Measuring Alignment with Human Judgment\n",
    "\n",
    "Word similarity tasks evaluate whether cosine similarity between word vectors correlates with human-rated semantic similarity. Standard benchmarks include WordSim-353 and SimLex-999.\n",
    "\n",
    "#### Exercise 7.2: Implementing Word Similarity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb39de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curated similarity benchmark reflecting human judgments\n",
    "similarity_benchmark = [\n",
    "    # High similarity pairs\n",
    "    (\"cat\", \"dog\", 7.42),\n",
    "    (\"car\", \"automobile\", 9.44),\n",
    "    (\"king\", \"queen\", 8.58),\n",
    "    # Moderate similarity (functional relationships)\n",
    "    (\"book\", \"paper\", 6.48),\n",
    "    (\"computer\", \"keyboard\", 7.62),\n",
    "    # Low similarity (dissimilar concepts)\n",
    "    (\"cat\", \"car\", 1.82),\n",
    "    # Antonyms (should have low similarity)\n",
    "    (\"happy\", \"sad\", 2.62),\n",
    "]\n",
    "\n",
    "def evaluate_similarity(model, benchmark):\n",
    "    \"\"\"\n",
    "    Evaluate embeddings against human similarity judgments using Spearman correlation.\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained embedding model\n",
    "        benchmark: List of (word1, word2, human_score) tuples\n",
    "\n",
    "    Returns:\n",
    "        float: Spearman correlation coefficient\n",
    "        list: Results for valid word pairs\n",
    "        list: Missing word pairs\n",
    "    \"\"\"\n",
    "    model_sims, human_sims = [], []\n",
    "    missing_words = []\n",
    "\n",
    "    # TODO: Complete this function\n",
    "    # For each word pair in benchmark:\n",
    "    #   1. Check if both words exist in model vocabulary\n",
    "    #   2. If yes: compute cosine similarity and append to model_sims/human_sims\n",
    "    #   3. If no: append to missing_words list\n",
    "    # Hint: Use model.similarity(word1, word2)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for w1, w2, human_score in benchmark:\n",
    "        if _________________________:\n",
    "            model_sim = _________________________\n",
    "            model_sims.append(model_sim)\n",
    "            human_sims.append(human_score)\n",
    "        else:\n",
    "            _________________________\n",
    "\n",
    "    # Compute Spearman correlation if we have enough data\n",
    "    if len(model_sims) < 2:\n",
    "        return 0.0, [], missing_words\n",
    "\n",
    "    corr, p_val = spearmanr(model_sims, human_sims)\n",
    "    results = list(zip(\n",
    "        [f\"{w1}-{w2}\" for w1, w2, _ in benchmark if w1 in model and w2 in model],\n",
    "        model_sims,\n",
    "        human_sims\n",
    "    ))\n",
    "\n",
    "    return corr, results, missing_words\n",
    "\n",
    "# Evaluate pretrained embeddings\n",
    "correlation, results, missing = evaluate_similarity(w2v_google, similarity_benchmark)\n",
    "\n",
    "print(f\"üìä Word Similarity Evaluation (Spearman œÅ = {correlation:.4f})\")\n",
    "print(\"=\"*75)\n",
    "print(f\"{'Word Pair':<22} {'Model Similarity':<20} {'Human Rating':<15}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "# TODO: Print results table with alignment indicator\n",
    "# For each result, compute absolute difference between scaled model similarity and human rating\n",
    "# Add visual indicator: \"‚úì‚úì\" if diff < 1.5, \"‚úì\" if diff < 3.0, \"‚úó\" otherwise\n",
    "\n",
    "# YOUR CODE HERE\n",
    "for pair, model_sim, human_sim in results:\n",
    "    diff = _________________________\n",
    "    indicator = _________________________\n",
    "    print(f\"{pair:<22} {model_sim:<20.4f} {human_sim:<15.2f} {indicator}\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing words in vocabulary: {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48cfb5",
   "metadata": {},
   "source": [
    "**Question 7.2**: What does a Spearman correlation of 0.8+ indicate about the quality of embeddings? Why don't we expect perfect correlation (1.0) with human judgments?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "### 7.3 Intrinsic Evaluation II: Semantic Analogies\n",
    "\n",
    "#### Theory: Testing Relational Reasoning\n",
    "\n",
    "Analogies evaluate whether vector offsets consistently represent semantic relationships:\n",
    "\n",
    "```\n",
    "king : queen :: man : woman   ‚Üí   king - man + woman ‚âà queen\n",
    "France : Paris :: Japan : Tokyo ‚Üí France - Paris + Tokyo ‚âà Japan\n",
    "```\n",
    "\n",
    "#### Exercise 7.3: Implementing Analogy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representative analogy tasks\n",
    "analogy_tasks = {\n",
    "    \"üåç Geography\": [\n",
    "        (\"france\", \"paris\", \"germany\", \"berlin\"),\n",
    "        (\"japan\", \"tokyo\", \"china\", \"beijing\"),\n",
    "    ],\n",
    "    \"üë• Family Relationships\": [\n",
    "        (\"brother\", \"sister\", \"uncle\", \"aunt\"),\n",
    "        (\"son\", \"daughter\", \"nephew\", \"niece\"),\n",
    "    ],\n",
    "    \"‚öñÔ∏è Gender\": [\n",
    "        (\"man\", \"woman\", \"king\", \"queen\"),\n",
    "        (\"actor\", \"actress\", \"prince\", \"princess\"),\n",
    "    ],\n",
    "}\n",
    "\n",
    "def evaluate_analogies(model, tasks, topn=4):\n",
    "    \"\"\"\n",
    "    Evaluate analogy solving accuracy.\n",
    "\n",
    "    Strategy: For a:b :: c:d, compute d' = b - a + c\n",
    "    Count as correct if true answer d appears in top-n predictions\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained embedding model\n",
    "        tasks: Dictionary of category ‚Üí list of (a,b,c,d) tuples\n",
    "        topn: Number of predictions to consider for correctness\n",
    "\n",
    "    Returns:\n",
    "        dict: Category-wise results (correct, total)\n",
    "        float: Overall accuracy\n",
    "    \"\"\"\n",
    "    category_results = {}\n",
    "    total_correct, total_count = 0, 0\n",
    "\n",
    "    # TODO: Complete this function\n",
    "    # For each category and analogy task:\n",
    "    #   1. Skip if any word missing from vocabulary\n",
    "    #   2. Solve analogy using model.most_similar(positive=[b,c], negative=[a])\n",
    "    #   3. Check if expected answer appears in top-n predictions\n",
    "    #   4. Track correct/total counts per category and overall\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for category, analogies in tasks.items():\n",
    "        correct = 0\n",
    "        valid = 0\n",
    "\n",
    "        for a, b, c, d in analogies:\n",
    "            if _________________________:\n",
    "                continue\n",
    "\n",
    "            valid += 1\n",
    "            try:\n",
    "                predicted = _________________________\n",
    "                predicted_words = _________________________\n",
    "\n",
    "                if _________________________:\n",
    "                    correct += 1\n",
    "                    total_correct += 1\n",
    "                total_count += 1\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        if valid > 0:\n",
    "            category_results[category] = (correct, valid)\n",
    "\n",
    "    overall_acc = total_correct / total_count if total_count > 0 else 0\n",
    "    return category_results, overall_acc\n",
    "\n",
    "# Evaluate pretrained model\n",
    "results, accuracy = evaluate_analogies(w2v_google, analogy_tasks)\n",
    "\n",
    "print(f\"üß† Analogy Reasoning Evaluation\")\n",
    "print(\"=\"*75)\n",
    "print(f\"{'Category':<25} {'Correct':<12} {'Total':<10} {'Accuracy':<12}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "# TODO: Print results with visual progress bars\n",
    "# For each category, print accuracy with bar visualization (e.g., \"‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\" for 50%)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "for category, (correct_cat, total_cat) in results.items():\n",
    "    acc = _________________________\n",
    "    bar = _________________________\n",
    "    print(f\"{category:<25} {correct_cat:<12} {total_cat:<10} {acc:>7.1%} {bar}\")\n",
    "\n",
    "print(\"-\"*75)\n",
    "print(f\"{'OVERALL':<25} {total_correct:<12} {total_count:<10} {accuracy:>7.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f409825",
   "metadata": {},
   "source": [
    "**Question 7.3**: Why might syntactic analogies (e.g., verb tenses, plurals) be harder for embeddings to capture than semantic analogies (e.g., country-capital pairs)? What does this tell us about the nature of distributional learning?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "### 7.4 Extrinsic Evaluation: Downstream Task Performance\n",
    "\n",
    "#### Theory: The Ultimate Test‚ÄîDoes It Help Real Applications?\n",
    "\n",
    "Intrinsic metrics don't guarantee utility for actual tasks. **Extrinsic evaluation** measures performance on downstream applications like text classification.\n",
    "\n",
    "#### Exercise 7.4: Text Classification with Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7391a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load 20 Newsgroups dataset: Baseball vs Hockey classification\n",
    "categories = ['rec.sport.baseball', 'rec.sport.hockey']\n",
    "dataset = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                           remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# TODO: Implement document vectorization via mean pooling\n",
    "def doc_to_vec(text, model):\n",
    "    \"\"\"\n",
    "    Convert document to vector by averaging in-vocabulary word embeddings.\n",
    "\n",
    "    Args:\n",
    "        text (str): Document text\n",
    "        model: Pretrained embedding model\n",
    "\n",
    "    Returns:\n",
    "        np.array: Document vector (size = model.vector_size)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    words = _________________________\n",
    "    if not words:\n",
    "        return _________________________\n",
    "    return _________________________\n",
    "\n",
    "# Prepare features and labels\n",
    "X = np.array([doc_to_vec(doc, w2v_google) for doc in dataset.data])\n",
    "y = dataset.target\n",
    "\n",
    "# TODO: Split data and train classifier\n",
    "# 1. Use train_test_split with test_size=0.3, random_state=42, stratify=y\n",
    "# 2. Train LogisticRegression with max_iter=1000\n",
    "# 3. Evaluate on test set and print classification report\n",
    "\n",
    "# YOUR CODE HERE\n",
    "X_train, X_test, y_train, y_test = _________________________\n",
    "\n",
    "clf = _________________________\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = _________________________\n",
    "\n",
    "print(\"‚öæüèí Text Classification: Baseball vs Hockey Articles\")\n",
    "print(\"=\"*75)\n",
    "print(classification_report(y_test, y_pred,\n",
    "                          target_names=['Baseball', 'Hockey']))\n",
    "print(f\"‚úÖ Test Accuracy: {accuracy_score(y_test, y_pred):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c8825b",
   "metadata": {},
   "source": [
    "**Question 7.4**: Why do pretrained embeddings achieve high accuracy (>90%) on this task with minimal training? How would performance differ if we used one-hot vectors or random embeddings instead?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Practical Applications with Production Embeddings\n",
    "\n",
    "### 8.1 Semantic Search Engine (Beyond Keyword Matching)\n",
    "\n",
    "Traditional search relies on exact keyword matches. Semantic search retrieves documents based on _conceptual relevance_.\n",
    "\n",
    "#### Exercise 8.1: Building a Semantic Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a05574",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearchEngine:\n",
    "    \"\"\"Search documents using semantic similarity (not keyword matching).\"\"\"\n",
    "\n",
    "    def __init__(self, model, documents):\n",
    "        \"\"\"\n",
    "        Initialize search engine with embeddings model and document corpus.\n",
    "\n",
    "        Args:\n",
    "            model: Pretrained embedding model\n",
    "            documents: List of document strings\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.documents = documents\n",
    "\n",
    "        # TODO: Create document vectors using doc_to_vec function\n",
    "        # Store in self.doc_vectors as numpy array\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        self.doc_vectors = _________________________\n",
    "\n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Return top-k documents most semantically similar to query.\n",
    "\n",
    "        Args:\n",
    "            query (str): Search query\n",
    "            top_k (int): Number of results to return\n",
    "\n",
    "        Returns:\n",
    "            list: Top k (similarity_score, document) tuples\n",
    "        \"\"\"\n",
    "        # TODO: Complete search implementation\n",
    "        # 1. Convert query to vector using doc_to_vec\n",
    "        # 2. Compute cosine similarities between query and all documents\n",
    "        #    Hint: Use np.dot and normalize by vector norms\n",
    "        # 3. Return top-k results sorted by similarity\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        q_vec = _________________________\n",
    "        if _________________________:\n",
    "            return []\n",
    "\n",
    "        sims = _________________________\n",
    "\n",
    "        top_idx = _________________________\n",
    "        return [(sims[i], self.documents[i]) for i in top_idx]\n",
    "\n",
    "# Technical document corpus\n",
    "docs = [\n",
    "    \"Neural networks learn hierarchical representations from data\",\n",
    "    \"Transformers use self-attention to process sequential data\",\n",
    "    \"Word2Vec learns word embeddings through context prediction\",\n",
    "    \"Convolutional networks excel at spatial feature extraction\",\n",
    "    \"Reinforcement learning optimizes actions through environmental rewards\",\n",
    "    \"Gradient descent minimizes loss functions by following negative gradients\"\n",
    "]\n",
    "\n",
    "# Initialize search engine\n",
    "search = SemanticSearchEngine(w2v_google, docs)\n",
    "\n",
    "# Test semantic queries (note: NO exact keyword matches required)\n",
    "queries = [\n",
    "    \"language understanding models\",   # Should retrieve transformers/Word2Vec docs\n",
    "    \"optimization algorithms\",         # Should retrieve gradient descent doc\n",
    "    \"deep learning architectures\"      # Should retrieve CNN/neural network docs\n",
    "]\n",
    "\n",
    "print(\"üîç Semantic Search Results (No Keyword Matching Required)\")\n",
    "print(\"=\"*80)\n",
    "for q in queries:\n",
    "    print(f\"\\n\tQuery: '{q}'\")\n",
    "    results = search.search(q, top_k=2)\n",
    "\n",
    "    # TODO: Print results with conceptual match indicator\n",
    "    # If query words don't appear in document ‚Üí \"‚úÖ Conceptual match\"\n",
    "    # Otherwise ‚Üí \"‚ö†Ô∏è  Keyword match\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for rank, (score, doc) in enumerate(results, 1):\n",
    "        match_type = _________________________\n",
    "        print(f\"  {rank}. [{score:.3f}] {match_type}\")\n",
    "        print(f\"     ‚Üí {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6520762f",
   "metadata": {},
   "source": [
    "**Question 8.1**: What business applications could benefit from semantic search instead of traditional keyword search? Describe one concrete example.\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "### 8.2 Bias Auditing: Critical for Ethical Deployment\n",
    "\n",
    "Embeddings encode societal biases present in training data. We must audit before deployment.\n",
    "\n",
    "#### Exercise 8.2: Detecting Gender Bias in Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40bdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_gender_bias(model, gender_pairs, target_words):\n",
    "    \"\"\"\n",
    "    Quantify gender bias using Bolukbasi et al. (2016) methodology.\n",
    "\n",
    "    Steps:\n",
    "    1. Compute gender direction from definitional pairs (he/she, man/woman)\n",
    "    2. Project target words onto this direction\n",
    "    3. Positive score = male-associated, Negative = female-associated\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained embedding model\n",
    "        gender_pairs: List of (male_word, female_word) tuples\n",
    "        target_words: Words to measure bias for\n",
    "\n",
    "    Returns:\n",
    "        dict: {word: bias_score} where positive = male-associated\n",
    "    \"\"\"\n",
    "    # TODO: Compute gender direction vector\n",
    "    # 1. For each gender pair, compute difference vector (male - female)\n",
    "    # 2. Average all difference vectors to get gender direction\n",
    "    # 3. Normalize the direction vector\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    gender_vecs = []\n",
    "    for w1, w2 in gender_pairs:\n",
    "        if _________________________:\n",
    "            gender_vecs.append(_________________________)\n",
    "\n",
    "    if not gender_vecs:\n",
    "        raise ValueError(\"Insufficient gender pairs in vocabulary\")\n",
    "\n",
    "    gender_dir = _________________________\n",
    "    gender_dir /= _________________________\n",
    "\n",
    "    # TODO: Measure bias for target words\n",
    "    # For each word in target_words:\n",
    "    #   1. Get word vector\n",
    "    #   2. Project onto gender direction: dot product normalized by vector norm\n",
    "    #   3. Store in biases dictionary\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    biases = {}\n",
    "    for word in target_words:\n",
    "        if _________________________:\n",
    "            vec = _________________________\n",
    "            bias = _________________________\n",
    "            biases[word] = bias\n",
    "\n",
    "    return biases\n",
    "\n",
    "# Define bias measurement parameters\n",
    "gender_pairs = [(\"he\", \"she\"), (\"man\", \"woman\"), (\"boy\", \"girl\")]\n",
    "professions = [\"doctor\", \"nurse\", \"engineer\", \"teacher\", \"programmer\",\n",
    "               \"secretary\", \"scientist\", \"homemaker\", \"pilot\"]\n",
    "\n",
    "# TODO: Measure and display bias scores\n",
    "# 1. Call measure_gender_bias with appropriate arguments\n",
    "# 2. Sort professions by bias score (descending)\n",
    "# 3. Print table with visual indicators:\n",
    "#    üî¥ for |score| > 0.15 (strong bias)\n",
    "#    üü† for 0.05 < |score| <= 0.15 (moderate bias)\n",
    "#    üü¢ for |score| <= 0.05 (neutral)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "biases = _________________________\n",
    "\n",
    "print(\"‚öñÔ∏è  Gender Bias Audit: Google News Embeddings\")\n",
    "print(\"(Positive = male-associated | Negative = female-associated)\")\n",
    "print(\"=\"*75)\n",
    "print(f\"{'Profession':<18} {'Bias Score':<15} {'Association':<25}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "for prof in sorted(biases, key=biases.get, reverse=True):\n",
    "    score = biases[prof]\n",
    "    if score > 0.15:\n",
    "        marker = \"üî¥\"\n",
    "        assoc = \"Strongly male-associated ‚ôÇ‚ôÇ\"\n",
    "    elif score > 0.05:\n",
    "        marker = \"üü†\"\n",
    "        assoc = \"Moderately male-associated ‚ôÇ\"\n",
    "    elif score < -0.15:\n",
    "        marker = \"üî¥\"\n",
    "        assoc = \"Strongly female-associated ‚ôÄ‚ôÄ\"\n",
    "    elif score < -0.05:\n",
    "        marker = \"üü†\"\n",
    "        assoc = \"Moderately female-associated ‚ôÄ\"\n",
    "    else:\n",
    "        marker = \"üü¢\"\n",
    "        assoc = \"Neutral ‚ö≤\"\n",
    "\n",
    "    print(f\"{marker} {prof:<16} {score:+.4f}        {assoc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f089494",
   "metadata": {},
   "source": [
    "**Question 8.2**: Why do embeddings contain societal biases? What are potential real-world harms if biased embeddings are deployed in hiring or lending systems without auditing?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Limitations and Future Directions\n",
    "\n",
    "### 9.1 The Polysemy Problem: Static vs. Contextual Embeddings\n",
    "\n",
    "Static embeddings assign one vector per word type‚Äîfailing for polysemous words.\n",
    "\n",
    "#### Exercise 9.1: Demonstrating the Polysemy Limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b51caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_polysemy(model, word, contexts):\n",
    "    \"\"\"\n",
    "    Show how static embeddings cannot distinguish word senses.\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained embedding model\n",
    "        word (str): Polysemous word to analyze\n",
    "        contexts (list): Sentences showing different meanings\n",
    "    \"\"\"\n",
    "    if word not in model:\n",
    "        print(f\"‚ö†Ô∏è  '{word}' not in vocabulary\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nüî§ Polysemy Analysis: '{word}'\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Single static vector (first 10 dimensions): {model[word][:10].round(3)}\")\n",
    "    print(\"\\nContext examples (static embedding cannot distinguish these):\")\n",
    "\n",
    "    # TODO: For each context sentence:\n",
    "    # 1. Compute sentence vector using doc_to_vec\n",
    "    # 2. Compute similarity between word vector and sentence context\n",
    "    # 3. Print context with similarity score\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for i, ctx in enumerate(contexts, 1):\n",
    "        ctx_vec = _________________________\n",
    "        if _________________________:\n",
    "            continue\n",
    "        sim = _________________________\n",
    "        print(f\"  {i}. '{ctx}'\")\n",
    "        print(f\"     ‚Üí Similarity to '{word}' vector: {sim:.3f}\")\n",
    "\n",
    "# Demonstrate with polysemous words\n",
    "demonstrate_polysemy(w2v_google, \"bank\", [\n",
    "    \"I deposited money at the bank\",          # Financial institution\n",
    "    \"We sat on the river bank\",               # River edge\n",
    "    \"The plane made a steep bank turn\"        # Aviation maneuver\n",
    "])\n",
    "\n",
    "# TODO: Add another polysemous word example (e.g., \"apple\", \"light\", \"crane\")\n",
    "# YOUR CODE HERE\n",
    "demonstrate_polysemy(w2v_google, _________________________, [\n",
    "    _________________________,  # Meaning 1\n",
    "    _________________________,  # Meaning 2\n",
    "    _________________________   # Meaning 3\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f70ab",
   "metadata": {},
   "source": [
    "**Question 9.1**: How would contextual embeddings (like BERT) solve the polysemy problem? Describe the key architectural difference that enables this capability.\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your answer here]\n",
    "```\n",
    "\n",
    "### 9.2 When to Use Static vs. Contextual Embeddings\n",
    "\n",
    "| Decision Factor          | Static Embeddings (Word2Vec/GloVe)      | Contextual Embeddings (BERT)                |\n",
    "| ------------------------ | --------------------------------------- | ------------------------------------------- |\n",
    "| **Task complexity**      | Simple classification, clustering       | WSD, coreference, complex QA                |\n",
    "| **Compute constraints**  | ‚úÖ Low latency (<10ms), CPU-friendly    | ‚ö†Ô∏è High latency (50-200ms), GPU recommended |\n",
    "| **Data availability**    | Works with small labeled datasets       | Requires fine-tuning data                   |\n",
    "| **Polysemy handling**    | ‚ùå Impossible                           | ‚úÖ Native capability                        |\n",
    "| **Model size**           | 100-300MB                               | 300MB-2GB+                                  |\n",
    "| **Real-world use cases** | Recommendation systems, semantic search | Virtual assistants, machine translation     |\n",
    "\n",
    "> üí° **Practical Guidance**: Start with static embeddings for most applications. Upgrade to contextual models only when polysemy disambiguation is critical to task success.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Final Exercises\n",
    "\n",
    "### Exercise 10.1: Semantic Arithmetic Exploration\n",
    "\n",
    "Explore vector relationships that reveal linguistic regularities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_analogy(model, positive, negative, topn=5):\n",
    "    \"\"\"\n",
    "    Explore vector arithmetic relationships.\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained embedding model\n",
    "        positive: List of words to add\n",
    "        negative: List of words to subtract\n",
    "        topn: Number of results to return\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = model.most_similar(positive=positive, negative=negative, topn=topn)\n",
    "        print(f\"\\n{' + '.join(positive)} - {' - '.join(negative)} ‚âà\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "        # TODO: Print results with visual confidence bars\n",
    "        # For each result, print word, score, and bar proportional to score\n",
    "        # Example: \"  1. berlin    0.7821 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        for i, (word, score) in enumerate(result, 1):\n",
    "            bar = _________________________\n",
    "            print(f\"  {i}. {word:<20} {score:.4f} {bar}\")\n",
    "        return result\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ö†Ô∏è  Missing word in vocabulary: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üß† SEMANTIC ARITHMETIC WITH GOOGLE NEWS EMBEDDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# TODO: Explore at least 3 different types of analogies:\n",
    "# 1. Geography relationships (capitals)\n",
    "# 2. Gender transformations\n",
    "# 3. Temporal relationships (verb tenses)\n",
    "# 4. Conceptual relationships (abstract concepts)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "print(\"\\nüåç Capital Cities:\")\n",
    "explore_analogy(w2v_google, _________________________, _________________________, topn=3)\n",
    "\n",
    "print(\"\\n‚öñÔ∏è  Gender Transformations:\")\n",
    "explore_analogy(w2v_google, _________________________, _________________________, topn=3)\n",
    "\n",
    "print(\"\\n‚è≥ Verb Tenses:\")\n",
    "explore_analogy(w2v_google, _________________________, _________________________, topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857107d9",
   "metadata": {},
   "source": [
    "### Exercise 10.2: Build a Semantic Recommendation System\n",
    "\n",
    "Implement semantic product recommendations for e-commerce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09319ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E-commerce product catalog\n",
    "products = {\n",
    "    \"laptop\": \"powerful laptop with fast processor and long battery life for work and gaming\",\n",
    "    \"tablet\": \"lightweight tablet with touchscreen display and all-day battery for media consumption\",\n",
    "    \"smartphone\": \"premium smartphone with high-resolution camera system and 5G connectivity\",\n",
    "    \"wireless_headphones\": \"noise-cancelling wireless headphones with 30-hour battery life\",\n",
    "    \"mechanical_keyboard\": \"tactile mechanical keyboard with RGB lighting for gaming and typing\",\n",
    "    \"ultrawide_monitor\": \"34-inch ultrawide curved monitor with 144Hz refresh rate for productivity\",\n",
    "}\n",
    "\n",
    "# TODO: Convert products to vectors using doc_to_vec\n",
    "# Store in product_vectors dictionary: {product_name: vector}\n",
    "# YOUR CODE HERE\n",
    "product_vectors = {}\n",
    "for name, desc in products.items():\n",
    "    vec = _________________________\n",
    "    if _________________________:\n",
    "        product_vectors[name] = vec\n",
    "\n",
    "def recommend_products(product_id, top_k=3):\n",
    "    \"\"\"\n",
    "    Recommend semantically similar products.\n",
    "\n",
    "    Args:\n",
    "        product_id (str): Product the user is viewing\n",
    "        top_k (int): Number of recommendations to return\n",
    "\n",
    "    Returns:\n",
    "        list: Top k (product_name, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # TODO: Implement recommendation logic\n",
    "    # 1. Get target product vector\n",
    "    # 2. Compute cosine similarity with all other products\n",
    "    # 3. Return top-k most similar products (excluding self)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    if _________________________:\n",
    "        return []\n",
    "\n",
    "    target_vec = _________________________\n",
    "    similarities = {}\n",
    "\n",
    "    for name, vec in product_vectors.items():\n",
    "        if _________________________:\n",
    "            sim = _________________________\n",
    "            similarities[name] = sim\n",
    "\n",
    "    return _________________________\n",
    "\n",
    "# TODO: Generate and display recommendations for 3 different products\n",
    "# Format output with business-friendly similarity indicators:\n",
    "#   \"Excellent match\" for similarity > 0.6\n",
    "#   \"Good match\" for 0.45 < similarity <= 0.6\n",
    "#   \"Moderate match\" for similarity <= 0.45\n",
    "\n",
    "# YOUR CODE HERE\n",
    "print(\"üõí SEMANTIC PRODUCT RECOMMENDATIONS\")\n",
    "print(\"=\"*75)\n",
    "\n",
    "for product in [\"laptop\", \"wireless_headphones\", \"mechanical_keyboard\"]:\n",
    "    print(f\"\\nüëÄ User viewing: '{product}'\")\n",
    "    print(f\"   Description: {products[product][:60]}...\")\n",
    "\n",
    "    recommendations = _________________________\n",
    "    print(\"   ‚û°Ô∏è  Recommended alternatives:\")\n",
    "\n",
    "    for rank, (rec, score) in enumerate(recommendations, 1):\n",
    "        if score > 0.6:\n",
    "            quality = \"Excellent match\"\n",
    "        elif score > 0.45:\n",
    "            quality = \"Good match\"\n",
    "        else:\n",
    "            quality = \"Moderate match\"\n",
    "\n",
    "        print(f\"      {rank}. {rec:<22} (similarity: {score:.3f}) ‚Üí {quality}\")\n",
    "        print(f\"         Preview: {products[rec][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44bab7",
   "metadata": {},
   "source": [
    "**Reflection Question R1**: What was the most surprising semantic relationship you discovered through vector arithmetic? Why was it surprising?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your reflection here]\n",
    "```\n",
    "\n",
    "**Reflection Question R2**: How might bias in embeddings impact the product recommendations generated in Exercise 10.2? Describe one potential fairness concern and how you might address it.\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your reflection here]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Summary and Key Takeaways\n",
    "\n",
    "‚úÖ **Embeddings require scale**: Semantic geometry emerges only with massive training data (100M+ tokens)  \n",
    "‚úÖ **Pretrained > custom-trained**: For most applications, pretrained embeddings outperform custom models unless you have domain-specific data at scale  \n",
    "‚úÖ **Evaluation must be multi-faceted**: Use both intrinsic (similarity/analogies) and extrinsic (downstream tasks) metrics  \n",
    "‚úÖ **Bias is inevitable**: Embeddings encode societal patterns‚Äîalways audit before deployment in sensitive applications  \n",
    "‚úÖ **Static vs. contextual tradeoffs**: Choose static embeddings for simplicity/speed; contextual for disambiguation tasks\n",
    "\n",
    "> \"Word embeddings don't _understand_ language‚Äîthey compress statistical patterns from human text. Their power comes not from intelligence, but from scale: the geometric regularities that emerge when modeling billions of word contexts.\"\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "**Question R1:** What was the most surprising thing you learned about word embeddings?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your reflection here]\n",
    "```\n",
    "\n",
    "**Question R2:** How would you explain the distributional hypothesis to someone without an NLP background?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your reflection here]\n",
    "```\n",
    "\n",
    "**Question R3:** What are the most important factors when training word embeddings for a real application?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your reflection here]\n",
    "```\n",
    "\n",
    "**Question R4:** How might word embeddings be used in your own research or projects?\n",
    "\n",
    "_Your Answer:_\n",
    "\n",
    "```\n",
    "[Write your reflection here]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### Essential Papers\n",
    "\n",
    "1. **Mikolov et al. (2013).** \"Efficient Estimation of Word Representations in Vector Space\"\n",
    "\n",
    "   - Original Word2Vec paper\n",
    "\n",
    "2. **Pennington et al. (2014).** \"GloVe: Global Vectors for Word Representation\"\n",
    "\n",
    "   - Combines count-based and predictive methods\n",
    "\n",
    "3. **Bojanowski et al. (2017).** \"Enriching Word Vectors with Subword Information\"\n",
    "   - FastText and character n-grams\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- **Levy & Goldberg (2014):** \"Neural Word Embeddings as Implicit Matrix Factorization\"\n",
    "- **Arora et al. (2018):** \"Linear Algebraic Structure of Word Senses\"\n",
    "- CS224N: http://web.stanford.edu/class/cs224n/\n",
    "- Pre-trained embeddings: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Session 3:** Text Classification & Methodology\n",
    "\n",
    "   - Building robust NLP baselines\n",
    "   - Experimental design\n",
    "   - Train/Val/Test splitting\n",
    "\n",
    "2. **Future Topics:**\n",
    "   - Recurrent Neural Networks (RNNs, LSTMs)\n",
    "   - Attention mechanisms\n",
    "   - Transformers and BERT\n",
    "   - Modern LLMs (GPT, Claude, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## Submission Guidelines\n",
    "\n",
    "### What to Submit\n",
    "\n",
    "1. **Completed Notebook** with all exercises filled in\n",
    "2. **Reflection Document** answering all questions\n",
    "3. **Final Project Code** (Exercise 10.3)\n",
    "\n",
    "### Grading Criteria\n",
    "\n",
    "- **Correctness (40%):** Do your implementations work correctly?\n",
    "- **Completeness (20%):** Did you complete all exercises?\n",
    "- **Understanding (20%):** Do your answers demonstrate deep understanding?\n",
    "- **Creativity (20%):** Did you extend beyond the basic requirements?\n",
    "\n",
    "### Deadline\n",
    "\n",
    "Check the course website for submission deadline.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
